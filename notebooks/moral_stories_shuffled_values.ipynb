{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e069b4c",
   "metadata": {},
   "source": [
    "# Shuffling norm judgments\n",
    "***\n",
    "Does the value of a norm have impact on the classification accuracy? We test this hypothesis by applying the `Learn2Split` model and shuffling of the value judgments. In the end, we train the action classification task and compare.\n",
    "\n",
    "Shuffle strategies:\n",
    "* Truly randomn\n",
    "    * Will cause most errornous sentences of the strategies\n",
    "* Mirroring polarity: Choose the closest opposite value.\n",
    "* Single polarity: All values are the same\n",
    "    * This effectively reduces the task to NLI! Here we HAVE to be better!\n",
    "    \n",
    "    \n",
    "### Approach\n",
    "***\n",
    "The `L2S` model not only splits a norm, but also conjugates the action:\n",
    "* `It's wrong to become addicted to gambling.` becomes\n",
    "    * `It's wrong`\n",
    "    * `becoming addicted to gambling`\n",
    "\n",
    "Since the splitting is not reversible as is, there are two possible options to consider:\n",
    "1. Re-run the original model on concatenated `L2S` output. If the accuracy does not drop, then we can simply do our comparison experiments on `L2S` concat'ed. \n",
    "2. Train a `L2S` reverse model (*sigh*)\n",
    "\n",
    "##### ...and then:\n",
    "Train the action classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e74d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "\n",
    "from ailignment.datasets.util import get_accuracy_metric\n",
    "from ailignment.datasets.moral_stories import make_action_classification_dataframe, get_random_value_dataset\n",
    "import ailignment.datasets.moral_stories_clustered as msc\n",
    "from ailignment.training import sequence_classification\n",
    "\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "dataframe = pd.read_pickle(\"../data/moral_stories_proto_l2s.dat\")\n",
    "\n",
    "# a function to convert train and test dataframes to huggingface datasets\n",
    "def convert_hf_dataset(tokenizer, train, test, padding=\"max_length\"):\n",
    "    def tok(samples):\n",
    "        return tokenizer(samples[\"action\"], samples[\"norm\"], padding=padding, \n",
    "                         truncation=True, return_token_type_ids=True)\n",
    "\n",
    "    train_data = Dataset.from_pandas(train)\n",
    "    train_data = train_data.map(tok, batched=True)\n",
    "    val_data = Dataset.from_pandas(test)\n",
    "    val_data = val_data.map(tok, batched=True)\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0b69df",
   "metadata": {},
   "source": [
    "### Action classification on `L2S` concatenated\n",
    "***\n",
    "Question: If we concatenate the norm judgment with the conjugated norm action, does the accuracy change?\n",
    "\n",
    "The task was run for 5 epochs on the `roberta-large` model:\n",
    "* Epoch 5: 0.913 accuracy\n",
    "\n",
    "Although not a thorough analysis in any capacity, we argue that the performance is not significantly reduced by the simplistic concatenation approach. The upcoming experiments on the shuffled datasets can be carried out without greater worries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e7d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataframe.copy()\n",
    "data[\"norm\"] = data[\"norm_value\"] + \" \" + data[\"norm_action\"]\n",
    "train, test = make_action_classification_dataframe(data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd16cd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff2664eb14a4d1ea8f12be479034397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7928211955f84b929d80e8e4eb1cea01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = \"bert-large-uncased\"\n",
    "#name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "\n",
    "train_data, val_data = convert_hf_dataset(tokenizer, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d731ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data/kiehne/results/shuffled_values/random/bert-large-uncased/0.50/\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='logs/',\n",
    "    log_level=\"info\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=30000000,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6\n",
    ")\n",
    "acc_metric = get_accuracy_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f50a80d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: intention, l2s_output, norm_action, ID, norm, norm_storyfied, situation, __index_level_0__, consequence, norm_value, norm_sentiment, action, actor_name.\n",
      "***** Running training *****\n",
      "  Num examples = 21592\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 13495\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='13495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/13495 : < :, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 1524, in forward\n    outputs = self.bert(\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 990, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 582, in forward\n    layer_outputs = layer_module(\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 470, in forward\n    self_attention_outputs = self.attention(\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 401, in forward\n    self_outputs = self.self(\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 333, in forward\n    attention_probs = self.dropout(attention_probs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/dropout.py\", line 58, in forward\n    return F.dropout(input, self.p, self.training, self.inplace)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/functional.py\", line 1169, in dropout\n    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\nRuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.76 GiB total capacity; 5.54 GiB already allocated; 9.31 MiB free; 5.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29720/3415396853.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macc_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1282\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1787\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1819\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 1524, in forward\n    outputs = self.bert(\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 990, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 582, in forward\n    layer_outputs = layer_module(\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 470, in forward\n    self_attention_outputs = self.attention(\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 401, in forward\n    self_outputs = self.self(\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 333, in forward\n    attention_probs = self.dropout(attention_probs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/modules/dropout.py\", line 58, in forward\n    return F.dropout(input, self.p, self.training, self.inplace)\n  File \"/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/functional.py\", line 1169, in dropout\n    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\nRuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.76 GiB total capacity; 5.54 GiB already allocated; 9.31 MiB free; 5.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics=acc_metric,\n",
    ")\n",
    "logs = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99934653",
   "metadata": {},
   "source": [
    "# Experiments: Single model training\n",
    "***\n",
    "Pick a dataset, then run the finetuning. Useful for explorative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d42171a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without clustering\n",
    "data = get_random_value_dataset(dataframe, p=0.5, top_n=20)\n",
    "train, test = make_action_classification_dataframe(data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d321b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with clustering\n",
    "dataframe = msc.assign_norm_clusters(dataframe)\n",
    "data = msc.get_random_value_dataset(dataframe, 0.5, top_n=20)\n",
    "train, test = msc.make_action_classification_dataframe(data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c764211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9410ddd552402aa3bbc1c035c370fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28e30e7ad6e4e948ecd830299a620f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = \"roberta-large\"\n",
    "name = \"bert-base-uncased\"\n",
    "name = \"/data/kiehne/results/shuffled_values/random/bert-base-uncased/0.00/checkpoint-5625/\"\n",
    "#name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "name = \"bert-large-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "\n",
    "train_data, val_data = convert_hf_dataset(tokenizer, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4241597",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data/kiehne/results/shuffled_values/random/bert-large-uncased/0.50/\",\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=500,\n",
    "    #weight_decay=0.01,\n",
    "    logging_dir='logs/',\n",
    "    log_level=\"info\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=30000000,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-6\n",
    ")\n",
    "acc_metric = get_accuracy_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d755951",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: situation, intention, l2s_output, norm_storyfied, flipped, action, norm_sentiment, norm_action, __index_level_0__, ID, norm, consequence, actor_name, norm_value.\n",
      "***** Running training *****\n",
      "  Num examples = 21592\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 18893\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5468' max='18893' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5468/18893 2:20:26 < 5:44:55, 0.65 it/s, Epoch 2.03/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.706600</td>\n",
       "      <td>0.692114</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.694700</td>\n",
       "      <td>0.682954</td>\n",
       "      <td>0.567083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: situation, intention, l2s_output, norm_storyfied, flipped, action, norm_sentiment, norm_action, __index_level_0__, ID, norm, consequence, actor_name, norm_value.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random/bert-large-uncased/0.50/checkpoint-2699\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random/bert-large-uncased/0.50/checkpoint-2699/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random/bert-large-uncased/0.50/checkpoint-2699/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: situation, intention, l2s_output, norm_storyfied, flipped, action, norm_sentiment, norm_action, __index_level_0__, ID, norm, consequence, actor_name, norm_value.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random/bert-large-uncased/0.50/checkpoint-5398\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random/bert-large-uncased/0.50/checkpoint-5398/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random/bert-large-uncased/0.50/checkpoint-5398/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics=acc_metric,\n",
    ")\n",
    "logs = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339f393",
   "metadata": {},
   "source": [
    "# Experiments: Consecutive models\n",
    "***\n",
    "Run several models over different training data or with different hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ed405",
   "metadata": {},
   "source": [
    "### Random values experiment\n",
    "***\n",
    "Given different values of $p$, how is the accuracy affected? \n",
    "\n",
    "First run:\n",
    "* Single epoch, batch size 8, $p\\in\\{0,0.1,0.2,..,1\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95dc783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa371144f7364250b6a4269f73427f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708e8780c41c4ab7957a06cb5aef04a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running training *****\n",
      "  Num examples = 21592\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 13495\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13495' max='13495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13495/13495 5:48:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.688500</td>\n",
       "      <td>0.669063</td>\n",
       "      <td>0.582083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.646622</td>\n",
       "      <td>0.628750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.637400</td>\n",
       "      <td>0.629407</td>\n",
       "      <td>0.642917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.615500</td>\n",
       "      <td>0.631621</td>\n",
       "      <td>0.655833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.621500</td>\n",
       "      <td>0.625205</td>\n",
       "      <td>0.659583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-2699\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-2699/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-2699/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-5398\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-5398/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-5398/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-8097\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-8097/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-8097/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-10796\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-10796/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-10796/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-13495\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-13495/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.00/checkpoint-13495/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /home/kiehne/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d8ad9de2224be0858a350e9c7a955e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bdccbe9b7b44cebe01186d1016cf4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running training *****\n",
      "  Num examples = 21592\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 13495\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13495' max='13495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13495/13495 5:48:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>0.692056</td>\n",
       "      <td>0.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.666379</td>\n",
       "      <td>0.592083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.671000</td>\n",
       "      <td>0.686432</td>\n",
       "      <td>0.570417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.652700</td>\n",
       "      <td>0.659723</td>\n",
       "      <td>0.602917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.661151</td>\n",
       "      <td>0.607083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-2699\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-2699/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-2699/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-5398\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-5398/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-5398/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-8097\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-8097/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-8097/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-10796\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-10796/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-10796/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-13495\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-13495/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.10/checkpoint-13495/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /home/kiehne/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e3936ecc664a6f92f68c0af4978f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f6aa0be730422f87dfbd189e89320c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running training *****\n",
      "  Num examples = 21592\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 13495\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13495' max='13495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13495/13495 5:49:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.705600</td>\n",
       "      <td>0.694622</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.698400</td>\n",
       "      <td>0.688698</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.682400</td>\n",
       "      <td>0.686607</td>\n",
       "      <td>0.556250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.677300</td>\n",
       "      <td>0.679257</td>\n",
       "      <td>0.581250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.681200</td>\n",
       "      <td>0.677327</td>\n",
       "      <td>0.584583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-2699\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-2699/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-2699/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-5398\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-5398/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-5398/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-8097\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-8097/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-8097/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-10796\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-10796/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-10796/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-13495\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-13495/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.20/checkpoint-13495/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /home/kiehne/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e184f804fda0464e96ffe96e84a27f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae96abe234d4f10a260581c4386ae41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running training *****\n",
      "  Num examples = 21592\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 13495\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13495' max='13495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13495/13495 5:49:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.697100</td>\n",
       "      <td>0.696442</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.704200</td>\n",
       "      <td>0.692649</td>\n",
       "      <td>0.510833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.696500</td>\n",
       "      <td>0.691308</td>\n",
       "      <td>0.532083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.694100</td>\n",
       "      <td>0.694949</td>\n",
       "      <td>0.525833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.690300</td>\n",
       "      <td>0.688906</td>\n",
       "      <td>0.546250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-2699\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-2699/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-2699/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-5398\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-5398/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-5398/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-8097\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-8097/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-8097/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-10796\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-10796/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-10796/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-13495\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-13495/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.30/checkpoint-13495/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /home/kiehne/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566afcc0f0414242838cfc9ef8ce3487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630463ecc1824a9381604f5e876bfd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running training *****\n",
      "  Num examples = 21592\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 13495\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13495' max='13495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13495/13495 5:49:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.698759</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.700500</td>\n",
       "      <td>0.693756</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.700100</td>\n",
       "      <td>0.694509</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.697800</td>\n",
       "      <td>0.693385</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.697200</td>\n",
       "      <td>0.693094</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-2699\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-2699/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-2699/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-5398\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-5398/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-5398/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-8097\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-8097/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-8097/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-10796\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-10796/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-10796/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-13495\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-13495/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.40/checkpoint-13495/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /home/kiehne/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e6b5bd119c4041bc1d825e83de0043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6441a991c1e1436890396b9f5c8ce516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running training *****\n",
      "  Num examples = 21592\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 13495\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13495' max='13495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13495/13495 5:49:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.704500</td>\n",
       "      <td>0.698303</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.700600</td>\n",
       "      <td>0.694564</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.697200</td>\n",
       "      <td>0.697316</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.699800</td>\n",
       "      <td>0.693213</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.699000</td>\n",
       "      <td>0.693200</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-2699\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-2699/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-2699/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-5398\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-5398/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-5398/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-8097\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-8097/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-8097/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-10796\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-10796/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-10796/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-13495\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-13495/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.50/checkpoint-13495/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /home/kiehne/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5a5d7b441949e3ae9d2b9bf8906214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc746be099e44ee8f568173c52c3bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running training *****\n",
      "  Num examples = 21592\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 13495\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13495' max='13495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13495/13495 5:49:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>0.694529</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.695217</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.697100</td>\n",
       "      <td>0.694257</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.698600</td>\n",
       "      <td>0.695150</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>0.693065</td>\n",
       "      <td>0.500417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-2699\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-2699/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-2699/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-5398\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-5398/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-5398/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-8097\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-8097/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-8097/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-10796\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-10796/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-10796/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-13495\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-13495/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.60/checkpoint-13495/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /home/kiehne/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e3c8a113394b16a11fcdce27dc6a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1252cf343b45c989148780e7ed86ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running training *****\n",
      "  Num examples = 21592\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 13495\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13495' max='13495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13495/13495 5:49:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.703400</td>\n",
       "      <td>0.695937</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.700900</td>\n",
       "      <td>0.695114</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.696600</td>\n",
       "      <td>0.692331</td>\n",
       "      <td>0.520833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.696800</td>\n",
       "      <td>0.691864</td>\n",
       "      <td>0.528750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.691700</td>\n",
       "      <td>0.690360</td>\n",
       "      <td>0.522917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-2699\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-2699/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-2699/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-5398\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-5398/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-5398/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-8097\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-8097/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-8097/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-10796\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-10796/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-10796/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-13495\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-13495/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.70/checkpoint-13495/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /home/kiehne/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f50f39caf1446dab17697a5c43d8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8fe80e7fdb4d50bfb582006f9f5a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running training *****\n",
      "  Num examples = 21592\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 13495\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13495' max='13495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13495/13495 5:50:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.698600</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.540833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.697500</td>\n",
       "      <td>0.686575</td>\n",
       "      <td>0.547917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.684400</td>\n",
       "      <td>0.688150</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.680400</td>\n",
       "      <td>0.683564</td>\n",
       "      <td>0.565417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.673500</td>\n",
       "      <td>0.684133</td>\n",
       "      <td>0.571250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-2699\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-2699/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-2699/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-5398\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-5398/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-5398/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-8097\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-8097/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-8097/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-10796\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-10796/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-10796/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-13495\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-13495/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.80/checkpoint-13495/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /home/kiehne/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08597e69d89141b19221f1763b10f37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3043526395ed40239bcbe8b9717cd4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running training *****\n",
      "  Num examples = 21592\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 13495\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13495' max='13495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13495/13495 5:49:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.699600</td>\n",
       "      <td>0.728070</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.686200</td>\n",
       "      <td>0.669119</td>\n",
       "      <td>0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.661500</td>\n",
       "      <td>0.665300</td>\n",
       "      <td>0.603750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.658900</td>\n",
       "      <td>0.669084</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.648900</td>\n",
       "      <td>0.659022</td>\n",
       "      <td>0.624583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-2699\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-2699/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-2699/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-5398\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-5398/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-5398/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-8097\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-8097/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-8097/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-10796\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-10796/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-10796/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-13495\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-13495/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/0.90/checkpoint-13495/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /home/kiehne/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af6034e42fe4a90b3fc002be0800691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdef01ccadf5425d990bf4ab5bd6df42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running training *****\n",
      "  Num examples = 21592\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 13495\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13495' max='13495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13495/13495 5:49:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.695000</td>\n",
       "      <td>0.688663</td>\n",
       "      <td>0.559583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.657100</td>\n",
       "      <td>0.636879</td>\n",
       "      <td>0.640833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.638300</td>\n",
       "      <td>0.653263</td>\n",
       "      <td>0.643750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.624900</td>\n",
       "      <td>0.623647</td>\n",
       "      <td>0.652917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.614800</td>\n",
       "      <td>0.625198</td>\n",
       "      <td>0.656667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-2699\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-2699/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-2699/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-5398\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-5398/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-5398/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-8097\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-8097/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-8097/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-10796\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-10796/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-10796/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: norm, ID, norm_sentiment, flipped, norm_action, actor_name, norm_storyfied, norm_value, intention, consequence, __index_level_0__, l2s_output, situation, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-13495\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-13495/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-large-uncased/1.00/checkpoint-13495/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformers.logging.set_verbosity_warning()\n",
    "\n",
    "ps = np.linspace(0,1,11)\n",
    "test_size = 0.1\n",
    "model = \"roberta-base\"\n",
    "#model = \"roberta-large\"\n",
    "\n",
    "#model = \"bert-base-uncased\"\n",
    "#model = \"/data/kiehne/results/shuffled_values/random/bert-base-uncased/1.00/checkpoint-5625/\"\n",
    "model = \"bert-large-uncased\"\n",
    "\n",
    "#model = \"albert-xxlarge-v2\"\n",
    "#model = \"albert-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "dataframe = pd.read_pickle(\"../data/moral_stories_proto_l2s.dat\")\n",
    "#dataframe = msc.assign_norm_clusters(dataframe)\n",
    "\n",
    "acc_metric = get_accuracy_metric()\n",
    "results = []\n",
    "\n",
    "for p in ps:\n",
    "    # create dataset\n",
    "    #random_values = msc.get_random_value_dataset(dataframe, p, top_n=20)\n",
    "    #train, test = msc.make_action_classification_dataframe(random_values, test_size=test_size)\n",
    "    data = get_random_value_dataset(dataframe, p=p, top_n=20)\n",
    "    train, test = make_action_classification_dataframe(data, test_size=test_size)\n",
    "    \n",
    "    # create training args\n",
    "    out_dir = f\"/data/kiehne/results/shuffled_values/random_no_pretrain/{model}/{p:0.2f}/\"\n",
    "    #out_dir = f\"/data/kiehne/results/shuffled_values/random_ms_only_pretrain/bert-base-uncased/{p:0.2f}/\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=500,\n",
    "        #weight_decay=0.01,\n",
    "        logging_steps=500,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_steps=30000000,\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=1e-6\n",
    "    )\n",
    "    \n",
    "    # create data loading function\n",
    "    data_loader = lambda tokenizer: convert_hf_dataset(tokenizer, train, test)\n",
    "    if \"gpt2\" in model:\n",
    "        data_loader = lambda tokenizer: convert_hf_dataset(tokenizer, train, test, padding=\"do_not_pad\")\n",
    "\n",
    "    \n",
    "    r = sequence_classification(data_loader, model, training_args, tokenizer, \n",
    "                                acc_metric, use_pretrained=False)\n",
    "    results.append(r)\n",
    "    \n",
    "    # save training data for later evaluation\n",
    "    train.to_pickle(out_dir+\"train.dat\")\n",
    "    test.to_pickle(out_dir+\"test.dat\")\n",
    "\n",
    "accuracies = [[y[\"eval_accuracy\"] for y in x if \"eval_accuracy\" in y] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d289a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a6a965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# topn=20\n",
    "import matplotlib.pyplot as plt\n",
    "# this is the random init bert\n",
    "max_acc = [max(a) for a in accuracies]\n",
    "plt.plot(ps, max_acc, \"*\")\n",
    "plt.ylim(0,1)\n",
    "plt.title(f\"Model: bert-base-uncased Anti-MS only pretrain\")\n",
    "plt.xlabel(\"$p$\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff26b22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEYCAYAAABRB/GsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcqUlEQVR4nO3debhdZX328e9NEghDGBNUyIQyBihCzxuwUgVBG7AmtSqG0VgkosVXqwVToQqIviIVK0rLULiQGaSFRgHBIshQgRwEEYKREDAJoAkYxhgg5Pf+8TwHFztn2OGctXfOee7PdeXKXsNe67f2Xmfdaz1r2IoIzMysXOu0uwAzM2svB4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGs5SRMlhaThTYw7Q9LtAz3dgZTnuW0r5zmUSDpR0sXtrqNZkm6R9Il21zEQJB0q6cZ211EHB8EAkvSYpJcljW7of2/eAE5sU2m1a1ew2J9I2id/B1c39N8t97+l0m+apPskPSfpKUk/lbRNy4tuEUkXSDqlP9OIiEsi4n0DVdPaxEEw8B4FDu7qkLQrsEH7yqmfN/5rlaXAOyRtUen3MeA3XR35iOxC4AvAJsA2wJnAqy2sc8Ao6de2rPR12EEw8C4Cjqh0f4z0R/caSZtIulDSUkm/lXRC14osaZikf8l7aQuA93fz3vMkPSnpcUmnSBrWj3r/TtITeXr/WJnPOpJmSXpE0tOSrpS0eR7Wtfd/pKSFwE+BW/Nbn5H0gqR39DLPAyUtyMt4WmXZ35b3TJ/Owy6RtGmlpi/mZX5e0jxJ+/VVa3fykdv+le7Xmlsqy/YxSQtzHcdXxh0m6Ut5Xs9LukfSuDzsO5IW5b3seyT9ZeV9kyV15mG/l3R6Zdhekv5X0jOSfilpn8qwbST9LM/rJ8Drjja78TJwDTC9q17go8AllXHeDjwaETdF8nxE/GdELOzh8+ptfZ0h6fa8zi6T9KikA7qZxrqS/pB3jLr6bSlpuaQx3Yw/Q9Idkr4n6VlJv+76vvPwWyR9TdIdwHLgrZJ2lPSTPJ95kg7K484EDgWOy+vmD3P/x/I6dT/woqThlfXoeUlzJX2woabbK90h6WhJD+fv7kxJ6uP7WTtFhP8N0D/gMWB/YB6wEzAMWAxMAAKYmMe7EPhvYBQwkbS3dmQedjTwa2AcsDlwc37v8Dz8auBsYENgS+Bu4JN52Azg9ko9PwJm9VDrxDzdy/K0diXtTe6fh38WuBMYC6yX53lZw3svzO9dv9JveB+fUeRl2hwYn5f9E3nYtsB78/zGkMLlX/OwHYBFwFaVGt7WV629fU+V7hOBixuW7dy8XLsBLwE75eHHAr/K9SgP3yIPOwzYAhhO2tv+HTAyD/s5cHh+vRGwV369NfA0cCBpx+y9uXtM5X2n5+V6F/B8V63dLNc+pPXtL4C7cr8DgRuATwC35H5vBVYA3wb2BTbq4zvrbX2dAbwCHEVa3z8FPAEoD7+l8v3+G3BqZbqfBX7YwzxnACuBfwBGkMLsWWDzynQXAjvnz3uTvH58PHfvDjwFTMrjXwCc0s16cB/pb2393O8jwFb5u/go8CLwlh7+voL0N7YpaV1eCkxp93boDW272l3AUPrHn4LgBOD/AVOAn+QVM/If0TDSXtukyvs+Wfkj/SlwdGXY+/J7hwNvIm2U1q8MPxi4Ob9+3YraR60T83R3rPT7JnBefv0QsF9l2FvyH/zwynvf2s30mgmCKZXuTwM39TDu3wD35tfbAkvy5zuiYbwea+3te6p0n8jqQTC2MvxuYHp+PQ+Y1uRnvAzYLb++FTgJGN0wzheBixr63UA6khxP2hhuWBl2KX0EQX79MCmsLiftDb8WBHn4XsCVpI3XCtKGcrVAaGJ9nQHMrwzbIH9+b87dt/CnINiTtPHuColO4KAelmUGlUCpfA+HV6Z7cmXYR4HbGqZxNvCV/PoCug+Cv+vjO7yv6/um+yDYu9J9JT3seK3t/9w0VI+LgENIK86FDcNGk/Zwflvp91vSniGkvZFFDcO6TMjvfTIfij5DWtm37EetjfPaqjKvqyvzeYjUhvymHt67GkkP5kPxF6rNJD3NU9KbJF2em3+eAy4mN4VExHzgc6SN9pI8Xp+1SjqrUsOXmvg8uvyu8no5aS8e0t7jIz0s7z9Keig3ZTxD2kvtaso5Etge+LWkOZL+ulL7R7pqz+/bmxRmWwHLIuLFhs+rGRcBx5D2+K9uHBgRd0bEQRExBvhL0tHG8Y3j0ff6CpXPKiKW55cb0SAi7iJ9lvtI2pEU7rN7WYbHI29hK/PdqtJdXY8mAHs2fI6HAm/uZfqN00DSEUon0bumsQu9N8f1tJ4MKg6CGkTEb0knjQ8E/qth8FOkvdUJlX7jgcfz6ydJG5vqsC6LSEcEoyNi0/xv44jYuR/lNs7ricq8DqjMZ9OIGBkRj1fGjx5epx4RO0fERvnfbU3M8+t5OrtGxMakppbX2lwj4tKI2Js/NbWd2letEXF0pYav5/Ff5PUn8PvaWFQtAt7W2DMH3XHAQcBmEbEpqSlDufaHI+JgUmifClwlacM8vYsaat8wIr5BWhc2y+NVP69mXEQ62rqusnHuVkTMIa2nu3QzuK/1dU19n/S9Hg5cFRErehl364Y29+q6Aq9f5xYBP2v4HDeKiE91M27Va/0lTSA1CR5Dau7bFHiAyjo4VDkI6nMk8J6GvTki4lXSIeTXJI3KK9/nSXu/5GH/V9JYSZsBsyrvfRK4EfiWpI2VTpK+TdK7+1HnP0vaQNLOpPbVK3L/s3KNEwAkjZE0rZfpLAVWkdqf+3KspM2UTrJ+tjLPUcALwLOStia1x5Pnv4Ok90haj9SU8cc8vzdS633AdEkjJHUAH26i5i7/AXxV0nZK/kzpCp1RpGacpcBwSV8GNq7Uf5ikMRGxCngm915F+t4/IOmvlE5Ej1S6DHRs3qHoBE5SOtm6N/CBZoqMiEeBd9PNXr6kvSUdJWnL3L0jMJV0nqVxOn2tr2vqYuCDpDBoPFputCXpb2GEpI+Qzrtd18O4PwK2l3R4Hn+EpP8jaac8/Pf0vW5uSAqGpQCSPk734TjkOAhqEhGPRERnD4M/Q9orXQDcTmr3PT8PO5fURvxL4BesfkRxBLAuMJfUBn0VqRlhNZKub6I55GfAfOAm4F8iouuGme+QDttvlPQ8aSOxZ08TyXudXwPuyIfVe/Uyz/8G7iFtkK8Fzsv9TwL2IO1JX8vrl3094BukPdTfkTYS//RGagX+mbRXvyzP89Jexm10OmnDeCPwXK59fdJ39mPSidTfksKq2uwwBXhQ0gu53ukR8ceIWARMA75E2gAtIgVg19/mIXlZ/gB8hb43nq+JiNsj4oluBj1D2vD/KtfzY1Lz0Td7mFRv6+saycv7C9IG97Y+Rr8L2I70nX8N+HBEPN3DdJ8nnU+bTjpq+B3pyGu9PMp5wKS8bl7TwzTmAt8inaD/PekCijuaXbbBrOukjZlZS0g6H3giIk7oZZwZpJPMe7essIIVfROFmbWW0t31f0u6vNPWErU1DUk6X9ISSQ/0MFySzpA0X9L9kvaoqxYzaz9JXyWdfD0tn8OwtURtTUOS3kU68XdhRKx2wkXSgaS2xwNJbaDfiYje2nXNzKwGtR0RRMStpBNcPZlGComIiDuBTSV1e9LTzMzq085zBFvz+qsqFud+TzaOqPSskJkAG2644Z/vuOOOLSnQzGyouOeee57KNxCuZlCcLI6Ic4BzADo6OqKzs6erMs3MrDuSerwrvZ33ETzO6+8wHcsbv1vRzMzeoHYGwWzgiHz10F7As/nOWTMza6HamoYkXUZ6GuJoSYtJd0WOAIiIs0i3ih9Iuqt1OenxBmZm1mK1BUF+wFZvwwP4+7rmb2ZmzfGzhszMCucgMDMrnIPAzKxwDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8I5CMzMCucgMDMrXDFBsOS5FRx09s9Z8vyKdpdiZrZWKSYIzrjpYeY89gfO+J+H212KmdlaZXi7C6jbDidcz0srV73WffFdC7n4roWsN3wd5p1yQBsrMzNr3pLnVnDMZffyvUN2Z8tRIwd02kP+iOC24/Zl6tu3YuSItKgjR6zDtLdvxW1f3LfNlZmZNa/OVo0hf0Sw5cYjGbXecF5auYr1hq/DSytXMWq94QOeqGZmdWhFq8aQPyIAeOqFlzh0zwlc/el3cuieE1j6wkvtLsnMrCmtaNUY8kcEAGcf3vHa61P+Zpc2VtJadbYpmrVTSet2K1o1ijgiaLd2XbrqK6Vap52XJ5d4aXS71u12fdZ1t2ooIgZ0gnXr6OiIzs7OdpexRk64+ldccvdCDp08nlM+uGvt82tsU+xSwpVS7dpTbPV3vDbMux2fdbvX7XZ+z/0l6Z6I6Oh2WJ1BIGkK8B1gGPAfEfGNhuHjge8Dm+ZxZkXEdb1NczAFQbtW2iXPreCU6x7ixgd/x4pXVjFyxDr81c5v5vj379SyP9hSNsjt3DCVuFFs17rd7s96IPQWBLU1DUkaBpwJHABMAg6WNKlhtBOAKyNid2A68G911dMO7bp0dW24UqrVh+47nHA9E2ddy8V3LSQiXVkxcda17HDC9bXOt52XJ7dr3u36rKF96/ZQvwy9zpPFk4H5EbEAQNLlwDRgbmWcADbOrzcBnqixnpZr5wa5q03xkMnjufTuhSxtUZtmu27gu+24fXvcU6xTO7/jdm4U2/FZd2nHur027FzVqc4g2BpYVOleDOzZMM6JwI2SPgNsCOzf3YQkzQRmAowfP37AC61TuzbI7bpSqsQNcru+43bNu90bxXat2+38nutW2zkCSR8GpkTEJ3L34cCeEXFMZZzP5xq+JekdwHnALhGxemNcNpjOEZTq+Kt/xaV3L2TdYevw8qurWtaG/MmLOhkzauTr/lCrGw0bOP6sB5/ezhHUeUTwODCu0j0296s6EpgCEBE/lzQSGA0sqbEuq1lpR0El8mc9tNQZBHOA7SRtQwqA6cAhDeMsBPYDLpC0EzASWFpjTdYC3kiYDS61XTUUESuBY4AbgIdIVwc9KOlkSVPzaF8AjpL0S+AyYEYMthsbzMwGuVofMZHvCbiuod+XK6/nAu+sswYzM+udHzFhZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeFqDQJJUyTNkzRf0qwexjlI0lxJD0q6tM56zMxsdcPrmrCkYcCZwHuBxcAcSbMjYm5lnO2AfwLeGRHLJG1ZVz1mZta9Oo8IJgPzI2JBRLwMXA5MaxjnKODMiFgGEBFLaqzHzMy6UWcQbA0sqnQvzv2qtge2l3SHpDslTeluQpJmSuqU1Ll06dKayjUzK1O7TxYPB7YD9gEOBs6VtGnjSBFxTkR0RETHmDFjWluhmdkQ12cQSPqApDcSGI8D4yrdY3O/qsXA7Ih4JSIeBX5DCgYzM2uRZjbwHwUelvRNSTuuwbTnANtJ2kbSusB0YHbDONeQjgaQNJrUVLRgDeZhZmb91GcQRMRhwO7AI8AFkn6e2+xH9fG+lcAxwA3AQ8CVEfGgpJMlTc2j3QA8LWkucDNwbEQ83Y/lMTOzNaSIaG5EaQvgcOBzpA37tsAZEfHd2qrrRkdHR3R2drZylmZmg56keyKio7thzZwjmCrpauAWYAQwOSIOAHYDvjCQhZqZWes1c0PZh4BvR8St1Z4RsVzSkfWUZWZmrdJMEJwIPNnVIWl94E0R8VhE3FRXYWZm1hrNXDX0A2BVpfvV3M/MzIaAZoJgeH5EBAD59br1lWRmZq3UTBAsrVzuiaRpwFP1lWRmZq3UzDmCo4FLJH0PEOn5QUfUWpWZmbVMn0EQEY8Ae0naKHe/UHtVZmbWMk39HoGk9wM7AyMlARARJ9dYl5mZtUgzN5SdRXre0GdITUMfASbUXJeZmbVIMyeL/yIijgCWRcRJwDtID4czM7MhoJkgWJH/Xy5pK+AV4C31lWRmZq3UzDmCH+YfizkN+AUQwLl1FmVmZq3TaxDkH6S5KSKeAf5T0o+AkRHxbCuKMzOz+vXaNBQRq4AzK90vOQTMzIaWZs4R3CTpQ+q6btTMzIaUZoLgk6SHzL0k6TlJz0t6rua6zMysRZq5s7jXn6Q0M7PBrc8gkPSu7vo3/lCNmZkNTs1cPnps5fVIYDJwD/CeWioyM7OWaqZp6APVbknjgH+tqyAzM2utZk4WN1oM7DTQhZiZWXs0c47gu6S7iSEFx9tJdxibmdkQ0Mw5gs7K65XAZRFxR031mJlZizUTBFcBKyLiVQBJwyRtEBHL6y3NzMxaoak7i4H1K93rA/9TTzlmZtZqzQTByOrPU+bXG9RXkpmZtVIzQfCipD26OiT9OfDH+koyM7NWauYcweeAH0h6gvRTlW8m/XSlmZkNAc3cUDZH0o7ADrnXvIh4pd6yzMysVZr58fq/BzaMiAci4gFgI0mfrr80MzNrhWbOERyVf6EMgIhYBhxVW0VmZtZSzQTBsOqP0kgaBqxbX0lmZtZKzZws/jFwhaSzc/cngevrK8nMzFqpmSD4IjATODp330+6csjMzIaAPpuG8g/Y3wU8RvotgvcADzUzcUlTJM2TNF/SrF7G+5CkkNTRXNlmZjZQejwikLQ9cHD+9xRwBUBE7NvMhPO5hDOB95IeXT1H0uyImNsw3ijgs6SwMTOzFuvtiODXpL3/v46IvSPiu8CrazDtycD8iFgQES8DlwPTuhnvq8CpwIo1mLaZmQ2Q3oLgb4EngZslnStpP9Kdxc3aGlhU6V6c+70mP7piXERc29uEJM2U1Cmpc+nSpWtQgpmZ9aXHIIiIayJiOrAjcDPpURNbSvp3Se/r74wlrQOcDnyhr3Ej4pyI6IiIjjFjxvR31mZmVtHMyeIXI+LS/NvFY4F7SVcS9eVxYFyle2zu12UUsAtwi6THgL2A2T5hbGbWWmv0m8URsSzvne/XxOhzgO0kbSNpXWA6MLsyrWcjYnRETIyIicCdwNSI6Ox+cmZmVoc38uP1TYmIlcAxwA2ky02vjIgHJZ0saWpd8zUzszXTzA1lb1hEXAdc19Dvyz2Mu0+dtZiZWfdqOyIwM7PBwUFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZla4WoNA0hRJ8yTNlzSrm+GflzRX0v2SbpI0oc56zMxsdbUFgaRhwJnAAcAk4GBJkxpGuxfoiIg/A64CvllXPWZm1r06jwgmA/MjYkFEvAxcDkyrjhARN0fE8tx5JzC2xnrMzKwbdQbB1sCiSvfi3K8nRwLXdzdA0kxJnZI6ly5dOoAlmpnZWnGyWNJhQAdwWnfDI+KciOiIiI4xY8a0tjgzsyFueI3TfhwYV+kem/u9jqT9geOBd0fESzXWY2Zm3ajziGAOsJ2kbSStC0wHZldHkLQ7cDYwNSKW1FiLmZn1oLYgiIiVwDHADcBDwJUR8aCkkyVNzaOdBmwE/EDSfZJm9zA5MzOrSZ1NQ0TEdcB1Df2+XHm9f53zNzOzvq0VJ4vNzKx9HARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFa7WIJA0RdI8SfMlzepm+HqSrsjD75I0sc56zMxsdbUFgaRhwJnAAcAk4GBJkxpGOxJYFhHbAt8GTq2rHjMz616dRwSTgfkRsSAiXgYuB6Y1jDMN+H5+fRWwnyTVWJOZmTUYXuO0twYWVboXA3v2NE5ErJT0LLAF8FR1JEkzgZm58wVJ895gTaMbp10AL3MZvMxl6M8yT+hpQJ1BMGAi4hzgnP5OR1JnRHQMQEmDhpe5DF7mMtS1zHU2DT0OjKt0j839uh1H0nBgE+DpGmsyM7MGdQbBHGA7SdtIWheYDsxuGGc28LH8+sPATyMiaqzJzMwa1NY0lNv8jwFuAIYB50fEg5JOBjojYjZwHnCRpPnAH0hhUad+Ny8NQl7mMniZy1DLMss74GZmZfOdxWZmhXMQmJkVbkgGQYmPtmhimT8vaa6k+yXdJKnHa4oHi76WuTLehySFpEF/qWEzyyzpoPxdPyjp0lbXONCaWLfHS7pZ0r15/T6wHXUOFEnnS1oi6YEehkvSGfnzuF/SHv2eaUQMqX+kE9OPAG8F1gV+CUxqGOfTwFn59XTginbX3YJl3hfYIL/+VAnLnMcbBdwK3Al0tLvuFnzP2wH3Apvl7i3bXXcLlvkc4FP59STgsXbX3c9lfhewB/BAD8MPBK4HBOwF3NXfeQ7FI4ISH23R5zJHxM0RsTx33km6r2Mwa+Z7Bvgq6RlWK1pZXE2aWeajgDMjYhlARCxpcY0DrZllDmDj/HoT4IkW1jfgIuJW0lWUPZkGXBjJncCmkt7Sn3kOxSDo7tEWW/c0TkSsBLoebTFYNbPMVUeS9igGsz6XOR8yj4uIa1tZWI2a+Z63B7aXdIekOyVNaVl19WhmmU8EDpO0GLgO+ExrSmubNf1779OgeMSEDRxJhwEdwLvbXUudJK0DnA7MaHMprTac1Dy0D+mo71ZJu0bEM+0sqmYHAxdExLckvYN0b9IuEbGq3YUNFkPxiKDER1s0s8xI2h84HpgaES+1qLa69LXMo4BdgFskPUZqS509yE8YN/M9LwZmR8QrEfEo8BtSMAxWzSzzkcCVABHxc2Ak6eFsQ1VTf+9rYigGQYmPtuhzmSXtDpxNCoHB3m4MfSxzRDwbEaMjYmJETCSdF5kaEZ3tKXdANLNuX0M6GkDSaFJT0YIW1jjQmlnmhcB+AJJ2IgXB0pZW2VqzgSPy1UN7Ac9GxJP9meCQaxqKtfPRFrVqcplPAzYCfpDPiy+MiKltK7qfmlzmIaXJZb4BeJ+kucCrwLERMWiPdptc5i8A50r6B9KJ4xmDecdO0mWkMB+dz3t8BRgBEBFnkc6DHAjMB5YDH+/3PAfx52VmZgNgKDYNmZnZGnAQmJkVzkFgZlY4B4GZWeEcBGZmhXMQmJkVzkFgZla4IXdDmVk75JuA1gG2Ad4EfHoIPezOhjgfEZgNjN2ABRExGTiUdDeo2aDgO4vN+knSSNJjgcdFxApJm5N+LGQwP+zNCuIjArP+2wV4OCK6fvxmD9IvaZkNCj5HYNZ/uwHj85HBMOAk4Lj2lmTWPAeBWf/tBvwXcBfpKZFfj4g72luSWfN8jsCsnyT9DJgZEfPaXYvZG+EgMOun/Mz48f5pRBusHARmZoXzVUNmZoVzEJiZFc5BYGZWOAeBmVnhHARmZoVzEJiZFc5BYGZWuP8P51IscY/nUcYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# topn=20\n",
    "import matplotlib.pyplot as plt\n",
    "# this is the random init bert\n",
    "max_acc = [max(a) for a in accuracies]\n",
    "plt.plot(ps, max_acc, \"*\")\n",
    "plt.ylim(0,1)\n",
    "plt.title(f\"Model: bert-base-uncased MS only pretrain\")\n",
    "plt.xlabel(\"$p$\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1040a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# topn=20\n",
    "import matplotlib.pyplot as plt\n",
    "# this is the random init bert\n",
    "max_acc = [max(a) for a in accuracies]\n",
    "plt.plot(ps, max_acc, \"*\")\n",
    "plt.ylim(0,1)\n",
    "plt.title(f\"Model: {model} no-pretrain\")\n",
    "plt.xlabel(\"$p$\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2efb16",
   "metadata": {},
   "source": [
    "# Experiments: Moral Stories only pretraining\n",
    "***\n",
    "If we pretrain an LM on the original Moral Stories dataset, is it able to achieve constant performance?\n",
    "\n",
    "Todo:\n",
    "* Take a pretrained BERT for the original MS task\n",
    "* Finetune it to the other versions of the CMS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19fc23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0239bf0e",
   "metadata": {},
   "source": [
    "# Post training evaluation\n",
    "***\n",
    "Whenever your local machine loses connection to a running notebook it will not receive the console outputs anymore, even though it might be connected to the kernel.\n",
    "This is a known issue in jupyter, which will not be addressed anytime soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ailignment.training.finetune import clean_up_mem\n",
    "\n",
    "@clean_up_mem\n",
    "def evaluate(path):\n",
    "    '''\n",
    "    Loads the model in `path` and runs test evaluation on it\n",
    "    '''\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=acc_metric,\n",
    "    )\n",
    "    logs = trainer.evaluate(val_data)\n",
    "    return logs\n",
    "\n",
    "def evaluate_folder(folder):\n",
    "    '''\n",
    "    Runs `evaluate` on all checkpoints in the given folder.\n",
    "    Returns a dict {\"checkpoint-X\": log_dict, .. }\n",
    "    '''\n",
    "    ckpts = os.listdir(folder)\n",
    "    results = {ckpt:evaluate(os.path.join(folder, ckpt)) for ckpt in ckpts}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "transformers.logging.set_verbosity_warning()\n",
    "\n",
    "ps = np.linspace(0,1,11)\n",
    "test_size = 0.1\n",
    "seed = 3\n",
    "model = \"roberta-base\"\n",
    "model = \"bert-base-uncased\"\n",
    "\n",
    "acc_metric = get_accuracy_metric()\n",
    "results = []\n",
    "\n",
    "dataframe = pd.read_pickle(\"../data/moral_stories_proto_l2s.dat\")\n",
    "\n",
    "\n",
    "paths = [f\"/data/kiehne/results/shuffled_values/random/{model}/{p:.2f}/checkpoint-5625\" for p in ps]\n",
    "for p, path in zip(ps,paths):\n",
    "    random_values = dataframe.apply(simplify_norm_value, axis=1).apply(flip_norm(p), axis=1)\n",
    "    train, test = make_action_classification_dataframe(random_values)\n",
    "\n",
    "    # create training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"/data/kiehne/results/shuffled_values/random/{model}/{p:0.2f}/\",\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=500,\n",
    "        #weight_decay=0.01,\n",
    "        logging_dir='logs/',\n",
    "        log_level=\"info\",\n",
    "        logging_steps=500,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_steps=30000000,\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=1e-5\n",
    "    )\n",
    "    \n",
    "    # create data loading function\n",
    "    def data_func(tokenizer):\n",
    "        def tok(samples):\n",
    "            return tokenizer(samples[\"action\"], samples[\"norm\"], padding=\"max_length\", \n",
    "                             truncation=True, return_token_type_ids=True)\n",
    "\n",
    "        train_data = Dataset.from_pandas(train)\n",
    "        train_data = train_data.map(tok, batched=True)\n",
    "        val_data = Dataset.from_pandas(test)\n",
    "        val_data = val_data.map(tok, batched=True)\n",
    "        return train_data, val_data\n",
    "    print(ps, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15a967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
