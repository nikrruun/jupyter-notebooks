{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e069b4c",
   "metadata": {},
   "source": [
    "# Shuffling norm judgments\n",
    "***\n",
    "Does the value of a norm have impact on the classification accuracy? We test this hypothesis by applying the `Learn2Split` model and shuffling of the value judgments. In the end, we train the action classification task and compare.\n",
    "\n",
    "Shuffle strategies:\n",
    "* Truly randomn\n",
    "    * Will cause most errornous sentences of the strategies\n",
    "* Mirroring polarity: Choose the closest opposite value.\n",
    "* Single polarity: All values are the same\n",
    "    * This effectively reduces the task to NLI! Here we HAVE to be better!\n",
    "    \n",
    "    \n",
    "### Approach\n",
    "***\n",
    "The `L2S` model not only splits a norm, but also conjugates the action:\n",
    "* `It's wrong to become addicted to gambling.` becomes\n",
    "    * `It's wrong`\n",
    "    * `becoming addicted to gambling`\n",
    "\n",
    "Since the splitting is not reversible as is, there are two possible options to consider:\n",
    "1. Re-run the original model on concatenated `L2S` output. If the accuracy does not drop, then we can simply do our comparison experiments on `L2S` concat'ed. \n",
    "2. Train a `L2S` reverse model (*sigh*)\n",
    "\n",
    "##### ...and then:\n",
    "Train the action classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e74d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "\n",
    "from ailignment.datasets.util import get_accuracy_metric\n",
    "from ailignment.datasets.moral_stories import make_action_classification_dataframe, get_random_value_dataset\n",
    "import ailignment.datasets.moral_stories_clustered as msc\n",
    "from ailignment.training import sequence_classification\n",
    "\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "dataframe = pd.read_pickle(\"../data/moral_stories_proto_l2s.dat\")\n",
    "\n",
    "# a function to convert train and test dataframes to huggingface datasets\n",
    "def convert_hf_dataset(tokenizer, train, test, padding=\"max_length\"):\n",
    "    def tok(samples):\n",
    "        return tokenizer(samples[\"action\"], samples[\"norm\"], padding=padding, \n",
    "                         truncation=True, return_token_type_ids=True)\n",
    "\n",
    "    train_data = Dataset.from_pandas(train)\n",
    "    train_data = train_data.map(tok, batched=True)\n",
    "    val_data = Dataset.from_pandas(test)\n",
    "    val_data = val_data.map(tok, batched=True)\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0b69df",
   "metadata": {},
   "source": [
    "### Action classification on `L2S` concatenated\n",
    "***\n",
    "Question: If we concatenate the norm judgment with the conjugated norm action, does the accuracy change?\n",
    "\n",
    "The task was run for 5 epochs on the `roberta-large` model:\n",
    "* Epoch 5: 0.913 accuracy\n",
    "\n",
    "Although not a thorough analysis in any capacity, we argue that the performance is not significantly reduced by the simplistic concatenation approach. The upcoming experiments on the shuffled datasets can be carried out without greater worries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataframe.copy()\n",
    "data[\"norm\"] = data[\"norm_value\"] + \" \" + data[\"norm_action\"]\n",
    "train, test = make_action_classification_dataframe(data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd16cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"roberta-large\"\n",
    "#name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "\n",
    "train_data, val_data = convert_hf_dataset(tokenizer, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d731ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data/kiehne/results/shuffled_values/verification/\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='logs/',\n",
    "    log_level=\"info\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=30000000,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5\n",
    "    \n",
    ")\n",
    "acc_metric = get_accuracy_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a80d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics=acc_metric,\n",
    ")\n",
    "logs = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99934653",
   "metadata": {},
   "source": [
    "# Experiments: Single model training\n",
    "***\n",
    "Pick a dataset, then run the finetuning. Useful for explorative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42171a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_pickle(\"../data/moral_stories_random_values.dat\")\n",
    "dataframe = msc.assign_norm_clusters(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d321b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = msc.get_random_value_dataset(dataframe, 0.5, top_n=20)\n",
    "train, test = msc.make_action_classification_dataframe(data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"roberta-large\"\n",
    "name = \"bert-base-uncased\"\n",
    "#name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
    "\n",
    "train_data, val_data = convert_hf_dataset(tokenizer, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4241597",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data/kiehne/results/shuffled_values/random/single_runs/\",\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=500,\n",
    "    #weight_decay=0.01,\n",
    "    logging_dir='logs/',\n",
    "    log_level=\"info\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=30000000,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5\n",
    ")\n",
    "acc_metric = get_accuracy_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d755951",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics=acc_metric,\n",
    ")\n",
    "logs = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339f393",
   "metadata": {},
   "source": [
    "# Experiments: Consecutive models\n",
    "***\n",
    "Run several models over different training data or with different hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ed405",
   "metadata": {},
   "source": [
    "### Random values experiment\n",
    "***\n",
    "Given different values of $p$, how is the accuracy affected? \n",
    "\n",
    "First run:\n",
    "* Single epoch, batch size 8, $p\\in\\{0,0.1,0.2,..,1\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95dc783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a4bc0cc6ca47308fa0f518b28496b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e2a26b3d714440875b70806c8a4e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b4119caec345cda66e6e2cd8c9ec34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: ID, flipped, norm_sentiment, norm_value, cluster, norm, __index_level_0__, consequence, actor_name, situation, l2s_output, action, norm_storyfied, intention, norm_action.\n",
      "***** Running training *****\n",
      "  Num examples = 21618\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6760\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4086' max='6760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4086/6760 27:30 < 18:00, 2.47 it/s, Epoch 3.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693400</td>\n",
       "      <td>0.615362</td>\n",
       "      <td>0.657961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.591300</td>\n",
       "      <td>0.624802</td>\n",
       "      <td>0.650379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.540900</td>\n",
       "      <td>0.566139</td>\n",
       "      <td>0.717776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: ID, flipped, norm_sentiment, norm_value, cluster, norm, __index_level_0__, consequence, actor_name, situation, l2s_output, action, norm_storyfied, intention, norm_action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2374\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-base-uncased/0.00/checkpoint-1352\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-base-uncased/0.00/checkpoint-1352/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-base-uncased/0.00/checkpoint-1352/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: ID, flipped, norm_sentiment, norm_value, cluster, norm, __index_level_0__, consequence, actor_name, situation, l2s_output, action, norm_storyfied, intention, norm_action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2374\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data/kiehne/results/shuffled_values/random_no_pretrain/bert-base-uncased/0.00/checkpoint-4056\n",
      "Configuration saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-base-uncased/0.00/checkpoint-4056/config.json\n",
      "Model weights saved in /data/kiehne/results/shuffled_values/random_no_pretrain/bert-base-uncased/0.00/checkpoint-4056/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "transformers.logging.set_verbosity_warning()\n",
    "\n",
    "ps = np.linspace(0,1,11)\n",
    "test_size = 0.1\n",
    "model = \"roberta-base\"\n",
    "model = \"roberta-large\"\n",
    "\n",
    "model = \"bert-base-uncased\"\n",
    "#model = \"bert-large-uncased\"\n",
    "\n",
    "#model = \"albert-xxlarge-v2\"\n",
    "#model = \"albert-base-v2\"\n",
    "\n",
    "dataframe = pd.read_pickle(\"../data/moral_stories_proto_l2s.dat\")\n",
    "dataframe = msc.assign_norm_clusters(dataframe)\n",
    "\n",
    "acc_metric = get_accuracy_metric()\n",
    "results = []\n",
    "\n",
    "for p in ps:\n",
    "    # create dataset\n",
    "    random_values = msc.get_random_value_dataset(dataframe, p, top_n=20)\n",
    "    train, test = msc.make_action_classification_dataframe(random_values, test_size=test_size)\n",
    "\n",
    "    # create training args\n",
    "    out_dir = f\"/data/kiehne/results/shuffled_values/random_no_pretrain/{model}/{p:0.2f}/\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=500,\n",
    "        #weight_decay=0.01,\n",
    "        logging_steps=500,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_steps=30000000,\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=1e-5\n",
    "    )\n",
    "    \n",
    "    # create data loading function\n",
    "    data_loader = lambda tokenizer: convert_hf_dataset(tokenizer, train, test)\n",
    "    if \"gpt2\" in model:\n",
    "        data_loader = lambda tokenizer: convert_hf_dataset(tokenizer, train, test, padding=\"do_not_pad\")\n",
    "\n",
    "    \n",
    "    r = sequence_classification(data_loader, model, training_args, acc_metric, use_pretrained=False)\n",
    "    results.append(r)\n",
    "    \n",
    "    # save training data for later evaluation\n",
    "    train.to_pickle(out_dir+\"train.dat\")\n",
    "    test.to_pickle(out_dir+\"test.dat\")\n",
    "\n",
    "\n",
    "accuracies = [[y[\"eval_accuracy\"] for y in x if \"eval_accuracy\" in y] for x in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1040a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# topn=20\n",
    "import matplotlib.pyplot as plt\n",
    "# this is the random init bert\n",
    "max_acc = [max(a) for a in accuracies]\n",
    "plt.plot(ps, max_acc, \"*\")\n",
    "plt.ylim(0,1)\n",
    "plt.title(f\"Model: {model} no-pretrain\")\n",
    "plt.xlabel(\"$p$\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0239bf0e",
   "metadata": {},
   "source": [
    "# Post training evaluation\n",
    "***\n",
    "Whenever your local machine loses connection to a running notebook it will not receive the console outputs anymore, even though it might be connected to the kernel.\n",
    "This is a known issue in jupyter, which will not be addressed anytime soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ailignment.training.finetune import clean_up_mem\n",
    "\n",
    "@clean_up_mem\n",
    "def evaluate(path):\n",
    "    '''\n",
    "    Loads the model in `path` and runs test evaluation on it\n",
    "    '''\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=acc_metric,\n",
    "    )\n",
    "    logs = trainer.evaluate(val_data)\n",
    "    return logs\n",
    "\n",
    "def evaluate_folder(folder):\n",
    "    '''\n",
    "    Runs `evaluate` on all checkpoints in the given folder.\n",
    "    Returns a dict {\"checkpoint-X\": log_dict, .. }\n",
    "    '''\n",
    "    ckpts = os.listdir(folder)\n",
    "    results = {ckpt:evaluate(os.path.join(folder, ckpt)) for ckpt in ckpts}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "transformers.logging.set_verbosity_warning()\n",
    "\n",
    "ps = np.linspace(0,1,11)\n",
    "test_size = 0.1\n",
    "seed = 3\n",
    "model = \"roberta-base\"\n",
    "model = \"bert-base-uncased\"\n",
    "\n",
    "acc_metric = get_accuracy_metric()\n",
    "results = []\n",
    "\n",
    "dataframe = pd.read_pickle(\"../data/moral_stories_proto_l2s.dat\")\n",
    "\n",
    "\n",
    "paths = [f\"/data/kiehne/results/shuffled_values/random/{model}/{p:.2f}/checkpoint-5625\" for p in ps]\n",
    "for p, path in zip(ps,paths):\n",
    "    random_values = dataframe.apply(simplify_norm_value, axis=1).apply(flip_norm(p), axis=1)\n",
    "    train, test = make_action_classification_dataframe(random_values)\n",
    "\n",
    "    # create training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"/data/kiehne/results/shuffled_values/random/{model}/{p:0.2f}/\",\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=500,\n",
    "        #weight_decay=0.01,\n",
    "        logging_dir='logs/',\n",
    "        log_level=\"info\",\n",
    "        logging_steps=500,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_steps=30000000,\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=1e-5\n",
    "    )\n",
    "    \n",
    "    # create data loading function\n",
    "    def data_func(tokenizer):\n",
    "        def tok(samples):\n",
    "            return tokenizer(samples[\"action\"], samples[\"norm\"], padding=\"max_length\", \n",
    "                             truncation=True, return_token_type_ids=True)\n",
    "\n",
    "        train_data = Dataset.from_pandas(train)\n",
    "        train_data = train_data.map(tok, batched=True)\n",
    "        val_data = Dataset.from_pandas(test)\n",
    "        val_data = val_data.map(tok, batched=True)\n",
    "        return train_data, val_data\n",
    "    print(ps, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15a967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
