{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614d78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "499392ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_pickle(\"../data/moral_stories_proto_light.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b15e098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitch together moral and immoral norms\n",
    "moral_df = dataframe[[\"moral_action\", \"norm_story\"]].copy()\n",
    "immoral_df = dataframe[[\"immoral_action\", \"norm_story\"]].copy()\n",
    "moral_df.rename(columns={\"moral_action\":\"action\"}, inplace=True)\n",
    "immoral_df.rename(columns={\"immoral_action\":\"action\"}, inplace=True)\n",
    "moral_df[\"label\"] = 1\n",
    "immoral_df[\"label\"] = 0\n",
    "moral_df[\"sentiment\"] = dataframe[\"norm_sentiment\"].apply(lambda x: int(x==\"POSITIVE\"))\n",
    "immoral_df[\"sentiment\"] = dataframe[\"norm_sentiment\"].apply(lambda x: int(x==\"POSITIVE\"))\n",
    "\n",
    "data = pd.concat([moral_df, immoral_df], ignore_index=True)\n",
    "#data = immoral_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c58d11d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/cross-encoder/nli-distilroberta-base/resolve/main/config.json from cache at C:\\Users\\kiehne/.cache\\huggingface\\transformers\\7f9bded27e75864e85c373d68a16d8472076fb5fa77327c8b7f4602d3d277730.6aff37823e5626e1269f292e8316f0a5560a297862b63a5d062c0b4b21edebd2\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"entailment\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 1,\n",
      "    \"neutral\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/cross-encoder/nli-distilroberta-base/resolve/main/vocab.json from cache at C:\\Users\\kiehne/.cache\\huggingface\\transformers\\c8675ad34ebb0e4c9b17eb7257b29ec0e583cc874ace15660c2bf690177f6b71.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
      "loading file https://huggingface.co/cross-encoder/nli-distilroberta-base/resolve/main/merges.txt from cache at C:\\Users\\kiehne/.cache\\huggingface\\transformers\\52297921694b561cb2225bab5ad4b8b50c1b101657678d5e1385a2a5506d9d8c.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/cross-encoder/nli-distilroberta-base/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/cross-encoder/nli-distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/cross-encoder/nli-distilroberta-base/resolve/main/special_tokens_map.json from cache at C:\\Users\\kiehne/.cache\\huggingface\\transformers\\820e5b4505679726273564732508f5fb05d62b06b37750732d52562357144944.cb2244924ab24d706b02fd7fcedaea4531566537687a539ebb94db511fd122a0\n",
      "loading file https://huggingface.co/cross-encoder/nli-distilroberta-base/resolve/main/tokenizer_config.json from cache at C:\\Users\\kiehne/.cache\\huggingface\\transformers\\5a2483b730dffcbae22c1fc66edb773084510a5e52145d928216b83fb4c56445.024cc07195c0ba0b51d4f80061c6115996ff26233f3d04788855b23cdf13fbd5\n",
      "loading configuration file https://huggingface.co/cross-encoder/nli-distilroberta-base/resolve/main/config.json from cache at C:\\Users\\kiehne/.cache\\huggingface\\transformers\\7f9bded27e75864e85c373d68a16d8472076fb5fa77327c8b7f4602d3d277730.6aff37823e5626e1269f292e8316f0a5560a297862b63a5d062c0b4b21edebd2\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"entailment\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 1,\n",
      "    \"neutral\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/cross-encoder/nli-distilroberta-base/resolve/main/config.json from cache at C:\\Users\\kiehne/.cache\\huggingface\\transformers\\7f9bded27e75864e85c373d68a16d8472076fb5fa77327c8b7f4602d3d277730.6aff37823e5626e1269f292e8316f0a5560a297862b63a5d062c0b4b21edebd2\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"entailment\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 1,\n",
      "    \"neutral\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/cross-encoder/nli-distilroberta-base/resolve/main/config.json from cache at C:\\Users\\kiehne/.cache\\huggingface\\transformers\\7f9bded27e75864e85c373d68a16d8472076fb5fa77327c8b7f4602d3d277730.6aff37823e5626e1269f292e8316f0a5560a297862b63a5d062c0b4b21edebd2\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"contradiction\",\n",
      "    \"1\": \"entailment\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 0,\n",
      "    \"entailment\": 1,\n",
      "    \"neutral\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/cross-encoder/nli-distilroberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\kiehne/.cache\\huggingface\\transformers\\35ae319c2c9242ab86db084d12bc72cc945224b772276dfc8a0215974831a02b.847b12adafbcddc0c3e46b96bf470c181ca43a2653a1b2da5ab7d8dbae6e53d4\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at cross-encoder/nli-distilroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load the NLI model and its tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "#name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\" # 81.5%\n",
    "name = 'cross-encoder/nli-distilroberta-base' # 80%\n",
    "#name = \"boychaboy/SNLI_bert-base-uncased\" # 75%\n",
    "#name =\"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9e4a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26de744631d745748b2b8fd3726b46f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert the dataframe to a huggingface dataset and tokenize the sentences\n",
    "from datasets import Dataset\n",
    "\n",
    "def tok(samples):\n",
    "    return tokenizer(samples[\"action\"], samples[\"norm_story\"], padding=\"max_length\", \n",
    "                     truncation=True, return_token_type_ids=True)\n",
    "\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.map(tok, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d03c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "646835ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentiment, action, norm_story.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 16890\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='528' max='528' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [528/528 03:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35748963883955004\n"
     ]
    }
   ],
   "source": [
    "# run evaluation\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results/\",\n",
    "    num_train_epochs=0,              # total number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=50,                # how often to log\n",
    "    evaluation_strategy=\"epoch\",     # when to run evaluation\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    ")\n",
    "results = trainer.predict(eval_set)\n",
    "scores = torch.softmax(torch.from_numpy(results.predictions),1).numpy()\n",
    "\n",
    "is_entailed = (scores[:,0] > scores[:,2]).astype(\"int32\")\n",
    "labels = np.array(eval_set[\"label\"])\n",
    "sentiment = np.array(eval_set[\"sentiment\"])\n",
    "y_pred = is_entailed == sentiment\n",
    "\n",
    "data[\"y_pred\"] = y_pred\n",
    "data[\"is_entailed\"] = is_entailed\n",
    "misclassed = data[y_pred != labels]\n",
    "\n",
    "acc = (y_pred == labels).mean()\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b6dd00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6894612196566016\n"
     ]
    }
   ],
   "source": [
    "is_entailed = (scores[:,1] > scores[:,0]).astype(\"int32\")\n",
    "labels = np.array(eval_set[\"label\"])\n",
    "sentiment = np.array(eval_set[\"sentiment\"])\n",
    "y_pred = is_entailed == sentiment\n",
    "\n",
    "acc = (y_pred == labels).mean()\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f2a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some mis classified samples\n",
    "misclassed.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700bc6a3",
   "metadata": {},
   "source": [
    "### Running a classifier on the NLI scores\n",
    "***\n",
    "Are there better decision boundaries than $P(entailment)>P(contradiction)$?\n",
    "* So far: No standard ML classifier was better than our simple rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9381bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test, whether a classifier improves the performance\n",
    "\n",
    "x = np.concatenate([scores, sentiment[:,np.newaxis]], axis=1).copy()\n",
    "# shuffling\n",
    "index = np.arange(len(x))\n",
    "np.random.shuffle(index)\n",
    "x = x[index]\n",
    "y = labels[index]\n",
    "\n",
    "v = 0.1\n",
    "n = int(len(x)*v)\n",
    "\n",
    "x_train, y_train = x[n:], y[n:]\n",
    "x_test, y_test = x[:n], y[:n]\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22511c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, ensemble\n",
    "cl = svm.SVC(C=2, kernel=\"rbf\")\n",
    "cl.fit(x_train, y_train)\n",
    "y_pred = cl.predict(x_test)\n",
    "print((y_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4011ab",
   "metadata": {},
   "source": [
    "##  Testing the prototype with finetuning\n",
    "***\n",
    "Goal: Finetune the NLI models on our norm-stories. The task is to learn, which norm and action pair are entailing or contradicting.\n",
    "\n",
    "The following pairs of matches are possible:\n",
    "* moral action + incentivizing norm: we want entailment\n",
    "* moral action + prohibiting norm: we want contradiction\n",
    "* immoral action + incentivizing norm: we want contradiction\n",
    "* immoral action + prohibiting norm: we want entailment\n",
    "\n",
    "Unless the performance is significantly higher than in the original paper, this experiment is rather pointless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bdd9ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>norm_story</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jack honestly points out the flaws he had with his ex-wife.</td>\n",
       "      <td>Jack tells his ex the truth about his feelings</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jim tells Carrie that her new hair really shows off her cheekbones.</td>\n",
       "      <td>Jim compliments someone when they get a new look</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kyle goes long distance running with the dog regularly.</td>\n",
       "      <td>Kyle is responsible with his pets</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nina calls the college to schedule a private tour so that she can attend her friend's wedding.</td>\n",
       "      <td>Nina is there for her friend at their wedding</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob explains that the radio is making work difficult for Bob and asks his coworker to turn it down or wear headphones.</td>\n",
       "      <td>Bob tries to get along with his coworkers</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16885</th>\n",
       "      <td>Bobby ignores Joseph to keep studying and get his homework done early.</td>\n",
       "      <td>Bobby gives his buddy a high five</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16886</th>\n",
       "      <td>Perry tells his doctor that he has chronic back issues in order to get a prescription for pain medicine.</td>\n",
       "      <td>Perry lies about medical conditions</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16887</th>\n",
       "      <td>Sammy tells Claire that she can't go to dinner because she's caring for her sick grandmother.</td>\n",
       "      <td>Sammy lies about his life</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16888</th>\n",
       "      <td>Brian holds the door shut so Kai can't get out easily.</td>\n",
       "      <td>Brian traps or set up another person</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16889</th>\n",
       "      <td>Rich tells Ron that he didn't know that he had a retarded brother.</td>\n",
       "      <td>Rich uses the word retarded anymore</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16890 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       action  \\\n",
       "0                                                                 Jack honestly points out the flaws he had with his ex-wife.   \n",
       "1                                                         Jim tells Carrie that her new hair really shows off her cheekbones.   \n",
       "2                                                                     Kyle goes long distance running with the dog regularly.   \n",
       "3                              Nina calls the college to schedule a private tour so that she can attend her friend's wedding.   \n",
       "4      Bob explains that the radio is making work difficult for Bob and asks his coworker to turn it down or wear headphones.   \n",
       "...                                                                                                                       ...   \n",
       "16885                                                  Bobby ignores Joseph to keep studying and get his homework done early.   \n",
       "16886                Perry tells his doctor that he has chronic back issues in order to get a prescription for pain medicine.   \n",
       "16887                           Sammy tells Claire that she can't go to dinner because she's caring for her sick grandmother.   \n",
       "16888                                                                  Brian holds the door shut so Kai can't get out easily.   \n",
       "16889                                                      Rich tells Ron that he didn't know that he had a retarded brother.   \n",
       "\n",
       "                                             norm_story  label  sentiment  \n",
       "0        Jack tells his ex the truth about his feelings      1          1  \n",
       "1      Jim compliments someone when they get a new look      1          1  \n",
       "2                     Kyle is responsible with his pets      1          1  \n",
       "3         Nina is there for her friend at their wedding      1          1  \n",
       "4             Bob tries to get along with his coworkers      1          1  \n",
       "...                                                 ...    ...        ...  \n",
       "16885                 Bobby gives his buddy a high five      0          1  \n",
       "16886               Perry lies about medical conditions      1          0  \n",
       "16887                         Sammy lies about his life      1          0  \n",
       "16888              Brian traps or set up another person      1          0  \n",
       "16889               Rich uses the word retarded anymore      1          0  \n",
       "\n",
       "[16890 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the labels need to be adjusted for the nli task according to the above cases\n",
    "data_nli = data.copy()\n",
    "# maps entailment (True) or contradiction (False) to class indices of the model\n",
    "class_map = {True: 1, False: 0}\n",
    "data_nli[\"label\"] = (data[\"sentiment\"] == data[\"label\"]).apply(class_map.get)\n",
    "data_nli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "821004fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run evaluation\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results/\",\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=50,                # how often to log\n",
    "    evaluation_strategy=\"epoch\",     # when to run evaluation\n",
    ")\n",
    "acc_metric = get_accuracy_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc955e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data_nli, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "777b6e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effc9423ccff44c78c35d15de4bdcb27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1b9bd2030046ffa3449d07060d5beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split into val data\n",
    "# convert the dataframe to a huggingface dataset and tokenize the sentences\n",
    "from datasets import Dataset\n",
    "\n",
    "def tok(samples):\n",
    "    return tokenizer(samples[\"action\"], samples[\"norm_story\"], padding=\"max_length\", \n",
    "                     truncation=True, return_token_type_ids=True)\n",
    "\n",
    "train_data = Dataset.from_pandas(train)\n",
    "train_data = train_data.map(tok, batched=True)\n",
    "val_data = Dataset.from_pandas(test)\n",
    "val_data = val_data.map(tok, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7661f",
   "metadata": {},
   "source": [
    "# Testing the prototype without finetuning\n",
    "***\n",
    "So far, we obtained several situations of people actually performing the norm, which we call `norm-story`. Now, we'd like to test, whether natural language inference (NLI), or more specifically, textual entailment models are able to tell apart the moral action from the immoral ones:\n",
    "* Norm: *It's manipulative to try to force a partner into marriage.*\n",
    "* Norm-story: *Jake tries to force a partner into marriage*\n",
    "* Moral action: *Jake proposes to Harry at the bar they met at.*\n",
    "* Immoral action: *Jake tells Harry that he will kill himself if he doesn't marry him.*\n",
    "\n",
    "General idea: If an action $A$ entails the norm-story $A_N$ we assume that the actor also performed $A_N$ and is therefore exposed to the value-judgement of the norm. Continuing the above example:\n",
    "* If we find that $A=$*Jake proposes to Harry at the bar they met at.* is a sufficient condition for the statement $A_N=$*Jake proposes to Harry at the bar they met at.*, then we assign the value *manipulative* to $A$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f2bac09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: norm_story, sentiment, __index_level_0__, action.\n",
      "***** Running training *****\n",
      "  Num examples = 13512\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8445\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8445' max='8445' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8445/8445 52:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.493100</td>\n",
       "      <td>0.455604</td>\n",
       "      <td>0.795145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.468600</td>\n",
       "      <td>0.589875</td>\n",
       "      <td>0.792185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.331600</td>\n",
       "      <td>0.840236</td>\n",
       "      <td>0.804026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>1.072576</td>\n",
       "      <td>0.804322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1.187049</td>\n",
       "      <td>0.811723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-500\n",
      "Configuration saved in results/checkpoint-500\\config.json\n",
      "Model weights saved in results/checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-1000\n",
      "Configuration saved in results/checkpoint-1000\\config.json\n",
      "Model weights saved in results/checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-1500\n",
      "Configuration saved in results/checkpoint-1500\\config.json\n",
      "Model weights saved in results/checkpoint-1500\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: norm_story, sentiment, __index_level_0__, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3378\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-2000\n",
      "Configuration saved in results/checkpoint-2000\\config.json\n",
      "Model weights saved in results/checkpoint-2000\\pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-2500\n",
      "Configuration saved in results/checkpoint-2500\\config.json\n",
      "Model weights saved in results/checkpoint-2500\\pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-3000\n",
      "Configuration saved in results/checkpoint-3000\\config.json\n",
      "Model weights saved in results/checkpoint-3000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: norm_story, sentiment, __index_level_0__, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3378\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-3500\n",
      "Configuration saved in results/checkpoint-3500\\config.json\n",
      "Model weights saved in results/checkpoint-3500\\pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-4000\n",
      "Configuration saved in results/checkpoint-4000\\config.json\n",
      "Model weights saved in results/checkpoint-4000\\pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-4500\n",
      "Configuration saved in results/checkpoint-4500\\config.json\n",
      "Model weights saved in results/checkpoint-4500\\pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-5000\n",
      "Configuration saved in results/checkpoint-5000\\config.json\n",
      "Model weights saved in results/checkpoint-5000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: norm_story, sentiment, __index_level_0__, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3378\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-5500\n",
      "Configuration saved in results/checkpoint-5500\\config.json\n",
      "Model weights saved in results/checkpoint-5500\\pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-6000\n",
      "Configuration saved in results/checkpoint-6000\\config.json\n",
      "Model weights saved in results/checkpoint-6000\\pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-6500\n",
      "Configuration saved in results/checkpoint-6500\\config.json\n",
      "Model weights saved in results/checkpoint-6500\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: norm_story, sentiment, __index_level_0__, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3378\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to results/checkpoint-7000\n",
      "Configuration saved in results/checkpoint-7000\\config.json\n",
      "Model weights saved in results/checkpoint-7000\\pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-7500\n",
      "Configuration saved in results/checkpoint-7500\\config.json\n",
      "Model weights saved in results/checkpoint-7500\\pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-8000\n",
      "Configuration saved in results/checkpoint-8000\\config.json\n",
      "Model weights saved in results/checkpoint-8000\\pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: norm_story, sentiment, __index_level_0__, action.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3378\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8445, training_loss=0.3130043581436907, metrics={'train_runtime': 3160.1887, 'train_samples_per_second': 21.378, 'train_steps_per_second': 2.672, 'total_flos': 8949657054781440.0, 'train_loss': 0.3130043581436907, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_data,   # training dataset\n",
    "    eval_dataset=val_data,     # evaluation dataset\n",
    "    compute_metrics=acc_metric,     # code to run accuracy metric\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a0136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
