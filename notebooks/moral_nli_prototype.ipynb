{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d7661f",
   "metadata": {},
   "source": [
    "# Testing the prototype without finetuning\n",
    "***\n",
    "So far, we obtained several situations of people actually performing the norm, which we call `norm-story`. Now, we'd like to test, whether natural language inference (NLI), or more specifically, textual entailment models are able to tell apart the moral action from the immoral ones:\n",
    "* Norm: *It's manipulative to try to force a partner into marriage.*\n",
    "* Norm-story: *Jake tries to force a partner into marriage*\n",
    "* Moral action: *Jake proposes to Harry at the bar they met at.*\n",
    "* Immoral action: *Jake tells Harry that he will kill himself if he doesn't marry him.*\n",
    "\n",
    "General idea: If an action $A$ entails the norm-story $A_N$ we assume that the actor also performed $A_N$ and is therefore exposed to the value-judgement of the norm. Continuing the above example:\n",
    "* If we find that $A=$*Jake proposes to Harry at the bar they met at.* is a sufficient condition for the statement $A_N=$*Jake proposes to Harry at the bar they met at.*, then we assign the value *manipulative* to $A$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614d78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', 400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499392ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_pickle(\"../data/moral_stories_proto_l2s.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15e098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitch together moral and immoral norms\n",
    "story_col = \"norm_storyfied\"\n",
    "moral_df = dataframe[[\"moral_action\", story_col]].copy()\n",
    "immoral_df = dataframe[[\"immoral_action\", story_col]].copy()\n",
    "moral_df.rename(columns={\"moral_action\":\"action\"}, inplace=True)\n",
    "immoral_df.rename(columns={\"immoral_action\":\"action\"}, inplace=True)\n",
    "moral_df[\"label\"] = 1\n",
    "immoral_df[\"label\"] = 0\n",
    "moral_df[\"sentiment\"] = dataframe[\"norm_sentiment\"].apply(lambda x: int(x==\"POSITIVE\"))\n",
    "immoral_df[\"sentiment\"] = dataframe[\"norm_sentiment\"].apply(lambda x: int(x==\"POSITIVE\"))\n",
    "\n",
    "data = pd.concat([moral_df, immoral_df], ignore_index=True)\n",
    "#data = immoral_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c19dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#data, data_val = train_test_split(data, train_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c58d11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the NLI model and its tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\" # 79.5%\n",
    "#name = \"cross-encoder/nli-distilroberta-base\" # 80%\n",
    "#name = \"boychaboy/SNLI_bert-base-uncased\" # 75%\n",
    "checkpoint = \"/data/kiehne/results/checkpoint-13495/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c9e4a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83ad9f08ab242b2b3df888d9fd4c042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert the dataframe to a huggingface dataset and tokenize the sentences\n",
    "from datasets import Dataset\n",
    "\n",
    "def tok(samples):\n",
    "    return tokenizer(samples[\"action\"], samples[story_col], padding=\"max_length\", \n",
    "                     truncation=True, return_token_type_ids=True)\n",
    "\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.map(tok, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7c38ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run evaluation\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results/\",\n",
    "    num_train_epochs=0,              # total number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=50,                # how often to log\n",
    "    evaluation_strategy=\"epoch\",     # when to run evaluation\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646835ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: norm_storyfied, action, sentiment.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 23992\n",
      "  Batch size = 64\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 08:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9821190396798933\n"
     ]
    }
   ],
   "source": [
    "results = trainer.predict(dataset)\n",
    "scores = torch.softmax(torch.from_numpy(results.predictions),1).numpy()\n",
    "\n",
    "is_entailed = (scores[:,0] > scores[:,2]).astype(\"int32\")\n",
    "labels = np.array(dataset[\"label\"])\n",
    "sentiment = np.array(dataset[\"sentiment\"])\n",
    "y_pred = (is_entailed == sentiment).astype(\"int32\")\n",
    "\n",
    "data[\"y_pred\"] = y_pred\n",
    "data[\"is_entailed\"] = is_entailed\n",
    "data = pd.concat([data, pd.DataFrame(scores,columns=[\"entailment\",\"neutral\",\"contradiction\"], index=data.index)],axis=1)\n",
    "misclassed = data[y_pred != labels]\n",
    "\n",
    "acc = (y_pred == labels).mean()\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f2a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some mis classified samples\n",
    "temp = data.join(misclassed[misclassed.columns[-5:]],how=\"inner\")\n",
    "temp[\"temp_ind\"] = temp.index\n",
    "temp[\"temp_ind\"] = temp[\"temp_ind\"].apply(lambda x: x if x<11996 else x-11996)\n",
    "cols = [\"norm\",\"l2s_output\",\"norm_action\",story_col, \"norm_value\",\"norm_sentiment\"]\n",
    "misclassed_frame = temp[temp.columns[2:]].join(dataframe[cols], on=\"temp_ind\")\n",
    "misclassed_frame.drop(\"temp_ind\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ded04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassed_frame.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700bc6a3",
   "metadata": {},
   "source": [
    "### Running a classifier on the NLI scores\n",
    "***\n",
    "Are there better decision boundaries than $P(entailment)>P(contradiction)$?\n",
    "* So far: No standard ML classifier was better than our simple rule\n",
    "* On few occasions, an SVM improved the results by 0.5-1% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9381bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test, whether a classifier improves the performance\n",
    "\n",
    "x = np.concatenate([scores, sentiment[:,np.newaxis]], axis=1).copy()\n",
    "# shuffling\n",
    "index = np.arange(len(x))\n",
    "np.random.shuffle(index)\n",
    "x = x[index]\n",
    "y = labels[index]\n",
    "\n",
    "v = 0.1\n",
    "n = int(len(x)*v)\n",
    "\n",
    "x_train, y_train = x[n:], y[n:]\n",
    "x_test, y_test = x[:n], y[:n]\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22511c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, ensemble\n",
    "cl = svm.SVC(C=2, kernel=\"rbf\")\n",
    "cl.fit(x_train, y_train)\n",
    "y_pred = cl.predict(x_test)\n",
    "print((y_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4011ab",
   "metadata": {},
   "source": [
    "##  Testing the prototype with finetuning\n",
    "***\n",
    "Goal: Finetune the NLI models on our norm-stories. The task is to learn, which norm and action pair are entailing or contradicting.\n",
    "\n",
    "The following pairs of matches are possible:\n",
    "* moral action + incentivizing norm: we want entailment\n",
    "* moral action + prohibiting norm: we want contradiction\n",
    "* immoral action + incentivizing norm: we want contradiction\n",
    "* immoral action + prohibiting norm: we want entailment\n",
    "\n",
    "Unless the performance is significantly higher than in the original paper, this experiment is rather pointless.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bdd9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the labels need to be adjusted for the nli task according to the above cases\n",
    "data_nli = data.copy()\n",
    "# maps entailment (True) or contradiction (False) to class indices of the model\n",
    "class_map = {True: 0, False: 2}\n",
    "data_nli[\"label\"] = (data[\"sentiment\"] == data[\"label\"]).apply(class_map.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc955e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data_nli, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1cab946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the NLI model and its tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\" # 79.5%\n",
    "#name = \"cross-encoder/nli-distilroberta-base\" # 80%\n",
    "name = \"boychaboy/SNLI_bert-base-uncased\" # 75%\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "777b6e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7adb73dd7c47d4b11dfe40d2acea84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e034694b8f476380a87a9e7a9afe79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split into val data\n",
    "# convert the dataframe to a huggingface dataset and tokenize the sentences\n",
    "from datasets import Dataset\n",
    "\n",
    "def tok(samples):\n",
    "    return tokenizer(samples[\"action\"], samples[story_col], padding=\"max_length\", \n",
    "                     truncation=True, return_token_type_ids=True)\n",
    "\n",
    "train_data = Dataset.from_pandas(train)\n",
    "train_data = train_data.map(tok, batched=True)\n",
    "val_data = Dataset.from_pandas(test)\n",
    "val_data = val_data.map(tok, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "821004fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run evaluation\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from ailignment.datasets.util import get_accuracy_metric\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data/kiehne/results/bert-snli/\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='logs/',\n",
    "    log_level=\"info\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=30000000,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5\n",
    "    \n",
    ")\n",
    "acc_metric = get_accuracy_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f2bac09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentiment, norm_storyfied, action, __index_level_0__.\n",
      "***** Running training *****\n",
      "  Num examples = 21592\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6750\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4727' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4727/6750 35:02 < 15:00, 2.25 it/s, Epoch 3.50/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.537900</td>\n",
       "      <td>0.504042</td>\n",
       "      <td>0.753750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.524654</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.307800</td>\n",
       "      <td>0.638413</td>\n",
       "      <td>0.766250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentiment, norm_storyfied, action, __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data/kiehne/results/bert-snli/checkpoint-1350\n",
      "Configuration saved in /data/kiehne/results/bert-snli/checkpoint-1350/config.json\n",
      "Model weights saved in /data/kiehne/results/bert-snli/checkpoint-1350/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentiment, norm_storyfied, action, __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data/kiehne/results/bert-snli/checkpoint-2700\n",
      "Configuration saved in /data/kiehne/results/bert-snli/checkpoint-2700/config.json\n",
      "Model weights saved in /data/kiehne/results/bert-snli/checkpoint-2700/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentiment, norm_storyfied, action, __index_level_0__.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2400\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /data/kiehne/results/bert-snli/checkpoint-4050\n",
      "Configuration saved in /data/kiehne/results/bert-snli/checkpoint-4050/config.json\n",
      "Model weights saved in /data/kiehne/results/bert-snli/checkpoint-4050/pytorch_model.bin\n",
      "/home/kiehne/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19681/3263358945.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macc_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1282\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1805\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/jupyter-notebooks/env/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics=acc_metric,\n",
    ")\n",
    "logs = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.predict(train_data)\n",
    "scores = torch.softmax(torch.from_numpy(results.predictions),1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4977fe4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
