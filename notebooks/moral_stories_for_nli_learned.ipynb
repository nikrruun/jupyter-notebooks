{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2063a2e",
   "metadata": {},
   "source": [
    "# Preparing Moral Stories for NLI with Learn2Split\n",
    "***\n",
    "Two steps to be done:\n",
    "1. Split the norms in the Moral Stories dataset into values and actions\n",
    "2. Extract the names of the actors\n",
    "3. Make stories from actions and actor names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca95f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailignment.datasets.moral_stories import get_moral_stories, make_action_classification_dataframe\n",
    "from ailignment.datasets import get_accuracy_metric, join_sentences, tokenize_and_split\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "import numpy as np\n",
    "from ailignment import sequence_classification\n",
    "\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "\n",
    "#transformers.logging.set_verbosity_warning()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ddbc9",
   "metadata": {},
   "source": [
    "## Applying Learn2Split\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b4079",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = get_moral_stories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e854396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "name = \"../data/models/learn_to_split\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(name).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def parse_split(x):\n",
    "    if \".\" in x:\n",
    "        value, action = x.split(\".\",1)\n",
    "    else:\n",
    "        value, action = x.split(\" \", 1)\n",
    "    action = action.strip()\n",
    "    return {\"action\": action, \"value\":value}\n",
    "\n",
    "def split(x):\n",
    "    inputs = tokenizer(x[\"norm\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {k:v.cuda() for k,v in inputs.items()}\n",
    "    out = model.generate(**inputs, do_sample=True, min_length=1, max_length=100, top_p=0.95, top_k=50, \n",
    "                         num_beams=7, temperature=1.0)\n",
    "    x[\"l2s_output\"] = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "    pairs = pd.DataFrame.from_records([parse_split(y) for y in x[\"l2s_output\"]])\n",
    "    \n",
    "    x[\"norm_action\"] = pairs[\"action\"].to_list()\n",
    "    x[\"norm_value\"] = pairs[\"value\"].to_list()\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to huggingface dataset to make use of their batch processing\n",
    "# (I really just wanted the progress bar...)\n",
    "data = Dataset.from_pandas(dataframe)\n",
    "dataframe = data.map(split, batch_size=32, batched=True).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_empty = dataframe[\"norm_action\"].apply(lambda x: len(x)==0).sum()\n",
    "print(\"The model failed to predict an action for\", num_empty,\"rows\")\n",
    "\n",
    "num_empty = dataframe[\"norm_value\"].apply(lambda x: len(x)==0).sum()\n",
    "print(\"The model failed to predict a norm value for\", num_empty,\"rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb20938b",
   "metadata": {},
   "source": [
    "## Estimating the sentiment\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a00141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "value_map = {v:classifier(v)[0][\"label\"] for v in dataframe[\"norm_value\"].unique()}\n",
    "dataframe[\"norm_sentiment\"] = dataframe[\"norm_value\"].apply(value_map.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc2e91",
   "metadata": {},
   "source": [
    "## Extracting the actor names\n",
    "***\n",
    "Our simple assumption: The name that comes up most in the row is likely to be the central person in the situation, a.k.a the actor. Therefore, we stitch together all parts of each moral story, apply POS tagging and find the most frequent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"\"\n",
    "def unquote(s):\n",
    "    if not isinstance(s, str): return s\n",
    "    if len(s) == 0: return s\n",
    "    if s[0] in \"\\\"'\" and s[-1] in \"\\\"'\":\n",
    "        s = s[1:-1]\n",
    "    return s\n",
    "dataframe = dataframe.progress_applymap(unquote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cfad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token, Span\n",
    "from spacy.language import Language\n",
    "from gender_guesser.detector import Detector\n",
    "\n",
    "# add a token extension that looks itself up in a name dict\n",
    "\n",
    "name_det = Detector(case_sensitive=True)\n",
    "name_det.names.update({\"Benard\":{\"male\":\"1\"},\n",
    "                       \"Haru\":{\"male\":\"1\"}, \n",
    "                       \"Carlow\":{\"male\":\"1\"},\n",
    "                       \"Bently\":{\"male\":\"1\"},\n",
    "                       \"Doro\":{\"male\":\"1\"},\n",
    "                       \"Thiago\":{\"male\",\"1\"}\n",
    "                      })\n",
    "\n",
    "def is_name(token):\n",
    "    return token.text in name_det.names\n",
    "\n",
    "Token.set_extension(\"is_name\", getter=is_name, force=True)\n",
    "\n",
    "# add a custom component that filters out the names from our entity ruler matches\n",
    "@Language.component(\"name_filter\")\n",
    "def name_filter(doc):\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"NAME\" and len(ent) > 1:\n",
    "            new_ent = Span(doc, ent.start, ent.start+1, label=ent.label)\n",
    "            new_ents.append(new_ent)\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    # if there are more than one entity, try to get rid of non-names\n",
    "    if len(new_ents)>1:\n",
    "        new_ents = [x for x in new_ents if x[0]._.is_name]\n",
    "    # take the first one, if there are still more\n",
    "    if len(new_ents)>1:\n",
    "        new_ents = new_ents[:1]\n",
    "    doc.ents = new_ents\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782459d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos tag the moral stories with custom patterns to find names\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", validate=True)\n",
    "nlp.add_pipe(\"name_filter\", after=\"entity_ruler\")\n",
    "\n",
    "do = {\"OP\":\"?\", \"LEMMA\":\"do\"}\n",
    "nt = {\"OP\":\"?\", \"LEMMA\":\"n't\"}\n",
    "adv = {\"OP\":\"?\", \"POS\":\"ADV\"}\n",
    "action_verbs = [do, nt, adv, {\"LEMMA\":{\"IN\":[\"wants\",\"want\",\"need\",\"have\"]}}]\n",
    "\n",
    "patterns = [\n",
    "    {\"label\":\"NAME\", \"pattern\":[{\"POS\": {\"IN\":[\"PROPN\",\"NOUN\"]}}] + action_verbs},\n",
    "    {\"label\":\"NAME\", \"pattern\":[{\"_\":{\"is_name\": True}}] + action_verbs},\n",
    "]\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff27616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = dataframe[\"intention\"].progress_apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfedd31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all docs with ties, should be empty\n",
    "counts = docs.apply(lambda doc: Counter([x[0].text for x in doc.ents]))\n",
    "\n",
    "def f(c):\n",
    "    if len(c) in {0,1}: return False\n",
    "    a,b = c.most_common(2)\n",
    "    if a[1] == b[1]: return True\n",
    "    return False\n",
    "\n",
    "d = dataframe[counts.apply(f)]\n",
    "assert 0 == len(d[[\"intention\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3507ba7",
   "metadata": {},
   "source": [
    "#### Special rules\n",
    "***\n",
    "We apply some special rules for the rest of the bunch\\\n",
    "**Note:** The NLP pipeline should not be used again after this, since the additional rules are extremely sensitive and only suited for the remaining special cases!\n",
    "\n",
    "1. Interpret all PROPN or NOUN at the start of a sentence as the name\n",
    "2. The remaining names are taken from the \"situation\" column, which was manually checked to be okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad1755",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_patterns = [\n",
    "    {\"label\":\"NAME\", \"pattern\":[\n",
    "        {\"IS_SENT_START\":True,\"POS\": {\"IN\":[\"PROPN\",\"NOUN\"]}}\n",
    "    ]}\n",
    "]\n",
    "ruler.add_patterns(weak_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82def222",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# find all docs without any names and apply the weaker patterns\n",
    "remaining = dataframe[\"intention\"][docs.apply(lambda x: len(x.ents)==0)]\n",
    "docs_remaining = remaining.apply(nlp)\n",
    "# update docs series\n",
    "docs[docs_remaining.index] = docs_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98116aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining = dataframe[\"situation\"][docs.apply(lambda x: len(x.ents)==0)].apply(nlp)\n",
    "docs[remaining.index] = remaining\n",
    "\n",
    "# finally get the names\n",
    "names = docs.apply(lambda x: x.ents[0].text)\n",
    "\n",
    "# make a quick spot check\n",
    "potential_non_names = names[names.apply(lambda x: x not in name_det.names)].to_list()\n",
    "#potential_non_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4334da5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe[\"actor_name\"] = names\n",
    "\n",
    "# save the dataframe for later use\n",
    "dataframe.to_pickle(\"../data/moral_stories_proto_l2s.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f52d8",
   "metadata": {},
   "source": [
    "# Create Norm-Stories\n",
    "***\n",
    "Now that we have actor names and the normative action extracted, we want to create the norm stories:\n",
    "* \"hurting someone else\" + \"Kevin\" = \"Kevin hurts someone else\"\n",
    "\n",
    "**NOTE:** We have to get rid of empty nom_actions or norm_values, which ideally should be only a few samples\n",
    "\n",
    "We will apply our `storify-transformer` model to obtain the norm stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "681c1443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11996"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_pickle(\"../data/moral_stories_proto_l2s.dat\").drop(\"__index_level_0__\",axis=1)\n",
    "\n",
    "dataframe = dataframe[dataframe.apply(lambda x: len(x[\"norm_action\"])>0 and len(x[\"norm_action\"])>0, axis=1)]\n",
    "len(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3e4d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "name = \"results/checkpoint-1000/\"\n",
    "name= \"../data/models/storifier/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(name).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3822929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def storify(x):\n",
    "    inputs = tokenizer(x[\"norm_action\"], x[\"actor_name\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {k:v.cuda() for k,v in inputs.items()}\n",
    "    out = model.generate(**inputs, do_sample=True, min_length=5, max_length=100, top_p=0.95, top_k=50, \n",
    "                         num_beams=5, temperature=1.0)\n",
    "    x[\"norm_storyfied\"] = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07af1d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2ca14983b44121b612192864fab991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nikla\\workspace\\piep\\env\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "# convert to huggingface dataset to make use of their batch processing\n",
    "# (I really just wanted the progress bar...)\n",
    "data = Dataset.from_pandas(dataframe)\n",
    "dataframe = data.map(storify, batch_size=32, batched=True).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25e2f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_pickle(\"../data/moral_stories_proto_l2s.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f120383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[\"norm_storyfied\"].apply(lambda x: len(x) == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98080d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
