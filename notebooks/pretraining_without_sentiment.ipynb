{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118dffad",
   "metadata": {},
   "source": [
    "# Pretraining with and without sentiment\n",
    "***\n",
    "research question: If we train a model solely on facts without any notion of good/bad, can the pretraining still alleviate the gap observed for random BERT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd328255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "\n",
    "from ailignment.datasets.util import get_accuracy_metric\n",
    "from ailignment.datasets.moral_stories import make_action_classification_dataframe, get_random_value_dataset\n",
    "import ailignment.datasets.moral_stories_clustered as msc\n",
    "from ailignment.training import sequence_classification\n",
    "\n",
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803651c0",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n",
    "***\n",
    "Steps:\n",
    "1. Extract sentences from wikitext using spacy\n",
    "2. Run sentiment analyzer on the sentences\n",
    "3. Filter out those with a lot of sentiment and those without any.\n",
    "4. Save both corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63d4104e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/home/kiehne/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14577caf074a4f528e9ab4f0a6cc6872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", 'wikitext-2-raw-v1')\n",
    "min_sentence_len=150\n",
    "# merge splits into one single dataset?\n",
    "# datasets.concatenate_datasets(list(dataset.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "361be5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kiehne/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-b7c067b1c46e7b00.arrow\n",
      "Loading cached processed dataset at /home/kiehne/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-4426706ea9c19aa2.arrow\n",
      "Loading cached processed dataset at /home/kiehne/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-891f737d3c0c97a7.arrow\n"
     ]
    }
   ],
   "source": [
    "# get rid of short lines\n",
    "min_token = 1500\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"].strip())>=min_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d5abe04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7fba9c40bd00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07f72508",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[\"test\"]\n",
    "\n",
    "def extract_sentences(data):\n",
    "    # given a huggingface dataset with a \"text\" column,\n",
    "    # find all sentences in there and return a new dataset\n",
    "    docs = nlp.pipe(data[\"text\"], batch_size=32)\n",
    "    sentences = [x.text for y in docs for x in y.sents]\n",
    "    return {\"sentences\":sentences}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b4348f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2afbadc1e02f47d7993d923317bb0be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c233f9cc2e446e09607c96bc9074d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78aae60c60a24b86bbc7f43c4ec6cb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = dataset.map(extract_sentences, batched=True, remove_columns=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4256d06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset bookcorpus (/home/kiehne/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/44662c4a114441c35200992bea923b170e6f13f2f0beb7c14e43759cec498700)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4480f5e48534325bc42fe9af8e69212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run this cell to add bookcorpus sentences\n",
    "books = load_dataset(\"bookcorpus\")\n",
    "books = books[\"train\"].rename_column(\"text\", \"sentences\")\n",
    "books = books.filter(lambda x: len(x[\"sentences\"])>=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ada0c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01054148a0d8444db4c57b2d05b86fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74005 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "books = books.filter(lambda x: len(x[\"sentences\"])>=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "19213735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentences'],\n",
       "    num_rows: 59313984\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "972ec07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dee0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize everything...\n",
    "def convert_hf_dataset(tokenizer, data, padding=\"max_length\"):\n",
    "    def tok(samples):\n",
    "        return tokenizer(samples[\"sentences\"], padding=padding, \n",
    "                         truncation=True, return_token_type_ids=True)\n",
    "\n",
    "    tok_data = data.map(tok, batched=True)\n",
    "    return tok_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2134f444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba227a498567418f8274789d5006cbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68bf1c5aafa4625a01f977a31b05522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d777895aa0e476b824e01a1e7900dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_sents = convert_hf_dataset(tokenizer, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82ee0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data/kiehne/results/shuffled_values/trash/\",\n",
    "    per_device_eval_batch_size=128,)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eb4adbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentences.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10680\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='424' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 02:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a974683267ab433bbeb4eb9bf53ecb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b5a4047a8348dfad6325861b820174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentences.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 88215\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7342760d1696438b93acf2e48970f2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a40030ef214578ab7167bd7619f322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentences.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9309\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe83a5b858f46d29a320ebdf36a5a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa6003f614f444596c7a2e5e669e1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral: DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['sentences'],\n",
      "        num_rows: 4097\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['sentences'],\n",
      "        num_rows: 30354\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentences'],\n",
      "        num_rows: 3561\n",
      "    })\n",
      "})\n",
      "With Sentiment: DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['sentences'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['sentences'],\n",
      "        num_rows: 2352\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentences'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "neutral_sents = datasets.DatasetDict()\n",
    "sentiment_sents = datasets.DatasetDict()\n",
    "neutral_threshold = 0.95\n",
    "neg = 0; neu=1; pos=2\n",
    "\n",
    "for split in sentences.keys():\n",
    "\n",
    "    r = trainer.predict(tok_sents[split])\n",
    "    scores = torch.softmax(torch.from_numpy(r.predictions), 1, torch.float32)\n",
    "    ns = sentences[split].filter(lambda x,i: i> scores[i,neu]>=neutral_threshold, with_indices=True)\n",
    "    ss = sentences[split].filter(lambda x,i: i> scores[i,neu]<(1-neutral_threshold), with_indices=True)\n",
    "    neutral_sents[split] = ns\n",
    "    sentiment_sents[split] = ss\n",
    "\n",
    "print(\"Neutral:\", neutral_sents)\n",
    "print(\"With Sentiment:\", sentiment_sents)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5935a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_sents.save_to_disk(\"/data/kiehne/results/shuffled_values/wiki-neutral/\")\n",
    "sentiment_sents.save_to_disk(\"/data/kiehne/results/shuffled_values/wiki-sentiment/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510cab8a",
   "metadata": {},
   "source": [
    "# Running BERT pretraining\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb66d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
