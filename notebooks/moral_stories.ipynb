{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a64ba63",
   "metadata": {},
   "source": [
    "# Loading the `Moral-Stories` dataset\n",
    "***\n",
    "The dataset and code can be found <a href=\"https://github.com/demelin/moral_stories\">here</a>.\\\n",
    "The authors provide 12k unique norms and, for some reason, additional 700k variations of the same norms, just with NaN fields every now and then. Zero additional information, but maybe I am overlooking something here?\n",
    "* Might be for different tasks? But then they only provide a single label which is always 1 for any NaN rows..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc517ac3",
   "metadata": {},
   "source": [
    "# Sample task: Action classification\n",
    "***\n",
    "For starters, let's reproduce a task from the paper:\n",
    "* Given an action, predict whether it is moral or immoral.\n",
    "* For simplicity, we do not use the splits introduced in the paper, but rather random splitting\n",
    "\n",
    "We start by loading the data as a `pandas.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bfcfee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailignment.datasets.moral_stories import get_moral_stories, make_action_classification_dataframe\n",
    "from ailignment.datasets import get_accuracy_metric, join_sentences, tokenize_and_split\n",
    "from transformers import TrainingArguments\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "from ailignment import sequence_classification\n",
    "\n",
    "transformers.logging.set_verbosity_warning()\n",
    "\n",
    "dataframe = get_moral_stories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "81fbcadc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# run everything through spacy for part of speech tags\n",
    "# find the common patterns up until the first verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3fc41453",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"norm_parsed\"] = dataframe[\"norm\"].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1cd3a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_until_verb(doc):\n",
    "    '''\n",
    "    Returns a tuple of Part-of-speech tags up until the first verb\n",
    "    Returns (\"EMPTY\") if no VERB is present\n",
    "    '''\n",
    "    l = [d.pos_ for d in doc]\n",
    "    if \"VERB\" not in l: return (\"EMPTY\",)\n",
    "    return tuple(l[:l.index(\"VERB\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "aba2e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"norm_pos\"] = dataframe[\"norm_parsed\"].apply(lambda x: [d.pos_ for d in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "874dec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"pos_verb\"] = dataframe[\"norm_parsed\"].apply(get_pos_until_verb)\n",
    "pos_verb = dataframe.groupby(\"pos_verb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8ec582ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>norm</th>\n",
       "      <th>situation</th>\n",
       "      <th>intention</th>\n",
       "      <th>moral_action</th>\n",
       "      <th>moral_consequence</th>\n",
       "      <th>immoral_action</th>\n",
       "      <th>immoral_consequence</th>\n",
       "      <th>norm_parsed</th>\n",
       "      <th>norm_pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_verb</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(PRON, AUX, ADJ, PART)</th>\n",
       "      <td>6518</td>\n",
       "      <td>6518</td>\n",
       "      <td>6518</td>\n",
       "      <td>6518</td>\n",
       "      <td>6518</td>\n",
       "      <td>6518</td>\n",
       "      <td>6518</td>\n",
       "      <td>6518</td>\n",
       "      <td>6518</td>\n",
       "      <td>6518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(PRON, AUX, PART)</th>\n",
       "      <td>2289</td>\n",
       "      <td>2289</td>\n",
       "      <td>2289</td>\n",
       "      <td>2289</td>\n",
       "      <td>2289</td>\n",
       "      <td>2289</td>\n",
       "      <td>2289</td>\n",
       "      <td>2289</td>\n",
       "      <td>2289</td>\n",
       "      <td>2289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(PRON, AUX)</th>\n",
       "      <td>864</td>\n",
       "      <td>864</td>\n",
       "      <td>864</td>\n",
       "      <td>864</td>\n",
       "      <td>864</td>\n",
       "      <td>864</td>\n",
       "      <td>864</td>\n",
       "      <td>864</td>\n",
       "      <td>864</td>\n",
       "      <td>864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(PRON, AUX, PART, ADJ, PART)</th>\n",
       "      <td>373</td>\n",
       "      <td>373</td>\n",
       "      <td>373</td>\n",
       "      <td>373</td>\n",
       "      <td>373</td>\n",
       "      <td>373</td>\n",
       "      <td>373</td>\n",
       "      <td>373</td>\n",
       "      <td>373</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(PRON, AUX, ADJ, PART, PART)</th>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(PRON, AUX, ADJ, ADV, PRON, PART, NOUN)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(DET, NOUN, AUX, DET, ADJ, NOUN, PART)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(PRON, AUX, ADJ, AUX, ADV)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(PRON, AUX, ADJ, AUX, PART)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(SCONJ, PRON, NOUN, AUX)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ID  norm  situation  intention  \\\n",
       "pos_verb                                                                    \n",
       "(PRON, AUX, ADJ, PART)                   6518  6518       6518       6518   \n",
       "(PRON, AUX, PART)                        2289  2289       2289       2289   \n",
       "(PRON, AUX)                               864   864        864        864   \n",
       "(PRON, AUX, PART, ADJ, PART)              373   373        373        373   \n",
       "(PRON, AUX, ADJ, PART, PART)              344   344        344        344   \n",
       "...                                       ...   ...        ...        ...   \n",
       "(PRON, AUX, ADJ, ADV, PRON, PART, NOUN)     1     1          1          1   \n",
       "(DET, NOUN, AUX, DET, ADJ, NOUN, PART)      1     1          1          1   \n",
       "(PRON, AUX, ADJ, AUX, ADV)                  1     1          1          1   \n",
       "(PRON, AUX, ADJ, AUX, PART)                 1     1          1          1   \n",
       "(SCONJ, PRON, NOUN, AUX)                    1     1          1          1   \n",
       "\n",
       "                                         moral_action  moral_consequence  \\\n",
       "pos_verb                                                                   \n",
       "(PRON, AUX, ADJ, PART)                           6518               6518   \n",
       "(PRON, AUX, PART)                                2289               2289   \n",
       "(PRON, AUX)                                       864                864   \n",
       "(PRON, AUX, PART, ADJ, PART)                      373                373   \n",
       "(PRON, AUX, ADJ, PART, PART)                      344                344   \n",
       "...                                               ...                ...   \n",
       "(PRON, AUX, ADJ, ADV, PRON, PART, NOUN)             1                  1   \n",
       "(DET, NOUN, AUX, DET, ADJ, NOUN, PART)              1                  1   \n",
       "(PRON, AUX, ADJ, AUX, ADV)                          1                  1   \n",
       "(PRON, AUX, ADJ, AUX, PART)                         1                  1   \n",
       "(SCONJ, PRON, NOUN, AUX)                            1                  1   \n",
       "\n",
       "                                         immoral_action  immoral_consequence  \\\n",
       "pos_verb                                                                       \n",
       "(PRON, AUX, ADJ, PART)                             6518                 6518   \n",
       "(PRON, AUX, PART)                                  2289                 2289   \n",
       "(PRON, AUX)                                         864                  864   \n",
       "(PRON, AUX, PART, ADJ, PART)                        373                  373   \n",
       "(PRON, AUX, ADJ, PART, PART)                        344                  344   \n",
       "...                                                 ...                  ...   \n",
       "(PRON, AUX, ADJ, ADV, PRON, PART, NOUN)               1                    1   \n",
       "(DET, NOUN, AUX, DET, ADJ, NOUN, PART)                1                    1   \n",
       "(PRON, AUX, ADJ, AUX, ADV)                            1                    1   \n",
       "(PRON, AUX, ADJ, AUX, PART)                           1                    1   \n",
       "(SCONJ, PRON, NOUN, AUX)                              1                    1   \n",
       "\n",
       "                                         norm_parsed  norm_pos  \n",
       "pos_verb                                                        \n",
       "(PRON, AUX, ADJ, PART)                          6518      6518  \n",
       "(PRON, AUX, PART)                               2289      2289  \n",
       "(PRON, AUX)                                      864       864  \n",
       "(PRON, AUX, PART, ADJ, PART)                     373       373  \n",
       "(PRON, AUX, ADJ, PART, PART)                     344       344  \n",
       "...                                              ...       ...  \n",
       "(PRON, AUX, ADJ, ADV, PRON, PART, NOUN)            1         1  \n",
       "(DET, NOUN, AUX, DET, ADJ, NOUN, PART)             1         1  \n",
       "(PRON, AUX, ADJ, AUX, ADV)                         1         1  \n",
       "(PRON, AUX, ADJ, AUX, PART)                        1         1  \n",
       "(SCONJ, PRON, NOUN, AUX)                           1         1  \n",
       "\n",
       "[219 rows x 10 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pos_verb.count().sort_values(\"ID\",ascending=False)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "929f4e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos_verb\n",
       "(PRON, AUX, ADJ, PART)                     0.543212\n",
       "(PRON, AUX, PART)                          0.733978\n",
       "(PRON, AUX)                                0.805984\n",
       "(PRON, AUX, PART, ADJ, PART)               0.837070\n",
       "(PRON, AUX, ADJ, PART, PART)               0.865739\n",
       "                                             ...   \n",
       "(PRON, AUX, ADJ, ADV, PRON, PART, NOUN)    0.999667\n",
       "(DET, NOUN, AUX, DET, ADJ, NOUN, PART)     0.999750\n",
       "(PRON, AUX, ADJ, AUX, ADV)                 0.999833\n",
       "(PRON, AUX, ADJ, AUX, PART)                0.999917\n",
       "(SCONJ, PRON, NOUN, AUX)                   1.000000\n",
       "Name: ID, Length: 219, dtype: float64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = s[\"ID\"].cumsum()/s[\"ID\"].sum()\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d211ace4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>norm</th>\n",
       "      <th>situation</th>\n",
       "      <th>intention</th>\n",
       "      <th>moral_action</th>\n",
       "      <th>moral_consequence</th>\n",
       "      <th>immoral_action</th>\n",
       "      <th>immoral_consequence</th>\n",
       "      <th>norm_parsed</th>\n",
       "      <th>norm_pos</th>\n",
       "      <th>pos_verb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7222</th>\n",
       "      <td>3TVRFO09GLDUXBWS12SEEPJ2264LXJ</td>\n",
       "      <td>If your job is preparing food you should wear ...</td>\n",
       "      <td>Sally works in the kitchen at a diner and is c...</td>\n",
       "      <td>Sally wants to create a meal.</td>\n",
       "      <td>Sally cuts the chicken wearing gloves and afte...</td>\n",
       "      <td>Sally is able to make a healthy meal that the ...</td>\n",
       "      <td>Sally holds the chicken with her bare hand and...</td>\n",
       "      <td>Sally cross contaminates the vegetables with c...</td>\n",
       "      <td>(If, your, job, is, preparing, food, you, shou...</td>\n",
       "      <td>[SCONJ, PRON, NOUN, AUX, VERB, NOUN, PRON, AUX...</td>\n",
       "      <td>(SCONJ, PRON, NOUN, AUX)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ID  \\\n",
       "7222  3TVRFO09GLDUXBWS12SEEPJ2264LXJ   \n",
       "\n",
       "                                                   norm  \\\n",
       "7222  If your job is preparing food you should wear ...   \n",
       "\n",
       "                                              situation  \\\n",
       "7222  Sally works in the kitchen at a diner and is c...   \n",
       "\n",
       "                          intention  \\\n",
       "7222  Sally wants to create a meal.   \n",
       "\n",
       "                                           moral_action  \\\n",
       "7222  Sally cuts the chicken wearing gloves and afte...   \n",
       "\n",
       "                                      moral_consequence  \\\n",
       "7222  Sally is able to make a healthy meal that the ...   \n",
       "\n",
       "                                         immoral_action  \\\n",
       "7222  Sally holds the chicken with her bare hand and...   \n",
       "\n",
       "                                    immoral_consequence  \\\n",
       "7222  Sally cross contaminates the vegetables with c...   \n",
       "\n",
       "                                            norm_parsed  \\\n",
       "7222  (If, your, job, is, preparing, food, you, shou...   \n",
       "\n",
       "                                               norm_pos  \\\n",
       "7222  [SCONJ, PRON, NOUN, AUX, VERB, NOUN, PRON, AUX...   \n",
       "\n",
       "                      pos_verb  \n",
       "7222  (SCONJ, PRON, NOUN, AUX)  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_verb.get_group(s.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0a418f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You should\": 975\n",
      "\"You shouldnt\": 2068\n",
      "\"It's\" 6431\n",
      "\"It is\" 1656\n",
      "Total 11130 of 11999 (92.76%)\n"
     ]
    }
   ],
   "source": [
    "shoulds = dataframe[dataframe[\"norm\"].apply(lambda x: x.lower().startswith(\"you should \"))]\n",
    "print(\"\\\"You should\\\":\", len(shoulds))\n",
    "shouldnts = dataframe[dataframe[\"norm\"].apply(lambda x: x.lower().startswith(\"you shouldn\"))]\n",
    "print(\"\\\"You shouldnt\\\":\", len(shouldnts))\n",
    "its = dataframe[dataframe[\"norm\"].apply(lambda x: x.lower().startswith(\"it's\"))]\n",
    "print(\"\\\"It's\\\"\", len(its))\n",
    "itis = dataframe[dataframe[\"norm\"].apply(lambda x: x.lower().startswith(\"it is\"))]\n",
    "print(\"\\\"It is\\\"\", len(itis))\n",
    "\n",
    "total = len(its)+len(shoulds)+len(shouldnts)+len(itis)\n",
    "print(\"Total\", total ,\"of\",len(dataframe), f\"({100*total/len(dataframe):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75767217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>norm</th>\n",
       "      <th>situation</th>\n",
       "      <th>intention</th>\n",
       "      <th>moral_action</th>\n",
       "      <th>moral_consequence</th>\n",
       "      <th>immoral_action</th>\n",
       "      <th>immoral_consequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3FTOP5WARGMG5V2HV5RUPWJZNBOJ0V</td>\n",
       "      <td>You should not shirk household chores.</td>\n",
       "      <td>Greg's mother has ordered him to clean his mes...</td>\n",
       "      <td>Greg needs to decide whether to accept James' ...</td>\n",
       "      <td>Greg tells James that he can play tomorrow and...</td>\n",
       "      <td>Greg's mother is happy with him and rewards hi...</td>\n",
       "      <td>Greg accepts James' invitation to play video g...</td>\n",
       "      <td>Greg's mother gets upset and grounds him for o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>3DR23U6WE6C9AODGF64DCR8CFCPTES</td>\n",
       "      <td>You should not inject a needle into your veins.</td>\n",
       "      <td>Hank is training to be a nurse in school and i...</td>\n",
       "      <td>Hank wants to inject a person.</td>\n",
       "      <td>Hank gets a volunteer and inserts a needle int...</td>\n",
       "      <td>The volunteer flinches, but Hank keeps his han...</td>\n",
       "      <td>Hank inserts the needle into his own vein, not...</td>\n",
       "      <td>Hank flinches while inserting the needle and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3PJ71Z61R50R68AQ5C4HE7O6WOG19Z</td>\n",
       "      <td>You should not make fun of people's mental dif...</td>\n",
       "      <td>Rick is in class watching John who is developm...</td>\n",
       "      <td>Rick wants to make a comment to John.</td>\n",
       "      <td>\"Rick yells out loud towards John \"\"Hey Johnny...</td>\n",
       "      <td>John feels extra confidence from being cheered...</td>\n",
       "      <td>\"Rick yells to John \"\"Hey John are you a moron...</td>\n",
       "      <td>John is embarrassed and recoils in horror at R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>3OB0CAO74INHBK3CFDZI5BX4N50YHQ</td>\n",
       "      <td>You should not argue in public</td>\n",
       "      <td>Katie is at the grocery store with her partner...</td>\n",
       "      <td>Katie wants to confront her partner about what...</td>\n",
       "      <td>Katie waits until they get back home to bring ...</td>\n",
       "      <td>Katie's and her partner are able to talk in pr...</td>\n",
       "      <td>Katie gets into it with her partner, and they ...</td>\n",
       "      <td>Katie's partner brushes her off and focuses on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>3WI0P0II62QR23MOFMOY6UP9RWODR1</td>\n",
       "      <td>You should not plan to date your friend's curr...</td>\n",
       "      <td>Eric and Donna have been in a relationship for...</td>\n",
       "      <td>Steven wants to ask Donna out for dinner.</td>\n",
       "      <td>Steven invites Donna to dinner and makes it cl...</td>\n",
       "      <td>Donna agrees to go to dinner with Steven and h...</td>\n",
       "      <td>Steven sends a flirty text to Donna inviting h...</td>\n",
       "      <td>Donna is surprised by the text message and rej...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11597</th>\n",
       "      <td>3WETL7AQWU64FOMTPQ5JQKY1A8235N</td>\n",
       "      <td>You should not make fun of people for their in...</td>\n",
       "      <td>Bruce is attending a freshman mixer just befor...</td>\n",
       "      <td>Bruce wants to make friends and increase his s...</td>\n",
       "      <td>Ed stands up for Harold saying he admires anyo...</td>\n",
       "      <td>Ed becomes friends with Harold who tutors him ...</td>\n",
       "      <td>Bruce joins in with a group of frat boys givin...</td>\n",
       "      <td>Bruce embarrasses Harold, but is invited to ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11637</th>\n",
       "      <td>3NAPMVF0ZXDUHA4T9J6B5U75T3U72W</td>\n",
       "      <td>You should not be nosy about someone else's life.</td>\n",
       "      <td>Lynn overhears her friend arguing with her boy...</td>\n",
       "      <td>Lynn wants to be there for her friend.</td>\n",
       "      <td>Lynn gives her friend privacy but lets her kno...</td>\n",
       "      <td>Lynn's friend appreciates the gesture and feel...</td>\n",
       "      <td>Lynn eavesdrops on the conversation and then g...</td>\n",
       "      <td>Lynn's friend accuses her of being nosy and st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11780</th>\n",
       "      <td>3I7DHKZYGOYZV02M9L6LRTNYB9EF5Z</td>\n",
       "      <td>You should not be happy about someone's death.</td>\n",
       "      <td>Steve's parents died in a tragic accident rece...</td>\n",
       "      <td>Steve wants to get his parent's funeral over w...</td>\n",
       "      <td>Steve remains silent and allows his family to ...</td>\n",
       "      <td>The rest of Steve's family thank Steve for pla...</td>\n",
       "      <td>\"Steve plays \"\"another one bites the dust\"\" th...</td>\n",
       "      <td>The rest of his family now hates him as much a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11886</th>\n",
       "      <td>3KYQYYSHYW5O5QU8NU7UCZK4IKZODM</td>\n",
       "      <td>You should not waste other people's time.</td>\n",
       "      <td>Harmony is browsing the displays at a busy jew...</td>\n",
       "      <td>Harmony wants to respond to the sales clerk.</td>\n",
       "      <td>Harmony explains that she's currently just bro...</td>\n",
       "      <td>The sales clerk appreciates how straightforwar...</td>\n",
       "      <td>Harmony asks the sales clerk a bunch of questi...</td>\n",
       "      <td>The sales clerk eventually figures out that sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11919</th>\n",
       "      <td>3M1CVSFP613TSV4CIX8WB37YC00QAA</td>\n",
       "      <td>You should not feel entitled to touch other pe...</td>\n",
       "      <td>Cody is at his friends house, and he sees that...</td>\n",
       "      <td>Cody wants to get a closer look at the collect...</td>\n",
       "      <td>Cody asks his friend to show him the collectib...</td>\n",
       "      <td>Cody's friend is happy to show Cody his collec...</td>\n",
       "      <td>Cody walks over and picks the collectible up g...</td>\n",
       "      <td>Cody's friend freaks out and gets really angry...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ID  \\\n",
       "3      3FTOP5WARGMG5V2HV5RUPWJZNBOJ0V   \n",
       "87     3DR23U6WE6C9AODGF64DCR8CFCPTES   \n",
       "173    3PJ71Z61R50R68AQ5C4HE7O6WOG19Z   \n",
       "298    3OB0CAO74INHBK3CFDZI5BX4N50YHQ   \n",
       "352    3WI0P0II62QR23MOFMOY6UP9RWODR1   \n",
       "...                               ...   \n",
       "11597  3WETL7AQWU64FOMTPQ5JQKY1A8235N   \n",
       "11637  3NAPMVF0ZXDUHA4T9J6B5U75T3U72W   \n",
       "11780  3I7DHKZYGOYZV02M9L6LRTNYB9EF5Z   \n",
       "11886  3KYQYYSHYW5O5QU8NU7UCZK4IKZODM   \n",
       "11919  3M1CVSFP613TSV4CIX8WB37YC00QAA   \n",
       "\n",
       "                                                    norm  \\\n",
       "3                 You should not shirk household chores.   \n",
       "87       You should not inject a needle into your veins.   \n",
       "173    You should not make fun of people's mental dif...   \n",
       "298                       You should not argue in public   \n",
       "352    You should not plan to date your friend's curr...   \n",
       "...                                                  ...   \n",
       "11597  You should not make fun of people for their in...   \n",
       "11637  You should not be nosy about someone else's life.   \n",
       "11780     You should not be happy about someone's death.   \n",
       "11886          You should not waste other people's time.   \n",
       "11919  You should not feel entitled to touch other pe...   \n",
       "\n",
       "                                               situation  \\\n",
       "3      Greg's mother has ordered him to clean his mes...   \n",
       "87     Hank is training to be a nurse in school and i...   \n",
       "173    Rick is in class watching John who is developm...   \n",
       "298    Katie is at the grocery store with her partner...   \n",
       "352    Eric and Donna have been in a relationship for...   \n",
       "...                                                  ...   \n",
       "11597  Bruce is attending a freshman mixer just befor...   \n",
       "11637  Lynn overhears her friend arguing with her boy...   \n",
       "11780  Steve's parents died in a tragic accident rece...   \n",
       "11886  Harmony is browsing the displays at a busy jew...   \n",
       "11919  Cody is at his friends house, and he sees that...   \n",
       "\n",
       "                                               intention  \\\n",
       "3      Greg needs to decide whether to accept James' ...   \n",
       "87                        Hank wants to inject a person.   \n",
       "173                Rick wants to make a comment to John.   \n",
       "298    Katie wants to confront her partner about what...   \n",
       "352            Steven wants to ask Donna out for dinner.   \n",
       "...                                                  ...   \n",
       "11597  Bruce wants to make friends and increase his s...   \n",
       "11637             Lynn wants to be there for her friend.   \n",
       "11780  Steve wants to get his parent's funeral over w...   \n",
       "11886       Harmony wants to respond to the sales clerk.   \n",
       "11919  Cody wants to get a closer look at the collect...   \n",
       "\n",
       "                                            moral_action  \\\n",
       "3      Greg tells James that he can play tomorrow and...   \n",
       "87     Hank gets a volunteer and inserts a needle int...   \n",
       "173    \"Rick yells out loud towards John \"\"Hey Johnny...   \n",
       "298    Katie waits until they get back home to bring ...   \n",
       "352    Steven invites Donna to dinner and makes it cl...   \n",
       "...                                                  ...   \n",
       "11597  Ed stands up for Harold saying he admires anyo...   \n",
       "11637  Lynn gives her friend privacy but lets her kno...   \n",
       "11780  Steve remains silent and allows his family to ...   \n",
       "11886  Harmony explains that she's currently just bro...   \n",
       "11919  Cody asks his friend to show him the collectib...   \n",
       "\n",
       "                                       moral_consequence  \\\n",
       "3      Greg's mother is happy with him and rewards hi...   \n",
       "87     The volunteer flinches, but Hank keeps his han...   \n",
       "173    John feels extra confidence from being cheered...   \n",
       "298    Katie's and her partner are able to talk in pr...   \n",
       "352    Donna agrees to go to dinner with Steven and h...   \n",
       "...                                                  ...   \n",
       "11597  Ed becomes friends with Harold who tutors him ...   \n",
       "11637  Lynn's friend appreciates the gesture and feel...   \n",
       "11780  The rest of Steve's family thank Steve for pla...   \n",
       "11886  The sales clerk appreciates how straightforwar...   \n",
       "11919  Cody's friend is happy to show Cody his collec...   \n",
       "\n",
       "                                          immoral_action  \\\n",
       "3      Greg accepts James' invitation to play video g...   \n",
       "87     Hank inserts the needle into his own vein, not...   \n",
       "173    \"Rick yells to John \"\"Hey John are you a moron...   \n",
       "298    Katie gets into it with her partner, and they ...   \n",
       "352    Steven sends a flirty text to Donna inviting h...   \n",
       "...                                                  ...   \n",
       "11597  Bruce joins in with a group of frat boys givin...   \n",
       "11637  Lynn eavesdrops on the conversation and then g...   \n",
       "11780  \"Steve plays \"\"another one bites the dust\"\" th...   \n",
       "11886  Harmony asks the sales clerk a bunch of questi...   \n",
       "11919  Cody walks over and picks the collectible up g...   \n",
       "\n",
       "                                     immoral_consequence  \n",
       "3      Greg's mother gets upset and grounds him for o...  \n",
       "87     Hank flinches while inserting the needle and t...  \n",
       "173    John is embarrassed and recoils in horror at R...  \n",
       "298    Katie's partner brushes her off and focuses on...  \n",
       "352    Donna is surprised by the text message and rej...  \n",
       "...                                                  ...  \n",
       "11597  Bruce embarrasses Harold, but is invited to ru...  \n",
       "11637  Lynn's friend accuses her of being nosy and st...  \n",
       "11780  The rest of his family now hates him as much a...  \n",
       "11886  The sales clerk eventually figures out that sh...  \n",
       "11919  Cody's friend freaks out and gets really angry...  \n",
       "\n",
       "[239 rows x 8 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shoulds[shoulds[\"norm\"].apply(lambda x: \" not \" in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6cfe808f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({\"it's \": 6431,\n",
       "         'you s': 3048,\n",
       "         'famil': 10,\n",
       "         'it is': 1656,\n",
       "         'peopl': 122,\n",
       "         'its w': 6,\n",
       "         \"you'r\": 31,\n",
       "         'yelli': 3,\n",
       "         'doxxi': 1,\n",
       "         'getti': 6,\n",
       "         'leavi': 8,\n",
       "         'you a': 27,\n",
       "         'terro': 1,\n",
       "         'using': 8,\n",
       "         'slow ': 1,\n",
       "         'itâ€™s ': 24,\n",
       "         'snoop': 2,\n",
       "         'spend': 3,\n",
       "         'shout': 2,\n",
       "         'paren': 30,\n",
       "         'child': 16,\n",
       "         'worke': 1,\n",
       "         'anima': 1,\n",
       "         'partn': 12,\n",
       "         'makin': 9,\n",
       "         'one s': 27,\n",
       "         'kicki': 3,\n",
       "         'glutt': 1,\n",
       "         'a mar': 1,\n",
       "         'attac': 3,\n",
       "         'avoid': 2,\n",
       "         'talki': 2,\n",
       "         'a goo': 2,\n",
       "         'menta': 2,\n",
       "         'fight': 1,\n",
       "         'steal': 7,\n",
       "         'excha': 1,\n",
       "         'hidin': 2,\n",
       "         'is wr': 16,\n",
       "         'one m': 2,\n",
       "         'every': 1,\n",
       "         'ignor': 2,\n",
       "         'work ': 1,\n",
       "         'it gr': 1,\n",
       "         'not h': 3,\n",
       "         'not w': 1,\n",
       "         'frien': 13,\n",
       "         'cheat': 6,\n",
       "         'worki': 2,\n",
       "         'takin': 11,\n",
       "         'when ': 1,\n",
       "         'breas': 1,\n",
       "         'you d': 7,\n",
       "         'often': 1,\n",
       "         'not p': 1,\n",
       "         'is is': 1,\n",
       "         'askin': 4,\n",
       "         'baili': 2,\n",
       "         'not t': 3,\n",
       "         'datin': 3,\n",
       "         'educa': 1,\n",
       "         'mocki': 1,\n",
       "         'showi': 3,\n",
       "         'havin': 1,\n",
       "         'refus': 7,\n",
       "         'death': 1,\n",
       "         'viole': 3,\n",
       "         'adult': 5,\n",
       "         'exclu': 5,\n",
       "         'givin': 4,\n",
       "         'ghost': 1,\n",
       "         'roomm': 4,\n",
       "         'abusi': 4,\n",
       "         'block': 1,\n",
       "         'lying': 4,\n",
       "         'is ri': 1,\n",
       "         'somet': 5,\n",
       "         'neigh': 3,\n",
       "         'sibli': 7,\n",
       "         'is im': 1,\n",
       "         'grown': 1,\n",
       "         'kids ': 4,\n",
       "         'revea': 1,\n",
       "         'men a': 1,\n",
       "         '\"you ': 1,\n",
       "         'cell ': 1,\n",
       "         'sexua': 2,\n",
       "         '\"it\\'s': 6,\n",
       "         'compl': 1,\n",
       "         'you c': 20,\n",
       "         'keepi': 3,\n",
       "         'aband': 2,\n",
       "         'liste': 1,\n",
       "         'regif': 1,\n",
       "         'pick ': 1,\n",
       "         'being': 14,\n",
       "         'locki': 1,\n",
       "         'stayi': 1,\n",
       "         'telli': 3,\n",
       "         'expec': 3,\n",
       "         'punct': 1,\n",
       "         'an en': 1,\n",
       "         'manag': 1,\n",
       "         'open ': 1,\n",
       "         'buyin': 2,\n",
       "         'litte': 2,\n",
       "         'behav': 1,\n",
       "         \"it' r\": 1,\n",
       "         'there': 2,\n",
       "         'drivi': 4,\n",
       "         'spoil': 1,\n",
       "         'calli': 5,\n",
       "         'endin': 1,\n",
       "         'it fe': 1,\n",
       "         'going': 2,\n",
       "         'alway': 2,\n",
       "         'nudit': 2,\n",
       "         'fanta': 1,\n",
       "         'you m': 4,\n",
       "         'a par': 2,\n",
       "         'not b': 2,\n",
       "         \"its' \": 1,\n",
       "         'its f': 1,\n",
       "         'homop': 1,\n",
       "         'helpi': 3,\n",
       "         'intro': 1,\n",
       "         'relat': 2,\n",
       "         'signi': 1,\n",
       "         'letti': 2,\n",
       "         'sayin': 1,\n",
       "         'it cr': 1,\n",
       "         'never': 3,\n",
       "         'one n': 1,\n",
       "         'if so': 2,\n",
       "         'name ': 1,\n",
       "         'embar': 1,\n",
       "         'conti': 1,\n",
       "         'celeb': 1,\n",
       "         'ridin': 1,\n",
       "         'commu': 1,\n",
       "         'stalk': 1,\n",
       "         'weari': 1,\n",
       "         'movin': 1,\n",
       "         'looki': 2,\n",
       "         'is in': 2,\n",
       "         'a pro': 1,\n",
       "         'is ge': 1,\n",
       "         'not e': 1,\n",
       "         'plagi': 2,\n",
       "         'suing': 1,\n",
       "         'knowi': 1,\n",
       "         'touch': 1,\n",
       "         'teach': 2,\n",
       "         'is it': 5,\n",
       "         'booki': 1,\n",
       "         'socia': 1,\n",
       "         'it im': 2,\n",
       "         'cyber': 1,\n",
       "         'its c': 1,\n",
       "         'gambl': 2,\n",
       "         'house': 1,\n",
       "         \"it''s\": 1,\n",
       "         'safet': 1,\n",
       "         'cutti': 2,\n",
       "         'it ca': 4,\n",
       "         'you h': 3,\n",
       "         'if it': 1,\n",
       "         'prete': 1,\n",
       "         'one i': 1,\n",
       "         'docto': 1,\n",
       "         'its u': 1,\n",
       "         'resea': 1,\n",
       "         'a man': 1,\n",
       "         'harmi': 1,\n",
       "         'grand': 2,\n",
       "         'killi': 1,\n",
       "         'broth': 1,\n",
       "         \"don't\": 4,\n",
       "         'overr': 1,\n",
       "         'playi': 2,\n",
       "         'pet o': 1,\n",
       "         'causi': 1,\n",
       "         'commi': 1,\n",
       "         'thera': 1,\n",
       "         'racis': 3,\n",
       "         'break': 1,\n",
       "         'gossi': 1,\n",
       "         'press': 1,\n",
       "         'it me': 1,\n",
       "         'both ': 1,\n",
       "         'stude': 3,\n",
       "         'is ex': 1,\n",
       "         'savin': 1,\n",
       "         'posti': 1,\n",
       "         'unusu': 1,\n",
       "         'cheer': 1,\n",
       "         'eatin': 2,\n",
       "         'secon': 1,\n",
       "         'draft': 1,\n",
       "         'lawye': 1,\n",
       "         'treat': 1,\n",
       "         'fat s': 1,\n",
       "         'if yo': 2,\n",
       "         'it sh': 2,\n",
       "         'a tip': 1,\n",
       "         'is go': 3,\n",
       "         'offen': 1,\n",
       "         'rapin': 1,\n",
       "         'suppo': 1,\n",
       "         'putti': 2,\n",
       "         'babie': 1,\n",
       "         'smell': 1,\n",
       "         'flirt': 1,\n",
       "         'a coa': 1,\n",
       "         'hard ': 1,\n",
       "         'wavin': 1,\n",
       "         'plant': 1,\n",
       "         'crude': 1,\n",
       "         'road ': 1,\n",
       "         'no on': 1,\n",
       "         'promo': 1,\n",
       "         'bring': 2,\n",
       "         'workp': 1,\n",
       "         'jokes': 1,\n",
       "         'huggi': 1,\n",
       "         'hurti': 2,\n",
       "         'provi': 2,\n",
       "         'deliv': 1,\n",
       "         'moms ': 1,\n",
       "         'bully': 2,\n",
       "         'its n': 4,\n",
       "         'trivi': 1,\n",
       "         'a hos': 1,\n",
       "         'marri': 1,\n",
       "         'a rel': 1,\n",
       "         'not r': 1,\n",
       "         'infor': 1,\n",
       "         'preju': 1,\n",
       "         'spray': 1,\n",
       "         'tippi': 1,\n",
       "         'bosse': 1,\n",
       "         'forge': 2,\n",
       "         'prost': 1,\n",
       "         'bigot': 1,\n",
       "         'you j': 1,\n",
       "         'teena': 1,\n",
       "         'judgi': 1,\n",
       "         'emplo': 2,\n",
       "         'smoki': 3,\n",
       "         'holdi': 1,\n",
       "         'decli': 2,\n",
       "         'peein': 2,\n",
       "         'vanda': 1,\n",
       "         'healt': 1,\n",
       "         'arrog': 1,\n",
       "         'blami': 1,\n",
       "         'pleas': 1,\n",
       "         'tryin': 1,\n",
       "         'under': 1,\n",
       "         'theft': 1,\n",
       "         'conta': 2,\n",
       "         'backi': 1,\n",
       "         'do no': 1,\n",
       "         'a kid': 1,\n",
       "         'not v': 1,\n",
       "         'abuse': 1,\n",
       "         'anyon': 1,\n",
       "         'overw': 1,\n",
       "         'it wr': 1,\n",
       "         'fabri': 1,\n",
       "         'argui': 1,\n",
       "         'runni': 1,\n",
       "         'outbu': 1,\n",
       "         'dog f': 1,\n",
       "         'good ': 1,\n",
       "         'joini': 1,\n",
       "         'bragg': 1,\n",
       "         'drink': 3,\n",
       "         'study': 2,\n",
       "         'shami': 1,\n",
       "         'pulli': 1,\n",
       "         'defac': 1,\n",
       "         'hitti': 1,\n",
       "         'durin': 1,\n",
       "         'firin': 1,\n",
       "         'shunn': 1,\n",
       "         'losin': 1,\n",
       "         \"it' n\": 1,\n",
       "         'quitt': 1,\n",
       "         \"you'l\": 1,\n",
       "         'discu': 1,\n",
       "         'if is': 1,\n",
       "         'stopp': 1,\n",
       "         'gamin': 1,\n",
       "         'allow': 1,\n",
       "         'reddi': 1,\n",
       "         'raisi': 1,\n",
       "         'perso': 1,\n",
       "         'a chi': 1,\n",
       "         'is ba': 1})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(dataframe[\"norm\"].apply(lambda x: x[:5].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6592a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for wd in [0,0.01,0.05,0.1]:\n",
    "    for lr in [1e-5, 5e-5]:\n",
    "        test_split = 0.2\n",
    "        batch_size = 12\n",
    "        model = \"distilbert-base-uncased\"\n",
    "        #model = \"albert-base-v2\"\n",
    "        action_dataframe = make_action_classification_dataframe(dataframe)\n",
    "        input_columns = [\"action\"]\n",
    "        action_dataframe[\"task_input\"] = join_sentences(action_dataframe, input_columns, \" \")\n",
    "        dataset = datasets.Dataset.from_pandas(action_dataframe)\n",
    "        dataset = dataset.train_test_split(test_size=test_split)\n",
    "\n",
    "        def data_all(tokenizer):\n",
    "            return tokenize_and_split(dataset, tokenizer, \"task_input\")\n",
    "        def data_small(tokenizer):\n",
    "            train, test = data_all(tokenizer)\n",
    "            train = train.shuffle(seed=42).select(range(1000))\n",
    "            test = test.shuffle(seed=42).select(range(1000))\n",
    "            return train, test\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"results/\",\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=wd,\n",
    "            learning_rate=lr,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=500,\n",
    "            save_steps=1000000,\n",
    "            save_total_limit=0,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "        )\n",
    "\n",
    "        r = sequence_classification(data_all, model, training_args, get_accuracy_metric())\n",
    "        acc = [x[\"eval_accuracy\"] for x in r if \"eval_accuracy\" in x]\n",
    "        results.append(r)\n",
    "        print(wd, lr, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157df0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x[\"eval_accuracy\"] for x in r if \"eval_accuracy\" in x] for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce840a5",
   "metadata": {},
   "source": [
    "# WIP: Get score output from LM\n",
    "***\n",
    "Question: Is there a better way to sample from generated LM outputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e617675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, DistilBertTokenizerFast,\n",
    "     Trainer, TrainingArguments, AutoModelWithLMHead, AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "model = \"distilbert-base-uncased\"\n",
    "model = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelWithLMHead.from_pretrained(model)\n",
    "\n",
    "prompt = \"Today the weather is really nice and I am planning on \"\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "prompt_length = len(tokenizer.decode(inputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "outputs = model.generate(inputs, max_length=250, do_sample=False, top_p=0.95, top_k=60,\n",
    "                        return_dict_in_generate=True, output_attentions=False,\n",
    "                        output_hidden_states=True, output_scores=True)\n",
    "#generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]\n",
    "\n",
    "p = torch.softmax(outputs.scores[0], dim=1)\n",
    "\n",
    "print(p.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a84ed",
   "metadata": {},
   "source": [
    "# WIP: Data augmentation with NER\n",
    "***\n",
    "Idea: Use Named entity recognition to find and replace persons etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c818fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailignment.datasets.moral_stories import get_moral_stories, make_action_classification_dataframe\n",
    "from ailignment import join_sentences, tokenize_and_split, get_accuracy_metric\n",
    "dataframe = get_moral_stories()\n",
    "columns = dataframe.columns[1:]\n",
    "print(\"Running NER on columns\", columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b146a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = join_sentences(dataframe ,columns, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4beffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "ner_pipe = nlp.pipe(tqdm(texts), disable=['tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "docs = [x for x in ner_pipe]\n",
    "\n",
    "displacy.render(docs[0], style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61607f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_frequent_entity(doc, entity=\"PERSON\", n=1):\n",
    "    '''\n",
    "    Returns the highest number of occurences of an\n",
    "    entity in the NER doc.\n",
    "    '''\n",
    "    occurences = [(x.text, x.label_) for x in doc.ents if x.label_ == entity]\n",
    "    c = Counter(occurences)\n",
    "    ents = []\n",
    "    for item, count in c.most_common(n):\n",
    "        ents.append([x for x in doc.ents if (x.text, x.label_) == item])\n",
    "    \n",
    "    if n == 1 and len(ents) != 0:\n",
    "        ents = ents[0]\n",
    "    return ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965139d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = [get_frequent_entity(x, \"PERSON\",n=1) for x in docs]\n",
    "# we are interested in the simplest case, where the NER found\n",
    "# exactly 6 matches\n",
    "matches = [x for x in persons if len(x) == 6]\n",
    "print(f\"Found {len(matches)} matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56082bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = matches[0]\n",
    "displacy.render(m[0].doc, \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded8d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entity(ents, s):\n",
    "    '''\n",
    "    Replaces all occurences of entities in `ents` with `s`.\n",
    "    `ents` is a list of entities as returned by `doc.ents`\n",
    "    from an NER pipeline, they need to be from the same doc!\n",
    "    '''\n",
    "    offset = 0\n",
    "    text = ents[0].doc.text\n",
    "    new_text = \"\"\n",
    "    for ent in ents:\n",
    "        start = ent.start_char\n",
    "        end = ent.end_char\n",
    "        left = text[offset:start]\n",
    "        new_text += left + s\n",
    "        offset = end\n",
    "    new_text += text[offset:]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae19b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = [replace_entity(m, \"Niklas\").split(\"\\n\") for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = [m[0].doc.text.split(\"\\n\") for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_replaced = pd.DataFrame(n_docs)\n",
    "dataframe_replaced.columns = columns\n",
    "dataframe_replaced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1896b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "test_split = 0.2\n",
    "batch_size = 8\n",
    "\n",
    "action_dataframe = make_action_classification_dataframe(dataframe_replaced)\n",
    "\n",
    "input_columns = [\"action\"]\n",
    "action_dataframe[\"task_input\"] = join_sentences(action_dataframe, input_columns, \" \")\n",
    "dataset = datasets.Dataset.from_pandas(action_dataframe)\n",
    "dataset = dataset.train_test_split(test_size=test_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b78c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, DistilBertTokenizerFast,\n",
    "     Trainer, TrainingArguments, AutoModelWithLMHead, AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "model = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "\n",
    "train_data, test_data = tokenize_and_split(dataset, tokenizer, \"task_input\")\n",
    "\n",
    "# for prototyping, optional\n",
    "small_train_data = train_data.shuffle(seed=42).select(range(1000))\n",
    "small_test_data = test_data.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results/\",\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=50,                # how often to log\n",
    "    save_steps=1000,\n",
    "    save_total_limit=0,\n",
    "    evaluation_strategy=\"epoch\",     # when to run evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e0723",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=small_train_data,   # training dataset\n",
    "    eval_dataset=small_test_data,     # evaluation dataset\n",
    "    compute_metrics=get_accuracy_metric,     # code to run accuracy metric\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbeaab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gender_guesser.detector import Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.get_gender(\"Jamie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff68cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailignment.datasets.moral_stories import _lemmatize, get_moral_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e4a88fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kiehne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa31496",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'textcat'])\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "stories = get_moral_stories()\n",
    "columns = [\"moral_action\", \"immoral_action\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b4c2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = stories[columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29b71164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_series(series, nlp, STOP_WORDS=None):\n",
    "    '''\n",
    "    Given a series of strings, returns a DataFrame([\"lemmas\", \"tokens\", \"maps\"])\n",
    "    of the lemmatized strings according to `_lemmatize` function.\n",
    "    '''\n",
    "    # get rid of whitespace\n",
    "    translation_table = str.maketrans(' ', ' ', punctuation)\n",
    "    series = series.map(lambda x: x.translate(translation_table))\n",
    "    series = series.map(lambda x: _lemmatize(x, nlp, STOP_WORDS))\n",
    "    data = pd.DataFrame(series.to_list(), columns=[\"lemmas\", \"tokens\", \"maps\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35699cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = series.map(lambda x: _lemmatize(x, nlp, STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3a3585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(y.to_list(), columns=[\"lemmas\", \"tokens\", \"maps\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3583c702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmas</th>\n",
       "      <th>tokens</th>\n",
       "      <th>maps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[kevin, decide, &lt;STPWRD&gt;, place, &lt;STPWRD&gt;, wag...</td>\n",
       "      <td>[Kevin, decides, to, place, a, wager, once, ev...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[curtis, explain, &lt;STPWRD&gt;, &lt;STPWRD&gt;, woman, &lt;...</td>\n",
       "      <td>[Curtis, explains, to, the, woman, that, he's,...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[anna, decide, &lt;STPWRD&gt;, attend, &lt;STPWRD&gt;, wed...</td>\n",
       "      <td>[Anna, decides, to, attend, the, wedding, even...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[greg, tell, james, &lt;STPWRD&gt;, &lt;STPWRD&gt;, &lt;STPWR...</td>\n",
       "      <td>[Greg, tells, James, that, he, can, play, tomo...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ben, call, &lt;STPWRD&gt;, tow, truck, &lt;STPWRD&gt;, ge...</td>\n",
       "      <td>[Ben, calls, a, tow, truck, and, gets, the, ca...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11994</th>\n",
       "      <td>[payton, ask, &lt;STPWRD&gt;, friend, &lt;STPWRD&gt;, &lt;STP...</td>\n",
       "      <td>[Payton, asks, her, friend, where, she, bought...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>[helen, &lt;STPWRD&gt;, &lt;STPWRD&gt;, conversation, &lt;STP...</td>\n",
       "      <td>[Helen, has, a, conversation, with, her, husba...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>[cole, move, away, &lt;STPWRD&gt;, &lt;STPWRD&gt;, vehicle...</td>\n",
       "      <td>[Cole, moves, away, from, the, vehicle, that, ...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>[harry, ask, steve, &lt;STPWRD&gt;, &lt;STPWRD&gt;, &lt;STPWR...</td>\n",
       "      <td>[Harry, asks, Steve, what, he, should, be, doi...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>[andrew, confront, &lt;STPWRD&gt;, father, ,, &lt;STPWR...</td>\n",
       "      <td>[Andrew, confronts, his, father,, and, tells, ...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [3], 5: [4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11999 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  lemmas  \\\n",
       "0      [kevin, decide, <STPWRD>, place, <STPWRD>, wag...   \n",
       "1      [curtis, explain, <STPWRD>, <STPWRD>, woman, <...   \n",
       "2      [anna, decide, <STPWRD>, attend, <STPWRD>, wed...   \n",
       "3      [greg, tell, james, <STPWRD>, <STPWRD>, <STPWR...   \n",
       "4      [ben, call, <STPWRD>, tow, truck, <STPWRD>, ge...   \n",
       "...                                                  ...   \n",
       "11994  [payton, ask, <STPWRD>, friend, <STPWRD>, <STP...   \n",
       "11995  [helen, <STPWRD>, <STPWRD>, conversation, <STP...   \n",
       "11996  [cole, move, away, <STPWRD>, <STPWRD>, vehicle...   \n",
       "11997  [harry, ask, steve, <STPWRD>, <STPWRD>, <STPWR...   \n",
       "11998  [andrew, confront, <STPWRD>, father, ,, <STPWR...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [Kevin, decides, to, place, a, wager, once, ev...   \n",
       "1      [Curtis, explains, to, the, woman, that, he's,...   \n",
       "2      [Anna, decides, to, attend, the, wedding, even...   \n",
       "3      [Greg, tells, James, that, he, can, play, tomo...   \n",
       "4      [Ben, calls, a, tow, truck, and, gets, the, ca...   \n",
       "...                                                  ...   \n",
       "11994  [Payton, asks, her, friend, where, she, bought...   \n",
       "11995  [Helen, has, a, conversation, with, her, husba...   \n",
       "11996  [Cole, moves, away, from, the, vehicle, that, ...   \n",
       "11997  [Harry, asks, Steve, what, he, should, be, doi...   \n",
       "11998  [Andrew, confronts, his, father,, and, tells, ...   \n",
       "\n",
       "                                                    maps  \n",
       "0      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "1      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "2      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "3      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "4      {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "...                                                  ...  \n",
       "11994  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "11995  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "11996  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "11997  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...  \n",
       "11998  {0: [0], 1: [1], 2: [2], 3: [3], 4: [3], 5: [4...  \n",
       "\n",
       "[11999 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "017f0c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3553355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf5d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
