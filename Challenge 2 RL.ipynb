{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "single-scope",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Setting up your environment\n",
    "***\n",
    "Run the cells below to install all packages and import the necessary code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-planner",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!git clone --depth=1 --branch aicamp2020 https://github.com/nikrruun/jupyter-notebooks.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-anderson",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%cd jupyter-notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-montgomery",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "# OpenAI Gym related:\n",
    "%pip install cmake\n",
    "%pip install atari_py \n",
    "'''OpenAI devs actually messed up an indentation\n",
    "    in the video recorder class in 0.18.x.\n",
    "    If you want to see videos, wait for a patch or\n",
    "    use 0.17.x.\n",
    "'''\n",
    "%pip install gym[box2d]==0.17.3\n",
    "%pip install pyvirtualdisplay\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-tender",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%pip uninstall -y gym_lunarlanderhardcore\n",
    "%pip install gym_lunarlanderhardcore/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-festival",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Utility code to display the runs as video\n",
    "'''\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import base64\n",
    "import io\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "if IN_COLAB:\n",
    "    from pyvirtualdisplay import Display\n",
    "    d = Display()\n",
    "    d.start()\n",
    "\n",
    "'''\n",
    "Below is a solution to record and display videos for\n",
    "OpenAI Gym environments.\n",
    "Why the hassle?\n",
    "    It works both in Jupyter Notebook and Google Colab!\n",
    "(The author would like to note how painful it has been\n",
    "developing this)\n",
    "'''    \n",
    "\n",
    "def display_video_from_monitor(monitor):\n",
    "    '''\n",
    "    Converts all videos in a monitor to HTML videos\n",
    "    '''\n",
    "    if len(monitor.videos) == 0:\n",
    "        print(\"No videos to render!\")\n",
    "        return\n",
    "    for f in monitor.videos:\n",
    "        video = io.open(f[0], 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        display.display(display.HTML(data=\"\"\"\n",
    "            <video alt=\"test\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "            </video>\n",
    "            \"\"\".format(encoded.decode('ascii'))))\n",
    "    return\n",
    "\n",
    "def make_video(env, model, max_steps=None):\n",
    "    mon = Monitor(env, \"/data/videos/\", force=True)\n",
    "    state = mon.reset()\n",
    "    i = 0\n",
    "    while True:\n",
    "        action = model(state)\n",
    "        state, r, d, _ = mon.step(action)\n",
    "        if d: break\n",
    "        i+=1\n",
    "        if max_steps is not None and i>=max_steps: break\n",
    "    if mon.stats_recorder.done == False:    \n",
    "        mon.stats_recorder.save_complete()\n",
    "        mon.stats_recorder.done = True\n",
    "    mon.reset()\n",
    "    display_video_from_monitor(mon)\n",
    "    return mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-grammar",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    ActorCritic class, useful for solving the task\n",
    "'''\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Input, Dropout    \n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def mov_avg(x, l):\n",
    "    return pd.Series(x).rolling(l).mean().iloc[l-1:].values\n",
    "\n",
    "def episode(env, model, max_steps=None):\n",
    "    '''\n",
    "    Runs a single episode of the model in env for at most max_steps\n",
    "    Returns states, rewards, action_probabilities and the\n",
    "    estimated rewards by the critic.\n",
    "    '''\n",
    "    states = []\n",
    "    action_probs = []\n",
    "    est_rewards = []\n",
    "    rewards = []\n",
    "\n",
    "    state = env.reset()\n",
    "    i = 0\n",
    "    max_steps = np.inf if max_steps is None else max_steps\n",
    "    while i < max_steps:\n",
    "        state = tf.convert_to_tensor(state)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        states.append(state)\n",
    "        a_p, e_w = model(state)\n",
    "\n",
    "        action = np.random.choice(env.action_space.n, p=np.squeeze(a_p))\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        action_probs.append(a_p[0, action])\n",
    "        est_rewards.append(e_w[0,0])\n",
    "        rewards.append(reward)\n",
    "\n",
    "        i += 1\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return states, rewards, action_probs, est_rewards\n",
    "\n",
    "def sample_action(model, state):\n",
    "    '''\n",
    "    Given a model and a state, samples an action from model\n",
    "    '''\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        state = tf.convert_to_tensor(state)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        a_p, e_w = model(state)\n",
    "        action = np.random.choice(env.action_space.n, p=np.squeeze(a_p))\n",
    "    return action\n",
    "\n",
    "class ActorCriticDiscrete():\n",
    "    '''\n",
    "    Implements the Actor Critic network structure for discrete action spaces\n",
    "    '''\n",
    "    def __init__(self, env, model, critic_loss=keras.losses.Huber(),\n",
    "                opt=\"Adam\", val_env=None):\n",
    "        self.env = env\n",
    "        self.val_env = env if val_env is None else val_env\n",
    "        self.model = model\n",
    "        self.critic_loss = critic_loss\n",
    "        self.opt = opt\n",
    "\n",
    "    def train_episode(self, discount, max_train_steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            states, rewards, action_probs, est_rewards = episode(self.env, self.model, max_train_steps)\n",
    "            reward = sum(rewards)\n",
    "\n",
    "            returns = []\n",
    "            discounted_sum = 0\n",
    "            for r in rewards[::-1]:\n",
    "                discounted_sum = r + discount * discounted_sum\n",
    "                returns.insert(0, discounted_sum)\n",
    "            returns = np.array(returns)\n",
    "            returns = (returns - returns.mean()) / (returns.std()+0.000001)\n",
    "\n",
    "            returns = tf.convert_to_tensor(returns, \"float32\")\n",
    "\n",
    "            critic_loss = self.critic_loss(est_rewards, returns)\n",
    "            al = -tf.math.log(action_probs) * (returns - est_rewards)\n",
    "            actor_loss = tf.reduce_mean(al)\n",
    "            loss = actor_loss + critic_loss\n",
    "\n",
    "            gradients = tape.gradient(loss, self.model.trainable_weights)\n",
    "            self.opt.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "        return reward\n",
    "\n",
    "    def val_episode(self, val_episodes=1, max_val_steps=None):\n",
    "        val_rewards = []\n",
    "        for val_e in range(val_episodes):\n",
    "            s, r, ap, er = episode(self.val_env, self.model, max_val_steps)\n",
    "            val_rewards.append(sum(r))\n",
    "        val_rewards = np.array(val_rewards)\n",
    "        return val_rewards\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        '''\n",
    "        Convenienve wrapper to quickly run a state through model.\n",
    "        Returns an action\n",
    "        '''\n",
    "        return sample_action(self.model, state)\n",
    "\n",
    "    def plot_reward_history(self, history, avg_length=5):\n",
    "        plt.figure(figsize=(14,5))\n",
    "        plt.plot(history, alpha=0.7, label=\"Rewards\")\n",
    "        plt.plot(np.arange(len(history))[avg_length-1:], mov_avg(history, avg_length), \n",
    "                 alpha=0.7, label=\"Smoothed rewards\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(self.env.spec.id)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def train(self, epochs, discount, max_train_steps = None, validate_every=5, \n",
    "              val_episodes=5, max_val_steps = 200, video=True, max_video_steps=500,\n",
    "             avg_reward_goal=None):\n",
    "\n",
    "        reward_history = []\n",
    "        try:\n",
    "            for epoch in tqdm(range(epochs)):\n",
    "\n",
    "                reward = self.train_episode(discount, max_train_steps)\n",
    "                reward_history.append(reward)\n",
    "                if epoch % validate_every == 0:\n",
    "                    if video:\n",
    "                        make_video(self.val_env, self.sample_action, max_video_steps)\n",
    "                    if val_episodes > 0:\n",
    "                        vr = self.val_episode(val_episodes, max_val_steps)\n",
    "                        print(f\"Epoch {epoch}/{epochs}: Validation reward mean/min/max: \"\n",
    "                              f\"{vr.mean():0.3f}, {vr.min():0.3f}, {vr.max():0.3f}\")\n",
    "                        if avg_reward_goal is not None and vr.mean() >= avg_reward_goal:\n",
    "                            print(f\"Reached goal of mean reward {avg_reward_goal}, stopping!\")\n",
    "                            break\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        reward_history = np.array(reward_history)\n",
    "        return reward_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-gates",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "''' \n",
    "    Evaluation code for the challenge\n",
    "'''\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import gym_lunarlanderhardcore\n",
    "\n",
    "# get env, get hardcore env\n",
    "# evaluate_easy, evaluate_hardcore\n",
    "def get_env():\n",
    "    '''\n",
    "    Returns the \"easy\" environment, which is the original\n",
    "    lunar lander env from OpenAI\n",
    "    https://gym.openai.com/envs/LunarLander-v2/\n",
    "    '''\n",
    "    return gym.make(\"LunarLander-v2\")\n",
    "\n",
    "def get_hardcore_env():\n",
    "    '''\n",
    "    Returns an adapted version of the lunar lander\n",
    "    '''\n",
    "    return gym.make(\"LunarLander-hardcore-v2\")\n",
    "\n",
    "def evaluate_env(env, agent, runs=1):\n",
    "    '''\n",
    "    Runs agent on the given env, returning the rewars per episode.\n",
    "    agent is a function (state)->(action), assigning each state\n",
    "    an integer.\n",
    "    '''\n",
    "    rewards = np.zeros(runs,\"float32\")\n",
    "    for i in tqdm(range(runs)):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = agent(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            rewards[i]+=reward\n",
    "            if done:\n",
    "                break\n",
    "    print(f\"[{env.spec.id}] Agent achieved {rewards.mean():0.2f} mean reward over {runs} runs\")\n",
    "    return rewards\n",
    "\n",
    "def evaluate_easy(agent, runs=100):\n",
    "    '''\n",
    "    agent is a function(state) that returns an action (integer)\n",
    "    We return the mean reward over the given number of runs on\n",
    "    the easy environment setting\n",
    "    '''\n",
    "    env = get_env()\n",
    "    r = evaluate_env(env, agent, runs)\n",
    "    return r.mean()\n",
    "\n",
    "def evaluate_hardcore(agent, runs=100):\n",
    "    '''\n",
    "    agent is a function(state) that returns an action (integer)\n",
    "    We return the mean reward over the given number of runs on\n",
    "    the easy environment setting\n",
    "    '''\n",
    "    env = get_hardcore_env()\n",
    "    r = evaluate_env(env, agent, runs)\n",
    "    return r.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-expense",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenge 2: Landing on the moon\n",
    "***\n",
    "Your task is to develop an algorithm that lands a spaceship on the moon:\n",
    "* Environments: \n",
    "    * Intermediate: `LunarLander-v2` (<a href=\"https://gym.openai.com/envs/LunarLander-v2/\" >Official doc</a>)\n",
    "    * Hard: `LunarLander-hardcore-v2`\n",
    "        * Lander position randomized, higher initial impulse, higher terrain\n",
    "\n",
    "Your algorithm might receive several rewards:\n",
    "* Landing is between 100 and 140 points\n",
    "* Crashing is -100 points\n",
    "* Landing outside of pad is possible, but reduces points received\n",
    "* Each leg touching the ground is +10\n",
    "* Firing main engine is -0.3 each frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-links",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-hardcore-v2')\n",
    "\n",
    "# random action model:\n",
    "model = lambda state: env.action_space.sample()\n",
    "\n",
    "make_video(env, model, max_steps=500)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-player",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: ActorCriticDiscrete\n",
    "***\n",
    "Here is an example to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-holmes",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    env = gym.make('LunarLander-hardcore-v2')\n",
    "    val_env = env.unwrapped\n",
    "\n",
    "    input_layer = Input(env.observation_space.shape)\n",
    "    l = Dense(128, \"relu\")(input_layer)\n",
    "    actor = Dense(env.action_space.n, \"softmax\", name=\"actor\")(l)\n",
    "    critic = Dense(1, name=\"critic\")(l)\n",
    "    model = Model(input_layer, [actor, critic])\n",
    "    model.summary(50)\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=0.01)\n",
    "    critic_loss = keras.losses.Huber()\n",
    "\n",
    "    acd = ActorCriticDiscrete(env, model, critic_loss, opt, \n",
    "                              val_env=val_env)\n",
    "\n",
    "    acd_hist = acd.train(epochs=51, discount=0.99, \n",
    "                          max_train_steps=500, validate_every=50, \n",
    "                          val_episodes=15, max_val_steps=500, \n",
    "                          video=True, max_video_steps=500, \n",
    "                          avg_reward_goal=None)\n",
    "    acd.plot_reward_history(acd_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-quilt",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation\n",
    "***\n",
    "Use the provided code to evaluate your agent. We will use `runs=1000` for final comparison.\n",
    "\n",
    "Evaluation on vanilla environment `LunarLander-v2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-chancellor",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean_r = evaluate_easy(acd.sample_action, runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-hacker",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Evaluation on hardcore environment `LunarLander-hardcore-v2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-destruction",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean_r = evaluate_hardcore(acd.sample_action, runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-testament",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Try to get the highest possible reward!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "rise": {
   "auto_select": "none",
   "enable_chalkboard": true,
   "overlay": "<div class='myheader'><img src='img/ai_camp.png' class='ifis_small'></div><div class='ifis_large'><img src='img/ifis_large.png'></div>",
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
