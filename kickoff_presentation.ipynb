{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cleared-friday",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# TODO\n",
    "* Leitfaden zur Reproduzierung der Slides und Code f√ºr\n",
    "    * Just Code\n",
    "        Remote: CPU/GPU: Colab\n",
    "        Remote: CPU only: Blender\n",
    "        Local: Clone Repo and follow instructions\n",
    "    * Code with slides:\n",
    "        Remote: CPU only: Blender\n",
    "        Local CPU/GPU: Clone repo and follow instructions\n",
    "* Better CSS for IFIS style slides\n",
    "    * Have H1 headings on top of screen\n",
    "    * Have Content starting at fixed height\n",
    "    * Add a footer with author name and Institute\n",
    "* Add docker container for repo\n",
    "    * with nvidia docker\n",
    "* We need a  leaderboard solution for our challenges\n",
    "    * check out kaggle. We need custom scoring functions and private challenges\n",
    "* We need a doodle for weekly date finding\n",
    "* Certificate: Participation\n",
    "    * 3/4 small challenges\n",
    "    * participate at big challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-macro",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# AI Camp 2020 Kickoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-assembly",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Agenda\n",
    "***\n",
    "1. What to expect\n",
    "2. Timeline\n",
    "3. How we code\n",
    "    * Jupyter Notebooks\n",
    "    * Google Colab\n",
    "4. Lecture 1: Python + Keras\n",
    "5. Challenge 1: Minimal MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-slide",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What to expect\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-filter",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Timeline\n",
    "***\n",
    "- **KW 16**: (20.04) Kickoff, Intro lecture, Minimal MNIST Competition\n",
    "    - KW 17: Q&A, optional\n",
    "- **KW 18**: Reinforcement Learning, Pole Cart Competition\n",
    "    - KW 19: Q&A, optional\n",
    "- **KW 20**: Guest lecture: Proc. Kacprowski (Data Science for Biomedicine), **TBD COMP**\n",
    "    - KW 21: Q&A, optional\n",
    "- **KW 22**: Comp Recap, NLP Lecture, Sentiment Analysis Competition\n",
    "    - KW 23: Q&A, optional\n",
    "- **KW 24: Presentation & selection of our grand competition**\n",
    "    - KW 25 to 28: Working on our competition\n",
    "- **KW 29: AI Camp wrap up, presenting the results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-buffalo",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Regular date\n",
    "***\n",
    "We need day + time for our meetings.\n",
    "\n",
    "Please check our doodle, if you haven't already!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-albert",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Organizational\n",
    "***\n",
    " - Lectures & Examples in Python\n",
    "     - Frameworks: Tensorflow, Pytorch, sklearn, etc.\n",
    " - Slides & Code in <a href=\"https://jupyter.org/\">Jupyter Notebooks</a>\n",
    " - Everything is located in Niklas' <a href=\"https://github.com/nikrruun/jupyter-notebooks/tree/aicamp2020\">GitHub Repo</a>\n",
    "     - Branch \"aicamp2020\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-hammer",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why Jupyter Notebooks?\n",
    "***\n",
    "We want to present math and code for AI models\n",
    "> We have PowerPoint for this!\n",
    "\n",
    "But wouldn't it be nice, if we also could\n",
    "- Actually run the code during presentation\n",
    "- Interact with it and observe changes\n",
    "- Easily share it and ship to remote hardware\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-sunrise",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# You're in a simulation\n",
    "***\n",
    "This presentation is just code + markdown: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-oasis",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(33):\n",
    "    print(f\"2^{i} =\", 2**i)\n",
    "    #time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-citizen",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is a running python interpreter!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-possibility",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Features \n",
    "***\n",
    "* Inline Latex support: $e^yx_k\\sum_{i=1}{2^{-i}}$\n",
    "* Text narratives via markdown (essentially a readme.md)\n",
    "* Usually, a notebook is a list of sequential code or text cells\n",
    "    - But, with some effort it automatically translates into a slideshow\n",
    "* Run a copy on <a href=\"https://colab.research.google.com\">Google Colab</a> in < 1 minute\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-detective",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Google Colab walkthrough\n",
    "***\n",
    "1. Open the link <a href=\"https://colab.research.google.com\">Google Colab</a>\n",
    "2. Sign up & in, and open a new notebook\n",
    "3. Go to tab \"GitHub\" and search for user \"nikrruun\". Choose as shown below:\n",
    "<img src=\"img/slides/colab_github_menu.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-yemen",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Google Colab (2)\n",
    "***\n",
    "4. Click on \"<i>kickoff_presentation.ipynb</i>\"\n",
    "    - Generally, Google Colab filters out all available notebooks from a given repository\n",
    "5. Use the \"Content\" Navigation (on the left) or scroll down until you see this cell\n",
    "6. Check if you can run this code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-acquisition",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = [72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100]\n",
    "print(\"\".join([chr(x) for x in data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-psychology",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# So, why *python*?\n",
    "***\n",
    "Python is the current standard for ML and Data Science frameworks:\n",
    "* Forced indentation\n",
    "* Interpreted, not compiled\n",
    "* Extremely slow\n",
    "\n",
    "It is possible to expose C/C++/CUDA code to python\n",
    "* Tons of highly optimized frameworks and libraries exist\n",
    "    * numpy, tensorflow, pytorch, pycuda ...\n",
    "* Performance still not perfect, but close\n",
    "* Interpreted code allows for interactive and explorative programming!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-longer",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Important libraries\n",
    "***\n",
    "There are a couple of standard tools often used for Data Science in python:\n",
    "* **numpy**: for fast math and array operations on CPU\n",
    "* **sklearn**: framework for machine learning pipelines\n",
    "* **pandas**: data analysis and manipulation framework\n",
    "* **tensorflow, keras**: Deep learning with gpu support\n",
    "* **matplotlib**: Data visualization\n",
    "\n",
    "On your own machine, you will have to manually install them (pip)\n",
    "* Google Colab comes with all of those pre-installed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-conditions",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 1: Intro Deep Learning\n",
    "***\n",
    "In the following, we will\n",
    "1. Take a look at the problem we would like to solve\n",
    "2. Learn how to make the computer learn\n",
    "3. Implement a deep neural network to solve (1.) in keras\n",
    "\n",
    "Let's start by importing the libraries we will need today:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amateur-begin",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-accordance",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# POV: You are a post office\n",
    "***\n",
    "We want to recognize handwritten digits to speed up our letter sorting machine!\n",
    "* We need an algorithm that takes in an image and gives back an integer\n",
    "    * We want to *classify* each image into its integer category\n",
    "\n",
    "Let's load the MNIST dataset. Keras has some built-in functions for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forced-cable",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-guyana",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `x_train`: 60k 28x28 images used for training\n",
    "* `y_train`: 60k integer labels\n",
    "* `x_test`: 10k 28x28 images used for testing\n",
    "* `y_test`: 10k integer labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-karaoke",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What do these images look like? matplotlib shows us how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confident-moisture",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAASzElEQVR4nO3de5BcdZnG8e+TMCSQCxKRGGI0EokYURIYQVbYoKws4NYCVRqlXIwsbtwVXBC8FVgaXS9oiYoCSpBLkJtuicquLAIRFy+ITBAIEjQQE5MQEkiAEAghk3n3jz64Q5jz6053T3dnfs+namq6z3tOn3c68+ScPpf5KSIws6FvWLsbMLPWcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhz2IUDSWZK+W+O8l0t6TtKyGucfIWmjpC2SPl/jMr+Q9IEa510m6e9qmbeZy+bIYW+z7QlGmYj4YkRsz2t8JSIm9+thlqTfSHpG0i+2ee3NETEauKqRHltNFV+WtK74+rIktbuvdtqp3Q1YmqSdIqJ3kFezHvgGsC/wtkFeV6vMAY4D9gcCuBn4M/CdNvbUVt6yN4mkKZLWSzqgeL6XpEclHZ5Y5gvAYcD5xa7y+cX0kHSKpCXAkmLaeZJWSNogaaGkw/q9zlxJVxaPJxfLz5b0F0mPSTo71XtE3BIRPwAebuxdGPBnnCLp58XW9TFJV0l6yTazvUnS/ZIel3SZpJH9lv8HSXdLeqLY+3hjjaueDZwbESsjYhVwLvD+pvxQOyiHvUki4iHgE8CVknYFLgPmR8QvEsucDfwSODUiRkfEqf3KxwEHA9OK53cC04FxwNXAf/YPxQAOBV4LHAF8WtLr6vixmkHAl4C9gNcBk4C528zzXuDvgSnAVOBTAJJmAJcCHwReClwEXC9pRA3rfT1wT7/n9xTTsuWwN1FEXAw8CNwBTACSW9QqvhQR6yNiU/HaV0bEuojojYhzgRFUwlzmsxGxKSLuofKLvn8DvdQtIh6MiJuLz/6PAl8DZm4z2/kRsSIi1gNfAE4ops8BLoqIOyJia0TMBzYDb65h1aOBJ/s9fxIYnfPndoe9+S4G9gO+FRGbG3idFf2fSPqopMWSnpT0BLAbsEdi+Uf6PX6Gyi9/y0kaL+laSaskbQCu5MV99/9Zl1PZCwB4FXBmsQv/RPFzT+pXT9kIjO33fCywMTK+zdNhbyJJo6kc6LoEmCtpXA2Llf3y/XV68fn848AsYPeIeAmVLdWOsJX6IpWf5Q0RMRb4J17c96R+j1/J/x87WAF8ISJe0u9r14i4pob1/oEX7s3sX0zLlsPeXOcBPcVpsJ9S25HfNcDeVeYZA/QCjwI7Sfo0L9xqNUTS8OLz/07AMEkjJXUl5n/+IODkGl5+DJWt7JOSJgIfG2CeUyS9ovjP8Wzg+8X0i4F/lXRwcSptlKR3SBpTw3qvAM6QNFHSXsCZwOU1LDdkOexNIulY4Cjg34pJZwAHSHpvlUXPA95ZHIn+Zsk8PwNuBP5EZTf3WbbZzW/QicAm4NtUzg5sohK0MpOKPlbV8NqfBQ6gsifyU+C6Aea5GrgJWAo8BHweICJ6gH8Bzgcep3I85P01rBMqB/P+C1gE3Fes+6Ialx2SlPFHmCxJupjKAbA1ETGlhvlHUNn76KJyMc5nJX0KeDQisg7PjsZhN8uEr6BrAUkbS0pHR8QvW9qMZctbdrNMtHTLvrNGxEhGtXKVZll5lqd5LjYPeEq2obBLOorK0eThwHcj4pzU/CMZxcE6opFVmlnCHbGgtFb3qTdJw4ELgKOpXL99gqRp6aXMrF0aOc9+EPBgRCyNiOeAa4Fjm9OWmTVbI2GfyAsv7FhZTHsBSXMk9Ujq2UIjl4qbWSMG/Qq6iJgXEd0R0d1FLXcmmtlgaCTsq3jhDQyvoLbLJ82sDRoJ+53APpJeLWln4D3A9c1py8yare5TbxHRK+lUKjdpDAcujYisbyE062QNnWePiBuAG5rUi5kNIt/iapYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmWjpkM029PS+7cBkffWHyof8uueQ+cll9799drK+1wU7J+vDb70rWc+Nt+xmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSZ8nt2S+mbOSNa/een5yfprusp/xfqqrPv3h1yWrP+xe2uy/rHJb66yhrw0FHZJy4CngK1Ab0R0N6MpM2u+ZmzZ3xoRjzXhdcxsEPkzu1kmGg17ADdJWihpzkAzSJojqUdSzxbKr5M2s8HV6G78oRGxStKewM2SHoiI2/rPEBHzgHkAYzUuGlyfmdWpoS17RKwqvq8FfgQc1IymzKz56g67pFGSxjz/GDgSuK9ZjZlZczWyGz8e+JGk51/n6oi4sSldWctsOTJ9tvTjF34vWZ/alb6nvC9xNn3pli3JZZ/sG5Gsz0iX2Xz0m0pru9y6KLls37PPpl98B1R32CNiKbB/E3sxs0HkU29mmXDYzTLhsJtlwmE3y4TDbpYJ3+I6BAwfO7a09vTf7ptc9iNfvzpZf+suG6usvf7txeWP/02yvuDCQ5L1X8/9ZrJ+83e/U1qbduWpyWX3/sTtyfqOyFt2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTPs8+BKy8YmJp7c43XdDCTrbP5/a8M1m/cXT6PPxJy45M1udPvqW0NnbauuSyQ5G37GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJnyefQfQ+7YDk/VrppcPmzyM9J96ruak5Uck6z23vC5ZX3RyeW+3bhqZXHbPnk3J+oOPp+/V7/riraW1YUouOiR5y26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZUIR0bKVjdW4OFjp87Y56ps5I1n/xvwLk/XXdNV/ucQ/PnB8sj78nU8n6+vf8dpkfd1+5Se0p16wIrls74qVyXo1/71qYWlt9db0Ofx/nv3vyfrwW++qq6fBdkcsYEOsH/BNr7pll3SppLWS7us3bZykmyUtKb7v3syGzaz5atmNvxw4aptpnwQWRMQ+wILiuZl1sKphj4jbgPXbTD4WmF88ng8c19y2zKzZ6v2wNz4iVhePHwHGl80oaQ4wB2Aku9a5OjNrVMNH46NyhK/0KF9EzIuI7ojo7mJEo6szszrVG/Y1kiYAFN/XNq8lMxsM9Yb9emB28Xg28JPmtGNmg6XqZ3ZJ1wCHA3tIWgl8BjgH+IGkk4HlwKzBbHJHpwNfn6w/dkb6nO/UrvQ96Qs3l9d+vnFactl1105K1l/6eHqc8t2u/G26nqj1JpccXOOHpz9Srjv9mWR9z/Jb5TtW1bBHxAklJV8dY7YD8eWyZplw2M0y4bCbZcJhN8uEw26WCf8p6SYYtmv6MuDer2xI1n+773XJ+p97n0vWzzjrzNLa7r/8S3LZPUelr4famqwOXQdNWJ6sL2tNG03lLbtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmfZ2+CTTPTt7D+bN/0n4Ku5gOnfSRZH/Pj8ttM23kbqXUWb9nNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0z4PHsTvPE/7k7Wh1X5P/Wk5ek/1LvLj3+3vS0Z0KXhpbUtVUYqH67WDWXeKt6ym2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZ8Hn2Gj1x4iGltU+N/2py2T6qDLl8U3pY5Vfym2TdBrYlyv/qfR99yWVvXJz+N9mHu+rqqZ2qbtklXSppraT7+k2bK2mVpLuLr2MGt00za1Qtu/GXA0cNMP3rETG9+LqhuW2ZWbNVDXtE3Aasb0EvZjaIGjlAd6qke4vd/N3LZpI0R1KPpJ4tbG5gdWbWiHrD/m1gCjAdWA2cWzZjRMyLiO6I6O5iRJ2rM7NG1RX2iFgTEVsjog+4GDiouW2ZWbPVFXZJE/o9PR64r2xeM+sMVc+zS7oGOBzYQ9JK4DPA4ZKmA0FlqOoPDl6LnaF3l/LabsPS59Fvfzb98WXvKx5OrztZHbqqjXv/wFf3q/IKC0sr7116dHLJfU/7c7K+I45bXzXsEXHCAJMvGYRezGwQ+XJZs0w47GaZcNjNMuGwm2XCYTfLhG9xbYF1W0cn671Ll7WmkQ5T7dTaH895Q7L+wLHnJ+v/88xupbWHL3hNctkxj5cPg72j8pbdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEz7O3wEd//a5kfWriVswdXd/MGaW1tWdsSi67uDt9Hv2IRe9O1kcdtbS0Noahdx69Gm/ZzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNM+Dx7rVReGlbl/8zzDr0mWb+AqfV01BGWf658KGuAH77va6W1qV3pP8F9wO9mJ+t7HX9/sm4v5C27WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpaJWoZsngRcAYynMkTzvIg4T9I44PvAZCrDNs+KiMcHr9U2i/JSH33JRWfusi5ZP/3yA5P1KZelX7/rkadKa2tmviy57Lh3r0zWP/zKBcn60bum78W//unxpbX3LToqueweF41K1m371LJl7wXOjIhpwJuBUyRNAz4JLIiIfYAFxXMz61BVwx4RqyPiruLxU8BiYCJwLDC/mG0+cNwg9WhmTbBdn9klTQZmAHcA4yNidVF6hMpuvpl1qJrDLmk08EPg9IjY0L8WEUHJp1pJcyT1SOrZwuaGmjWz+tUUdkldVIJ+VURcV0xeI2lCUZ8ArB1o2YiYFxHdEdHdxYhm9GxmdagadkkCLgEWR0T/W5iuB56/LWk28JPmt2dmzVLLLa5vAU4EFkm6u5h2FnAO8ANJJwPLgVmD0uEQMFLpt3nx27+TrP/qsJHJ+pLNLy+tnbTbsuSyjTrt4cOS9Rt/M720ts9p+f0553aqGvaI+BXld3Mf0dx2zGyw+Ao6s0w47GaZcNjNMuGwm2XCYTfLhMNulglVrnRtjbEaFwdrxzxbN3zqlNLa1GuWJ5f98stvb2jd1f5UdbVbbFN+vzn92if875xkfepJQ3e46R3RHbGADbF+wFPl3rKbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZpnwkM012vqnh0prS941ObnstA9/OFm/f9a36mmpJvve8KFk/bUXPpOsT/29z6MPFd6ym2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZ8P3sZkOI72c3M4fdLBcOu1kmHHazTDjsZplw2M0y4bCbZaJq2CVNknSrpPsl/UHSacX0uZJWSbq7+Dpm8Ns1s3rV8screoEzI+IuSWOAhZJuLmpfj4ivDl57ZtYsVcMeEauB1cXjpyQtBiYOdmNm1lzb9Zld0mRgBnBHMelUSfdKulTS7iXLzJHUI6lnC5sb69bM6lZz2CWNBn4InB4RG4BvA1OA6VS2/OcOtFxEzIuI7ojo7mJE4x2bWV1qCrukLipBvyoirgOIiDURsTUi+oCLgYMGr00za1QtR+MFXAIsjoiv9Zs+od9sxwP3Nb89M2uWWo7GvwU4EVgk6e5i2lnACZKmAwEsAz44CP2ZWZPUcjT+V8BA98fe0Px2zGyw+Ao6s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulomWDtks6VFgeb9JewCPtayB7dOpvXVqX+De6tXM3l4VES8bqNDSsL9o5VJPRHS3rYGETu2tU/sC91avVvXm3XizTDjsZplod9jntXn9KZ3aW6f2Be6tXi3pra2f2c2sddq9ZTezFnHYzTLRlrBLOkrSHyU9KOmT7eihjKRlkhYVw1D3tLmXSyWtlXRfv2njJN0saUnxfcAx9trUW0cM450YZryt7127hz9v+Wd2ScOBPwFvB1YCdwInRMT9LW2khKRlQHdEtP0CDEl/C2wEroiI/YppXwHWR8Q5xX+Uu0fEJzqkt7nAxnYP412MVjSh/zDjwHHA+2nje5foaxYteN/asWU/CHgwIpZGxHPAtcCxbeij40XEbcD6bSYfC8wvHs+n8svSciW9dYSIWB0RdxWPnwKeH2a8re9doq+WaEfYJwIr+j1fSWeN9x7ATZIWSprT7mYGMD4iVhePHwHGt7OZAVQdxruVthlmvGPeu3qGP2+UD9C92KERcQBwNHBKsbvakaLyGayTzp3WNIx3qwwwzPhftfO9q3f480a1I+yrgEn9nr+imNYRImJV8X0t8CM6byjqNc+PoFt8X9vmfv6qk4bxHmiYcTrgvWvn8OftCPudwD6SXi1pZ+A9wPVt6ONFJI0qDpwgaRRwJJ03FPX1wOzi8WzgJ23s5QU6ZRjvsmHGafN71/bhzyOi5V/AMVSOyD8EnN2OHkr62hu4p/j6Q7t7A66hslu3hcqxjZOBlwILgCXALcC4Durte8Ai4F4qwZrQpt4OpbKLfi9wd/F1TLvfu0RfLXnffLmsWSZ8gM4sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y8T/ATrJsjl2YrDRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    img = x_train[i]\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"x_train[{i}], label {y_train[i]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-treasurer",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is a classifier?\n",
    "***\n",
    "What we want is a function $C$, that assigns each image a number between 0 and 9:  \n",
    "$C: \\mathbb{N}_{28\\times28}\\rightarrow \\{0,1,\\dots,9\\}$\n",
    "\n",
    "We could also write the images as 1D arrays, row-by-row, so let's also allow  \n",
    "$C: \\mathbb{N}_{784\\times1}\\rightarrow \\{0,1,\\dots,9\\}$\n",
    "\n",
    "\n",
    "What about the *quality* of $C$ ?\n",
    "* We only want the correct assigments/label!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-radiation",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What we know is what we know\n",
    "***\n",
    "If we had perfect knowledge of the correct assignments of $C$\n",
    "* there wouldn't be any problems\n",
    "* a simple look-up-table gets it done\n",
    "    * Or does it?\n",
    "For pixels we have $x_{ij}\\in(0,255)$, i.e. 256 possible values.\n",
    "* Therefore, there are $256^n$ different images with $n$ pixels\n",
    "    * $256^{28*28}\\approx 1.1485*10^{1888}$\n",
    "        * We will need over $10^{1876}$ TB to store the lookup table!\n",
    "\n",
    "Usually, the input domains are just too large to materialize\n",
    "* Also, where do the labels come from? Hiwis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-charter",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# All models are wrong\n",
    "***\n",
    "Even if we reduced each image to one-millionth of its size\n",
    "* We still need over $10^{1870}$ TB of data\n",
    "    * How does nature do it? (*Visual cortices!*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-palestine",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Imagine you had to describe to someone what `1` or `0` looked like  \n",
    "Would you say:\n",
    "* \"Here are all possible images, just learn those by heart\"\n",
    "\n",
    "or\n",
    "* \"One is a vertical line and the other is a circle\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-boston",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concepts for computers\n",
    "***\n",
    "For our case, we want a computer capable of *generalization*:\n",
    "* However, computers don't know about concepts, only about facts\n",
    "\n",
    "We also don't want to have to explain everything\n",
    "* Just once, wouldn't it be nice if the computer figured things out on its own?\n",
    "    * Supervised: By learning from examples\n",
    "    * Unsupervised: Through its own experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-inside",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* What is the difference between stating all facts or explaining the underlying rules?\n",
    "* A *relation* can be defined by either\n",
    "    * Writing down all pairs, i.e. $R=\\{(a,b)\\mid\\dots\\}$\n",
    "    * Write down the *generating process* (Function)\n",
    "\n",
    "* Example:\n",
    "    * $f=\\{(1,1),(2,4),(3,9),\\dots\\}$\n",
    "    * $f(x)=x^2,x\\in \\mathbb{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-stroke",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If you now feel a vague discomfort thinking about what it means to\n",
    "   * *know*, *understand* or to *explain*,\n",
    "then we are good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-oxide",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finding patterns\n",
    "***\n",
    "Luckily (for you), our immediate problem is less philosophical\n",
    "   * Neither brain nor PC process infinite sets in finite time\n",
    "   * We want the *functional notation*!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-clothing",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<span style=\"color:red\">**We only have finite sets!**</span>\n",
    "* There are infinitely many different functions generating a finite dataset $D$\n",
    "    * Which one is correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-engine",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The real world is messy\n",
    "***\n",
    "Data is obtained through observation\n",
    "* E.g. by photography of zipcodes on letters\n",
    "\n",
    "Observation might interfere with the data\n",
    "   * Random noise, warping, distortions, ...\n",
    "    \n",
    "We want the true, *underlying* function of the data\n",
    "* A function perfectly generating $D$ repeats the interferences\n",
    "    * We want $f$ to *generalize* from the faulty data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-nancy",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression\n",
    "***\n",
    "Consider the following synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-railway",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def observe(x):\n",
    "    return x + np.random.rand(*x.shape) * 3\n",
    "\n",
    "n = 30\n",
    "X = np.arange(n)\n",
    "Y = observe(X)\n",
    "plt.plot(X, Y, \"*\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-cycling",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are searching for a fitting function\n",
    "* Get as close as possible to the data, but not too close\n",
    "* Gut feeling: The simplest model that just gets it done\n",
    "    * Linear functions, polynomials, ...\n",
    "\n",
    "Let $f_\\theta(x)$ denote our *model*, which we want to fit to our data\n",
    "* Let $\\theta=\\{w_0,w_1,\\dots\\}$ be a set of *parameters* $w_i$ that determine certain characteristics of $f$\n",
    "    * E.g. the coefficients of a polynomial\n",
    "    * We might omit explicitly stating $\\theta$ for brevity if its clear from context\n",
    "* $x\\in X$ are samples from the observed training data $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-rolling",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Objectives\n",
    "***\n",
    "Given $f_\\theta(x)$, a dataset $D=(X,Y)$ with\n",
    "* $X:=$ the training data, e.g. images, obtained through observation\n",
    "* $Y:=$ the corresponding labels\n",
    "\n",
    "and a function $L(f_\\theta,D)$ implementing a notion of how *closely* $f_\\theta$ approximates $D$\n",
    "\n",
    "Then, we can express our fitting problem as a minimization problem\n",
    "\n",
    "$\\hat{\\theta}=\\underset{\\theta}{argmin}\\: L(f_\\theta,D)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-council",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example: Linear regression\n",
    "***\n",
    "Let $f_\\theta(x)=w_0x+w_1$ be a linear function of $x$ with slope $w_0$ and y-intercept $w_1$\n",
    "\n",
    "Which notion $L$ of error/deviation/well-fitting do we want?\n",
    "* In general: Task-dependent\n",
    "* Here: Anything reasonable works\n",
    "\n",
    "Let's say we want the mean of squared differences to be minimized\\\n",
    "$\\displaystyle L_{MSE}=\\frac{1}{\\lvert D \\rvert}\\sum_{i=0}^{\\lvert D \\rvert} (f(X_i)-Y_i)^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-wellington",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The idea looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-silicon",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f_factory(w0, w1):\n",
    "    def f(x):\n",
    "        return w0*x + w1\n",
    "    return f\n",
    "\n",
    "f = f_factory(2,-10)\n",
    "y_pred = f(X)\n",
    "\n",
    "plt.plot(X, y_pred, label=\"f\")\n",
    "plt.plot(X,Y,\"*\", label=\"Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-queue",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `y_pred`: we call this the prediction of our model $f$ on the samples in $X$\n",
    "    * Think: $y\\_pred_i=f_\\theta(X_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-favor",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our notion of error should measure how far off $f_\\theta$ is for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-beach",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fplot = plt.plot(X, y_pred, label=\"f\")\n",
    "data_plot = plt.plot(X,Y,\"*\", label=\"Data\")\n",
    "for i in range(len(X)):\n",
    "    plt.plot([X[i],X[i]],[Y[i],y_pred[i]],\"--\", \n",
    "             color=\"red\", label=\"Error\")\n",
    "plt.legend([\"f\",\"Data\",\"Error\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-sarah",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can implement our error like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-contents",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def LMSE(Y, y_pred):\n",
    "    return ((y_pred-Y)**2).mean()\n",
    "\n",
    "y_pred = f(X)\n",
    "error_f = LMSE(Y, y_pred)\n",
    "print(\"L_MSE=\",error_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-scoop",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Cool, but where is the minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-rehabilitation",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# With brute force\n",
    "***\n",
    "Remember, we are looking for those $w_0, w_1$ that minimize $L_{MSE}$.\\\n",
    "How does $L_{MSE}$ look like?\n",
    "* Let's check for several different values for $w_0, w_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-container",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "def get_loss_for_model(w0, w1):\n",
    "    f = f_factory(w0, w1)\n",
    "    y_pred = f(X)\n",
    "    return LMSE(Y,y_pred)\n",
    "\n",
    "n = 50\n",
    "w0_range = np.linspace(-5,5,n)\n",
    "w1_range = np.linspace(-5,5,n)\n",
    "\n",
    "w0_grid, w1_grid = np.meshgrid(w0_range, w1_range)\n",
    "loss_grid = np.zeros((n,n),\"float32\")\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        loss_grid[i,j]=get_loss_for_model(w0_grid[i,j], w1_grid[i,j])\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot_surface(w0_grid, w1_grid, loss_grid, cmap=\"coolwarm\")\n",
    "ax.set_xlabel(\"w0\")\n",
    "ax.set_ylabel(\"w1\")\n",
    "ax.set_zlabel(\"LMSE\")\n",
    "plt.title(\"Loss surface linear regression model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-advertiser",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Mathematical optimization\n",
    "***\n",
    "A research area solely dedicated to finding optimal parameters\n",
    "* There are tons of different problem classes\n",
    "    * Discrete, linear, non-linear, integer, convex ...\n",
    "        * There are multiple lectures on each of the subfields @TU BS!\n",
    "* There are even more methods to solve or approximate them\n",
    "    * Bisection, simplex, ant colony optimization, gradient descent and many more\n",
    "\n",
    "For didactic reasons: **Gradient descent**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-croatia",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Reality check\n",
    "***\n",
    "Our example is extremely simplistic\n",
    "* One-dimensional dataset (MNIST has 784)\n",
    "* Only a few samples (MNIST has 60k)\n",
    "* At most two-dimensional model & loss\n",
    "    * State-of-the-art language models have billions of parameters and training samples\n",
    "\n",
    "The restrictions are only for illustration!\n",
    "* Who wants 784 dimensional plots?\n",
    "* The methods are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-serial",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# The curse of dimensionality\n",
    "***\n",
    "Let $\\lvert\\theta\\rvert$ be the number of parameters for a model\n",
    "* Let's say we want to check $n$ points for each parameter\n",
    "    * This leads to $n^{\\lvert\\theta\\rvert}$ different model parametrizations\n",
    "    * We will need to evaluate the loss $n^{\\lvert\\theta\\rvert}$ times!\n",
    "\n",
    "\n",
    "Our grid-based approach has exponential complexity!\n",
    "* In practice, you can't just plot the loss function and spot the optimum\n",
    "    * We won't know how $L$ looks like\n",
    "* A more efficient approach is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-ticket",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Iterative methods\n",
    "***\n",
    "The idea of iterative methods goes as follows:\n",
    "1. Initialize a first set of parameters, called $\\theta_0$\n",
    "    * Could be random or a good guess\n",
    "2. Evaluate the loss achieved with $\\theta_0$, called $L_0 = L(f_{\\theta_0},D)$\n",
    "3. Do\n",
    "    * Parameter update: $\\theta_{k+1}\\leftarrow SEARCH(\\theta_k)$\n",
    "    * Loss update: $L_{k+1} = L(f_{\\theta_k+1},D)$\n",
    "4. Until $L_k\\approx L_{k+1}$ (Or any other convergence or stopping criterion) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-undergraduate",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "***\n",
    "Local search based on derivatives of the loss function w.r.t. to the parameters\n",
    "\n",
    "$\\frac{dL}{d\\theta}$ expresses how changing $\\theta$ affects the value of $L$\n",
    "* If $\\frac{dL}{d\\theta}<0$, then there exists a $h$ with\n",
    "    * $L(f_{\\theta-h},D)<L(f_{\\theta},D)$\\\n",
    "    (Follows from the fundamental theorem of calculus)\n",
    "\n",
    "If the derivative is negative, there is at least one step $h$ that we can take to obtain a smaller $L$!\n",
    "* If it is positive, the same is true for $-\\frac{dL}{d\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-needle",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gradient descent: Sketch\n",
    "***\n",
    "The iterative method would look like this:\n",
    "\n",
    "3. At iteration $k$, do\n",
    "    * Compute gradient $\\nabla L(\\theta_k)=\\frac{dL({f_{\\theta}},D)}{d\\theta}(\\theta_k)$\n",
    "        * Now, we know about the slope of $L$ at $\\theta_k$\n",
    "    * Update $\\theta_{k+1}\\leftarrow\\theta_k-h*\\nabla L(\\theta_k)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-grace",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What the $h$?\n",
    "***\n",
    "Finding an exact $h$ is not trivial for arbitrary functions\n",
    "* It's simple for certain functions, e.g. convex losses\n",
    "    * But not for non-convex and non-linear ones\n",
    "\n",
    "* In practice, somewhat crude heuristics are used for $h$\n",
    "    * E.g. just using a *small enough* factor that *will probably not break anything*\n",
    "        * Usually called the **learning rate**\n",
    "\n",
    "Let's make an example in the following slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-handle",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent applied\n",
    "***\n",
    "For a short while, lets fix $w_1=0$, given $f(x)=w_0x + w_1$\n",
    "* This reduces the dimensionality of our minimization problem\n",
    "    * There is now only one parameter to tune (and to plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-basketball",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "n = 50\n",
    "w0_range = np.linspace(-5,5,n)\n",
    "w1 = 0\n",
    "\n",
    "losses = np.zeros(n, \"float32\")\n",
    "for i in range(n):\n",
    "    losses[i]=get_loss_for_model(w0_range[i], w1)\n",
    "\n",
    "plt.plot(w0_range, losses, label=\"Loss\")\n",
    "plt.plot(w0_range[1], losses[1], \".\", ms=10, label=\"You are here\")\n",
    "plt.xlabel(\"$w_0$\")\n",
    "plt.ylabel(\"LMSE\")\n",
    "plt.legend()\n",
    "plt.title(f\"Loss surface linear regression model ($w_1={w1}$)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-equality",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The loss gradient\n",
    "***\n",
    "The derivative for $L_{MSE}$ is pretty simple:\\\n",
    "$\\displaystyle\\frac{dL_{MSE}}{dw_0}= \\frac{d}{dw_0}\\frac{1}{\\lvert D \\rvert}\\sum_{i=0}^{\\lvert D \\rvert}  (f_{w_0}(X_i)-Y_i)^2$\n",
    "\n",
    "$f$ is the only term depending on $w_0$:\\\n",
    "$\\displaystyle\\frac{dL_{MSE}}{dw_0}= \\frac{1}{\\lvert D \\rvert}\\sum_{i=0}^{\\lvert D \\rvert} 2(f_{w_0}(X_i)-Y_i) \\frac{df}{dw_0}$\n",
    "\n",
    "Remember: $f_{w_0}(x)=w_0*x$, since we fixed $w_1=0$. So $\\frac{df}{dw_0}=x$ and\\\n",
    "$\\displaystyle\\frac{dL_{MSE}}{dw_0}= \\frac{1}{\\lvert D \\rvert}\\sum_{i=0}^{\\lvert D \\rvert} 2X_i(f_{w_0}(X_i)-Y_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-bulgaria",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The loss gradient visualized\n",
    "***\n",
    "We can directly implement the gradient and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-cuisine",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def LMSE_grad_w0(w0, w1, X, Y):\n",
    "    f = f_factory(w0, w1)\n",
    "    return (2*X*(f(X)-Y)).mean()\n",
    "\n",
    "grads_w0 = np.array([LMSE_grad_w0(w0, w1, X, Y) for w0 in w0_range])\n",
    "\n",
    "plt.plot(w0_range, losses, label=\"Loss\")\n",
    "plt.plot(w0_range[3], losses[3], \".\", ms=10, label=\"You are here\")\n",
    "plt.plot(w0_range, grads_w0, label=\"Loss grad $w_0$\")\n",
    "plt.xlabel(\"$w_0$\")\n",
    "plt.ylabel(\"LMSE\")\n",
    "plt.legend()\n",
    "plt.title(f\"Loss surface linear regression model ($w_1={w1}$)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-finnish",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "w0 = -5\n",
    "w1 = 0\n",
    "h = 0.001 # aka the learning rate!\n",
    "steps = 10\n",
    "w_h = np.zeros(steps,\"float32\") # history over steps\n",
    "grad_h = np.zeros(steps,\"float32\")\n",
    "loss_h = np.zeros(steps, \"float32\")\n",
    "\n",
    "for i in range(steps):\n",
    "    grad = LMSE_grad_w0(w0, w1, X, Y)\n",
    "    loss = get_loss_for_model(w0, w1) # loss update\n",
    "    w_h[i] = w0; grad_h[i] = grad; loss_h[i] = loss\n",
    "\n",
    "    w0 = w0 - h * grad # parameter update\n",
    "    # w0 = w0 - np.clip(h*grad, -0.1,0.1)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(w0_range, losses, label=\"Loss\")\n",
    "plt.plot(w_h, loss_h,\"--\", marker=\"*\", label=\"Gradient steps\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$w_0$\")\n",
    "plt.ylabel(\"$L_{MSE}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-shore",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Visualizing the model\n",
    "***\n",
    "Here is the last model we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-second",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "f_final = f_factory(w0, w1)\n",
    "y_pred = f_final(X)\n",
    "final_err = LMSE(Y, y_pred)\n",
    "plt.plot(X, y_pred, label=\"Our model\")\n",
    "plt.plot(X,Y,\"*\", label=\"Data\", color=\"orange\")\n",
    "plt.title(f\"Model error: {final_err:.3f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-football",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Congratulations!\n",
    "***\n",
    "You just trained a neural network using gradient descent!\n",
    "\n",
    "**See you all next week :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-merchandise",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Joke.\n",
    "\n",
    "It is the simplest possible network with trivial training data!\n",
    "* The model is linear and there are even direct solutions to find the optimal $\\theta$\n",
    "\n",
    "We'll get to the real stuff in just a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-cathedral",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Higher dimensions\n",
    "***\n",
    "Let's try to add back our second parameter $w_1$ which we fixed to $0$ in the previous examples\n",
    "\n",
    "Remember the method:\n",
    "* Compute gradient $\\nabla L(\\theta_k)=\\frac{dL({f_{\\theta}},D)}{d\\theta}(\\theta_k)$\n",
    "* Update $\\theta_{k+1}\\leftarrow\\theta_k-h*\\nabla L(\\theta_k)$\n",
    "\n",
    "All we need is the partial derivative $\\displaystyle\\frac{dL_{MSE}}{dw_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-tokyo",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The loss gradient for $w_1$\n",
    "***\n",
    "It is even simpler this time:\\\n",
    "$\\displaystyle\\frac{dL_{MSE}}{dw_1}= \\frac{1}{\\lvert D \\rvert}\\sum_{i=0}^{\\lvert D \\rvert} \\frac{df}{dw_1}2(f_{\\theta}(X_i)-Y_i) $\n",
    "\n",
    "Remember: $f_{\\theta}(x)=w_0*x+w_1$. So $\\frac{df}{dw_1}=1$ and\\\n",
    "$\\displaystyle\\frac{dL_{MSE}}{dw_1}= \\frac{1}{\\lvert D \\rvert}\\sum_{i=0}^{\\lvert D \\rvert} 2*(f_{\\theta}(X_i)-Y_i)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-senate",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def LMSE_grad_w1(w0, w1, X, Y):\n",
    "    f = f_factory(w0, w1)\n",
    "    return (2*(f(X)-Y)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-coordinate",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "w0 = -5\n",
    "w1 = -4\n",
    "lr = 0.001\n",
    "steps = 10\n",
    "w0_h, w1_h, loss_h, grad_w1_h, grad_w0_h = \\\n",
    "    [np.zeros(steps,\"float32\") for i in range(5)]\n",
    "\n",
    "for i in range(steps):\n",
    "    grad_w0 = LMSE_grad_w0(w0, w1, X, Y)\n",
    "    grad_w1 = LMSE_grad_w1(w0, w1, X, Y)\n",
    "    loss = get_loss_for_model(w0, w1) # loss update\n",
    "    w0_h[i] = w0; w1_h[i] = w1; loss_h[i] = loss\n",
    "    grad_w1_h[i] = grad_w1; grad_w0_h[i] = grad_w0\n",
    "\n",
    "    w0 = w0 - lr * grad_w0\n",
    "    w1 = w1 - lr * grad_w1\n",
    "    \n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot(w0_h, w1_h, loss_h, marker=\"*\", color=\"red\")\n",
    "ax.plot_surface(w0_grid, w1_grid, loss_grid, cmap=\"coolwarm\", \n",
    "                alpha=0.7)\n",
    "ax.set_xlabel(\"w0\")\n",
    "ax.set_ylabel(\"w1\")\n",
    "ax.set_zlabel(\"LMSE\")\n",
    "plt.title(f\"Last error: {loss_h[-1]:.3f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-virus",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why isn't $w_1$ changing that much?\n",
    "***\n",
    "Take a look at the gradient histories$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-snapshot",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(grad_w0_h, label=\"Gradient for $w_0$\")\n",
    "plt.plot(grad_w1_h, label=\"Gradient for $w_1$\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-doctrine",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In our implementation, the changes to the $w_i$ are directly proportional to the gradients\n",
    "* There are several options to fix that\n",
    "    * E.g. have a minimum step size\\\n",
    "Can you come up with a solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-gateway",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Leaving convex world\n",
    "***\n",
    "Our $L_{MSE}$ is a convex function\n",
    "* Each local optimium is also a global optimum\n",
    "\n",
    "What happens if we try to find the minimum of a non-linear function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-transition",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def rastrigin_1d(x):\n",
    "    return 10+x**2 - 10*np.cos(2*np.pi*x)\n",
    "def rastrigin_1d_grad(x):\n",
    "    return 2*(x+10*np.pi*np.sin(2*np.pi*x))\n",
    "\n",
    "w0_range = np.linspace(-10,10,1000)\n",
    "rastr = rastrigin_1d(w0_range)\n",
    "plt.plot(w0_range, rastr)\n",
    "plt.title(\"Rastrigin 1D\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-orange",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "w0 = -7.6\n",
    "lr = 0.001 # try 0.0115\n",
    "steps = 100\n",
    "w_h = np.zeros(steps,\"float32\") # history over steps\n",
    "grad_h = np.zeros(steps,\"float32\")\n",
    "loss_h = np.zeros(steps, \"float32\")\n",
    "\n",
    "for i in range(steps):\n",
    "    grad = rastrigin_1d_grad(w0)\n",
    "    loss = rastrigin_1d(w0)\n",
    "    w_h[i] = w0; grad_h[i] = grad; loss_h[i] = loss\n",
    "\n",
    "    w0 = w0 - lr * grad # parameter update\n",
    "    #w0 = w0 - np.clip(h*grad, -0.3,0.3)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(w0_range, rastr, label=\"Rastrigin 1D\")\n",
    "plt.plot(w_h, loss_h,\"--\", marker=\"*\", label=\"Gradient steps\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$w_0$\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "plt.plot(loss_h)\n",
    "plt.title(\"Loss history\"); \n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-happiness",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Summing up\n",
    "***\n",
    "Vanilla gradient descent gets stuck in local optima\n",
    "* Many extensions exist to *mitigate* this\n",
    "    * The **learning rate** controls the step size of the method\n",
    "    * But still, it remains a **heuristic** for non-convex functions\n",
    "\n",
    "On the plus side\n",
    "* It can be applied to any differentiable function\n",
    "    * With arbitrary dimensionality\n",
    "        * We can fit any differentiable model!\n",
    "* It requires few function evaluations\n",
    "    * In our context, that means few runs over the dataset!\n",
    "        * Very important property, since datasets tend to become rather large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-portable",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 10 Minute break\n",
    "***\n",
    "...and then finally we'll talk about neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-affect",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Towards artificial neural networks\n",
    "***\n",
    "So far, we learned about\n",
    "* Linear regression: Fitting a linear model to a set of data points\n",
    "* Loss functions: A notion of how close a model fits our data\n",
    "* Gradient descent: A method to find (local) optima\n",
    "    * We applied this to find better suited parameters for our linear model\n",
    "\n",
    "These topics weren't chosen randomly, since they prepared you for artificial neural networks:\n",
    "* Linear regression is a very special case of neural network\n",
    "    * A single layer, a single neuron and linear activation function\n",
    "* Loss functions and gradient descent are the standard method to train deep nets\n",
    "    * Backpropagation is just a method to efficiently obtain the gradients we talked about earlier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-egyptian",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Biological neurons\n",
    "***\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/36/Components_of_neuron.jpg\" style=\"width: 50%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-interpretation",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# A simplified biological model\n",
    "***\n",
    "A biological neuron could be broken down to\n",
    "* Dendrites, which receive incoming signals from other neurons\n",
    "    * There is also some sort of *synaptic strength* that weighs the connections\n",
    "* A cell body (soma), where all incoming signals are accumulated\n",
    "    * If some *excitation threshold* is reached, the neuron *fires a signal*\n",
    "* Axons, which pass outgoing signals to the dendrites of other neurons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-caribbean",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# A simplified artificial model\n",
    "***\n",
    "We need to:\n",
    "1. Gather all inputs to a neuron\n",
    "2. Attach a weight associated to each input\n",
    "3. Accumulate all inputs, i.e. sum them up\n",
    "4. Compute the excitation strength\n",
    "5. Set our own output signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-qualification",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x_least, err, rank, s = np.linalg.lstsq(X.reshape(-1,1),Y.reshape(-1,1), rcond=-1)\n",
    "x_least = x_least.squeeze()\n",
    "plt.plot(X*x_least, label=f\"x={x_least:.3f}\")\n",
    "plt.plot(X,Y,\"*\", label=\"Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,D,\"-\", label=\"Perfect fit\")\n",
    "plt.plot(X,D,\"*\", label=\"Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-tuner",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Neural networks are a regression method\n",
    "    * Why layers? --> reductionism perspective, abstractions, generalization, etc\n",
    "    * Backprop?, Autodiff, chain rule\n",
    "\n",
    "    $argmin_{\\theta}(C(X) - Y)^2$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Input, Dropout    \n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow import keras\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(env, model, render=False, max_steps=None):\n",
    "    states = []\n",
    "    action_probs = []\n",
    "    est_rewards = []\n",
    "    rewards = []\n",
    "\n",
    "    state = env.reset()\n",
    "    i = 0\n",
    "    while i < (j := max_steps if max_steps is not None else np.inf):\n",
    "        if render: \n",
    "            env.render()\n",
    "        state = tf.convert_to_tensor(state)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        states.append(state)\n",
    "\n",
    "        a_p, e_w = model(state)\n",
    "        action = np.random.choice(env.action_space.n, p=np.squeeze(a_p))\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        action_probs.append(a_p[0, action])\n",
    "        est_rewards.append(e_w[0,0])\n",
    "        rewards.append(reward)\n",
    "\n",
    "        i += 1\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return states, rewards, action_probs, est_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "gamma = 0.9\n",
    "train_max_steps = 200\n",
    "validation_every = 5\n",
    "validation_episodes = 5\n",
    "validation_max_steps = 200\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "#env = gym.make('MountainCar-v0').unwrapped\n",
    "\n",
    "critic_loss_func = keras.losses.Huber()\n",
    "\n",
    "input_layer = Input(env.observation_space.shape)\n",
    "l = Dense(128, \"relu\")(input_layer)\n",
    "l = Dense(64, \"relu\")(l)\n",
    "actor = Dense(env.action_space.n, \"softmax\", name=\"actor\")(l)\n",
    "critic = Dense(1, name=\"critic\")(l)\n",
    "model = Model(input_layer, [actor, critic])\n",
    "\n",
    "model.summary()\n",
    "opt = keras.optimizers.Adam(lr=0.01)\n",
    "reward_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        states, rewards, action_probs, est_rewards = episode(env, model, False, train_max_steps)\n",
    "        reward_history.append(sum(rewards))\n",
    "\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "\n",
    "        critic_labels = tf.convert_to_tensor(returns, \"float32\")\n",
    "\n",
    "        # critic_loss = critic_loss_func(est_rewards, critic_labels)\n",
    "        cl = []\n",
    "        for f,g in zip(est_rewards, critic_labels):\n",
    "            cl.append(critic_loss_func(tf.expand_dims(f,0), tf.expand_dims(g,0)))\n",
    "        critic_loss = sum(cl)\n",
    "\n",
    "        al = -tf.math.log(action_probs) * (critic_labels - est_rewards)\n",
    "        actor_loss = sum(al)\n",
    "        loss = actor_loss + critic_loss\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "        opt.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "    if epoch % validation_every == 0:\n",
    "        val_rewards = []\n",
    "        for val_e in range(validation_episodes):\n",
    "            states, rewards, action_probs, est_rewards = episode(env, model, False, validation_max_steps)\n",
    "            val_rewards.append(sum(rewards))\n",
    "        val_rewards = np.array(val_rewards)\n",
    "        print(f\"Epoch {epoch}/{epochs}\",np.mean(reward_history),  val_rewards.min(), val_rewards.max(), val_rewards.mean())\n",
    "        reward_history.clear()\n",
    "        #episode(env, model, True, validation_max_steps)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-management",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "rise": {
   "auto_select": "none",
   "enable_chalkboard": true,
   "overlay": "<div class='myheader'><img src='img/ai_camp.png' class='ifis_small'></div><div class='ifis_large'><img src='img/ifis_large.png'></div>",
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
