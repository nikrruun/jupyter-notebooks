{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "legal-macro",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# AI Camp 2021 Kickoff\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-assembly",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Agenda\n",
    "***\n",
    "1. What to expect\n",
    "2. Timeline\n",
    "3. How we code\n",
    "    * Jupyter notebooks\n",
    "4. **Lecture 1**: Intro Deep Learning\n",
    "5. Challenge 1: Minimal MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-slide",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What to expect\n",
    "***\n",
    "The AI Camp is for Artificial Intelligence enthusiasts\n",
    "* You have an interest in Data Science and math?\n",
    "\n",
    "4 Mini-lectures on selected topics\n",
    "* 4 Data science challenges, first one starting today!\n",
    "    * Can you beat us?\n",
    "\n",
    "Big joint competition in the last month\n",
    "* It's us against the world\n",
    "\n",
    "A platform for you own AI projects\n",
    "* Write us!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-filter",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Timeline\n",
    "***\n",
    "- **KW 16**: (20.04) Kickoff, Intro lecture, Minimal MNIST Competition\n",
    "    - KW 17: Q&A, Gradient Descent lecture, optional\n",
    "- **KW 18**: Reinforcement Learning\n",
    "    - KW 19: Q&A, optional\n",
    "- **KW 20**: Guest lecture: Proc. Kacprowski (Data Science for Biomedicine), **TBD COMP**\n",
    "    - KW 21: Q&A, optional\n",
    "- **KW 22**: Comp Recap, NLP Lecture, Sentiment Analysis Competition\n",
    "    - KW 23: Q&A, optional\n",
    "- **KW 24: Presentation & selection of our grand competition**\n",
    "    - KW 25 to 28: Working on our competition\n",
    "- **KW 29: AI Camp wrap up, presenting the results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-buffalo",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Regular date\n",
    "***\n",
    "We need day + time for our meetings.\n",
    "\n",
    "Please check our doodle:\n",
    "\n",
    "https://doodle.com/poll/wq8gvb3d762wr6aa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-albert",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Organizational\n",
    "***\n",
    " - Lectures & Examples in Python\n",
    "     - Frameworks: Tensorflow, Pytorch, sklearn, etc.\n",
    " - Slides & Code in <a href=\"https://jupyter.org/\">Jupyter Notebooks</a>\n",
    " - Everything is located in Niklas' <a href=\"https://github.com/nikrruun/jupyter-notebooks/tree/aicamp2020\">GitHub Repo</a>\n",
    "     - Branch \"aicamp2020\"\n",
    " - Join our Discord:\n",
    "     - https://discord.gg/77uHPAMt\n",
    "     - All notifications happen here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-hammer",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why Jupyter Notebooks?\n",
    "***\n",
    "We want to present math and code for AI models\n",
    "> We have PowerPoint for this!\n",
    "\n",
    "But wouldn't it be nice, if we also could\n",
    "- Actually run the code during presentation\n",
    "- Interact with it and observe changes\n",
    "- Easily share it and ship to remote hardware\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-sunrise",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# You're in a simulation\n",
    "***\n",
    "This presentation is just code + markdown: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-oasis",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"HELLO\")\n",
    "for i in range(5):\n",
    "    print(f\"2^{i} =\", 2**i)\n",
    "    #time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-citizen",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is a running python interpreter!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-possibility",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Features \n",
    "***\n",
    "* Inline Latex support: $e^yx_k\\sum_{i=1}{2^{-i}}$\n",
    "* Text narratives via markdown (essentially a readme.md)\n",
    "* Usually, a notebook is a list of sequential code or text cells\n",
    "    - But, with some effort it automatically translates into a slideshow\n",
    "* Run a copy on <a href=\"https://colab.research.google.com\">Google Colab</a> in < 1 minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-temperature",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to run this notebook\n",
    "***\n",
    "Option 1: Google Colab\n",
    "* Run everything in the cloud, everything comes pre-installed\n",
    "    * The content is the same, but no fancy slides!\n",
    "\n",
    "Option 2: Run it on your machine\n",
    "* Try cloning the GitHub and run the setup (Very WIP)\n",
    "    * Currently only available on Windows\n",
    "        * Docker support coming soon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-detective",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Google Colab walkthrough\n",
    "***\n",
    "1. Open the link <a href=\"https://colab.research.google.com\">Google Colab</a>\n",
    "2. Sign up & in, and open a new notebook\n",
    "3. Go to tab \"GitHub\" and search for user \"nikrruun\". Choose as shown below:\n",
    "<img src=\"img/slides/colab_github_menu.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-yemen",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Google Colab (2)\n",
    "***\n",
    "4. Click on \"<i>kickoff_presentation.ipynb</i>\"\n",
    "    - Generally, Google Colab filters out all available notebooks from a given repository\n",
    "5. Use the \"Content\" Navigation (on the left) or scroll down until you see this cell\n",
    "6. Check if you can run this code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-acquisition",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = [72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100]\n",
    "print(\"\".join([chr(x) for x in data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-psychology",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# So, why *python*?\n",
    "***\n",
    "Python is the current standard for ML and Data Science frameworks:\n",
    "* Forced indentation\n",
    "* Interpreted, not compiled\n",
    "* Extremely slow\n",
    "\n",
    "It is possible to expose C/C++/CUDA code to python\n",
    "* Tons of highly optimized frameworks and libraries exist\n",
    "    * numpy, tensorflow, pytorch, pycuda ...\n",
    "* Performance still not perfect, but close\n",
    "* Interpreted code allows for interactive and explorative programming!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-taxation",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 1: Intro Deep Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-attempt",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is an artificial neural network?\n",
    "***\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/851px-Colored_neural_network.svg.png\" style=\"max-width:25%;float:right;\">\n",
    "\n",
    "An artificial neural network is a set of interconnected neurons\n",
    "* In most modern frameworks, neurons are grouped into layers\n",
    "    * Often, we much more talk about these layers, than single neurons\n",
    "        * \"The first hidden layer contains 100 neurons!\"\n",
    "\n",
    "In general, there are three kinds of layers of neurons:\n",
    "* `input layer`: The input data on which our net operates on, e.g. images\n",
    "    * A single node represents a single value, e.g. a pixel\n",
    "* `hidden layer`: Intermediate neurons between `input` and `output`\n",
    "    * They receive the preceeding layer's output as input\n",
    "* `output layer`: Think a `hidden layer` without any successor\n",
    "    * These are the \"results\" the net computed on the \"input\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-queue",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Artificial neurons\n",
    "***\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/851px-Colored_neural_network.svg.png\" style=\"max-width:25%;float:right;\">\n",
    "\n",
    "They are the smallest unit in a neural network, loosely inspired by biological neurons\n",
    "* Each has a set of input connections to some preceeding neurons\n",
    "    * Each connection is assigned a **weight** $w$, representing the \\\n",
    "    importance of the incoming signal for a neuron\n",
    "* It *reacts* to incoming signals by *firing* an output signal:\n",
    "    * It sums up all weighed inputs\n",
    "    * Computes and outputs an **activation** based on the summed inputs\n",
    "* The *way it reacts* to a certain input is determined by its **activation function**\n",
    "    * The activation is inhibited by a **bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-polish",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Weights? Bias? Activation?\n",
    "***\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/851px-Colored_neural_network.svg.png\" style=\"max-width:25%;float:right;\">\n",
    "\n",
    "Consider the top node in the hidden layer (blue circle):\n",
    "* There are three incoming signals from three input nodes\n",
    "    * Signals are just float-values!\n",
    "* We want to assign different values of importance to each of the three signals\n",
    "    * Let's call the input values $x_0,x_1,x_2\\in\\mathbb{R}$\n",
    "        * With respective **weights** $w_0, w_1,w_2\\in\\mathbb{R}$\n",
    "* By multiplying a weight $w_i$ with its signal $x_i$\n",
    "    * We can express how $w_i$ influences the neurons activation:\n",
    "        * $w_i>>1$: $x_i$ will have significant positive impact\n",
    "        * $w_i\\approx0$: $x_i$ will have small to no impact\n",
    "        * $w_i<<-1$: $x_i$ will have significant negative impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-phrase",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summing up\n",
    "***\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/851px-Colored_neural_network.svg.png\" style=\"max-width:25%;float:right;\">\n",
    "\n",
    "Next, we need to sum up the weighed signals:\n",
    "\n",
    "$\\displaystyle w_0*x_0+w_1*x_1+w_2*x_2=\\sum_{i}w_ix_i$\n",
    "* This is the weighed input for our neuron!\n",
    "\n",
    "The **bias** of a neuron represents some sort of threshold to overcome\n",
    "* The larger the bias, the harder it is the get a large activation\n",
    "* In math: Just add it: $\\sum_{i}w_ix_i + b$\\\n",
    "(Intuitively, you would probably rather subtract, but it\\\n",
    "has become the standard to instead add a negative value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-advance",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Getting excited\n",
    "***\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/851px-Colored_neural_network.svg.png\" style=\"max-width:25%;float:right;\">\n",
    "\n",
    "Lastly, we compute the output $y$ of a neuron by applying the activation function $f$\n",
    "\n",
    "$\\displaystyle y=f\\left(\\sum_{i}w_ix_i + b\\right)$\n",
    "\n",
    "Why do we need an activation function?\n",
    "* Without it, our neuron will always be a linear function\n",
    "    * This limits the kind of functions we can express\n",
    "* Instead, we can freely control how our neuron should react to certain values\n",
    "    * E.g. only fire if there is a positive input\n",
    "        * $f_{relu}(x)=max(0,x)$\n",
    "\n",
    "Activation functions allow us to introduce non-linearities into our models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-atlanta",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Time for an example\n",
    "***\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/851px-Colored_neural_network.svg.png\" style=\"max-width:25%;float:right;\">\n",
    "\n",
    "Let's say we have our three inputs\\\n",
    "$x_0=2,x_1=-3,x_2=0.2$\n",
    "\n",
    "And our weights and the bias are set to\\\n",
    "$w_0=-10,w_1=0,w_2=50$, and $b=-30$\n",
    "\n",
    "Further, we choose $f(x)=x^2$\n",
    "\n",
    "The weighed input is then:\\\n",
    "$\\sum_{i}w_ix_i + b=2*(-10)+(-3)*0+0.2*50 + (-30)=-40$\n",
    "\n",
    "And plugging that into $f=x^2$ we get our output $y$:\\\n",
    "$y=f\\left(-40\\right)=1600$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-democracy",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The forward pass\n",
    "***\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/851px-Colored_neural_network.svg.png\" style=\"max-width:25%;float:right;\">\n",
    "\n",
    "Computing a neurons output based on its inputs is called a **forward pass**\n",
    "\n",
    "We can repeat this procedure for each neuron from left to right:\n",
    "* We can compute the state of the whole network\n",
    "    * And get the output of the net!\n",
    "\n",
    "What about the green nodes?\n",
    "* They depend on the output of the blue neurons\n",
    "* Run them last!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-journalism",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fancy functions\n",
    "***\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/851px-Colored_neural_network.svg.png\" style=\"max-width:25%;float:right;\">\n",
    "\n",
    "Think of an artificial neural network as a fancy way to express a function\n",
    "* It has multiple input arguments, namely $x_0,x_1,\\dots,x_n$\n",
    "* To evaluate it, we have to:\n",
    "    * Assign the input layer with the according values\n",
    "    * Apply layer-wise forward passes until the output layer is set\n",
    "    * Return the output values of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-lexington",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fancy indeed\n",
    "***\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/851px-Colored_neural_network.svg.png\" style=\"max-width:25%;float:right;\">\n",
    "\n",
    "$\\displaystyle y_{h_i}=f_{h_i} \\left(\\sum_{j}w_{ji}x_j + b_{h_i}\\right)$\n",
    "\n",
    "and\\\n",
    "$\\displaystyle y_{o_k}=f_{o_k} \\left(\\sum_{i}w_{ik}y_{h_i} + b_{o_k}\\right)$\n",
    "\n",
    "$\\displaystyle y_{o_k}=f_{o_k} \\left(\\left[\\sum_{i}w_{ik}f_{h_i} \\left(\\sum_{j}w_{ji}x_j + b_{h_i}\\right)\\right] + b_{o_k}\\right)$\n",
    "\n",
    "$y_{o_k}$ is the output of the $k$-th neuron in an arbitrary two-layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-bowling",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What about those functions?\n",
    "***\n",
    "Neural networks can be *trained*\n",
    "* By supervision: Learning from examples\n",
    "    * E.g. regression & classification\n",
    "* Unsupervised: Through its own experiences\n",
    "    * E.g. reinforcement learning, clustering\n",
    "\n",
    "It can be shown, that ANNs can approximate any functions to arbitrary degree\n",
    "* In theory, they could learn anything\n",
    "    * Funnily, *how* to get such a perfect network is a much harder problem!\n",
    "    \n",
    "Today: Supervised learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-frank",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why children are smarter than you think\n",
    "***\n",
    "The idea of supervised training is analoguos to how you would teach a child:\n",
    "* First, you would show some examples, e.g. in a book\n",
    "    * \"See, here is a cat!\"\n",
    "* Then, whenever you run across a cat, you'd ask the child:\n",
    "    * \"Hey, what animal is that?\"\n",
    "* If the child were wrong, you would correct it\n",
    "    * The child might then think: \"Oh, *that is a cat*!\"\n",
    "       * And hopefully learn from that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-bhutan",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Supervised training for ANNs\n",
    "***\n",
    "Supervised training roughly goes like this:\n",
    "1. Initialize the network weights (*parameters*) randomly\n",
    "2. Make a forward pass on some examples and obtain the network's outputs\n",
    "    * E.g. pictures of handwritten digits $\\rightarrow$ integers\n",
    "3. Compare the net's predictions to the correct *label* of the example\n",
    "    * Measure some sort of *error*, indicating correctness of prediction\n",
    "4. Adapt the weights, such that the error gets smaller over time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-toilet",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prerequisites for training\n",
    "***\n",
    "To run any training, we need:\n",
    "* A set of examples, together with their label\n",
    "    * E.g. a set of images, of which we know what kind of object they show\n",
    "    * Called a **dataset**\n",
    "* A notion of error which we would like to minimize\n",
    "    * Usually called the **loss**\n",
    "* A method that finds weights that lead to a small error (loss)\n",
    "    * We are looking for an optimization method!\n",
    "    * The current standard for ANNs is Gradient Descent\n",
    "        * More on that later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-exemption",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The loss function\n",
    "***\n",
    "We need a notion of error, that tells us, how far off a network's prediction is:\n",
    "* \"On the last 10 pictures I was slightly off on the first, but completely screwed up on the third\"\n",
    "    * Only in a digital version!\n",
    "* What an error is, and how *bad* it is, usually depends on the task at hand\n",
    "\n",
    "There are myriads of different loss functions for all sorts of tasks\n",
    "* E.g. mean squared error (MSE) for regression/classification\n",
    "    * Or crossentropy, hinge, kullback-leibler divergence, absolute error ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-london",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# POV: You are a post office\n",
    "***\n",
    "We want to recognize handwritten digits to speed up our letter sorting machine!\n",
    "* We want to *classify* each image into its integer category\n",
    "\n",
    "Let's load the MNIST dataset of handwritten digits\n",
    "* Keras has some built-in functions for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-drunk",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-monkey",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `x_train`: 60k 28x28 images used for training\n",
    "* `y_train`: 60k integer labels\n",
    "* `x_test`: 10k 28x28 images used for testing\n",
    "* `y_test`: 10k integer labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-offering",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What do these images look like? matplotlib shows us how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-bacteria",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(5):\n",
    "    img = x_train[i]\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"x_train[{i}], label {y_train[i]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-harvard",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Classes are not continuous\n",
    "***\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/851px-Colored_neural_network.svg.png\" style=\"max-width:25%;float:right;\">\n",
    "\n",
    "How can we adapt our network to output a class?\n",
    "* Idea: For each class we want to predict:\n",
    "    * Dedicate a neuron in the output layer just for that class\n",
    "        * Let the output be in the range $[0,1]$\n",
    "            * By using the right activation function!\n",
    "* Then, interpret $y_{o_k}$ as a probability of observing class $k$\n",
    "    * E.g. $y_{o_1}=0.4$ would express, that the model is 40% sure\\\n",
    "    that the class with index $k=1$ is the correct class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-sponsorship",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But what about our labels?\n",
    "***\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/851px-Colored_neural_network.svg.png\" style=\"max-width:25%;float:right;\">\n",
    "\n",
    "* Instead of showing the network which integer would have been correct\n",
    "* We tell the network which output neuron should compute to probability $1$\n",
    "\n",
    "Then, the labels are just binary vectors:\n",
    "* 4 becomes $[0,0,0,0,1,0,0,0,0,0]$\n",
    "    * This is called a one-hot encoding for class 4\n",
    "\n",
    "The model might return a less clean vector:\n",
    "* E.g. $y_o=[0.2,0.8,0,\\dots,0]$\n",
    "    * It says, \"I think it is a $1$ with $80\\%$ probability\"\n",
    "        * \"Or a $0$ with $20\\%$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-behalf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setting up a model in keras\n",
    "***\n",
    "We need to:\n",
    "* Create an input layer, representing a $28\\times 28$ image\n",
    "    * Serialize to $784$\n",
    "* Create a hidden layer\n",
    "    * How many neurons?\n",
    "    * Which activation function?\n",
    "* Create an output layer\n",
    "    * Exactly 10 neurons\n",
    "    * Which activation function??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-reform",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Flattening the images\n",
    "***\n",
    "We need to serialize our images to 1D arrays\n",
    "* Simple row-by-row is sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-performer",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = x_train.reshape(60000, 28*28)\n",
    "x_test = x_test.reshape(10000, 28*28)\n",
    "print(x_train.shape, x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-quilt",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Converting the labels\n",
    "***\n",
    "Our labels `y_train` and `y_test` are still in integer format\n",
    "* keras has built-in functionality to get binary vectors, too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-grave",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print(\"Label example:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-butterfly",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creating the layers\n",
    "***\n",
    "Creating layers is straightforward with keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-friday",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "input_layer = Input(shape=(28*28,))\n",
    "hidden_layer = Dense(units=16, activation=\"relu\")(input_layer)\n",
    "output_layer = Dense(10, \"softmax\")(hidden_layer) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-playlist",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `input_layer` holding $28\\times28=784$ values\n",
    "* `hidden_layer` with 16 neurons and a `relu` activation\n",
    "    * $f_{relu}(x)=max(x,0)$\n",
    "* `output_layer` with 10 neurons and the `softmax` activation\n",
    "    * The `softmax` *squashes* the outputs into range $[0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-representation",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creating the model\n",
    "***\n",
    "keras organizes several layers into a *model*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-armstrong",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=\"SGD\", loss=\"MSE\", metrics=[\"accuracy\"])\n",
    "print(\"Our model has\", model.count_params(), \"weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-dance",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `SGD` is short for *stochastic gradient descent*, the vanilla optimizer in keras\n",
    "    * There are many others, give them a try!\n",
    "* `MSE` is the *mean squared error* loss\n",
    "* We add the `accuracy` metric, which measure how many digits we correctly classified\n",
    "    * A `metric` has no influence on the training, as opposed to the `loss`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-tribune",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training the model\n",
    "***\n",
    "Training is started via the `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-killing",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "_ = model.fit(x_train, y_train, epochs=1, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-worse",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We train on the serialized images `x_train` and the binary labels `y_train`\n",
    "* `epochs` is the number of times we run over all samples in `x_train`\n",
    "* `batch_size` is the number of samples for each *mini-batch*\n",
    "    * Every `batch_size` steps the weights of the model are adapted\n",
    "        * Weight update is based on the last `batch_size` samples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-tenant",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing the model\n",
    "***\n",
    "Have you wondered about the use of `x_test` and `y_test`?\n",
    "* Training may collaps to perfect memorization (*overfitting*)\n",
    "    * Every sample in `x_train` will be remembered perfectly\n",
    "        * But unseen samples won't!\n",
    "    * Think of the \"peek-a-boo\" game with babies!\n",
    "        * By covering parts of your face, the still-developing brain can't *recognize* you\n",
    "\n",
    "To check whether a model *generalizes* from the training data to unseen samples\n",
    "* An evaluation on held-out data is crucial!\n",
    "    * That's what `x_test` and `y_test` are used for\n",
    "    * **Never** use those in training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-gregory",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-forwarding",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Putting it all together\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-mills",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Layers\n",
    "input_layer = Input(shape=(28*28,))\n",
    "hidden_layer = Dense(units=16, activation=\"relu\")(input_layer)\n",
    "output_layer = Dense(10, \"softmax\")(hidden_layer)\n",
    "\n",
    "# Model\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=\"SGD\", loss=\"MSE\", metrics=[\"accuracy\"])\n",
    "print(\"Our model has\", model.count_params(), \"weights\")\n",
    "\n",
    "# Training\n",
    "print(\"Training:\")\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=256, verbose=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Evaluation:\")\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-renaissance",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualizing the predictions\n",
    "***\n",
    "Let's take a look at the kind of mistakes our model makes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-pursuit",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "classes = y_pred.argmax(axis=1)\n",
    "wrong_indices = np.where(classes != y_test.argmax(axis=1))[0]\n",
    "print(\"Model failed on samples:\", wrong_indices)\n",
    "print(\"Mis-classified\", len(wrong_indices),\"out of\",len(y_test),\"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-exploration",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,3, squeeze=True)\n",
    "axes = axes.flatten()\n",
    "for i in range(6):\n",
    "    index = wrong_indices[i]\n",
    "    pred = classes[index]\n",
    "    axes[i].imshow(x_test[index].reshape(28,28))\n",
    "    axes[i].set_title(f\"Model says it is a {pred}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-airfare",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenge time!\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-armstrong",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Open the link <a href=\"https://colab.research.google.com\">Google Colab</a>\n",
    "2. Sign up & in, and open a new notebook\n",
    "3. Go to tab \"GitHub\" and search for user \"nikrruun\". Choose as shown below:\n",
    "<img src=\"img/slides/colab_github_menu.png\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "rise": {
   "auto_select": "none",
   "enable_chalkboard": true,
   "overlay": "<div class='myheader'><img src='img/ai_camp.png' class='ifis_small'></div><div class='ifis_large'><img src='img/ifis_large.png'></div>",
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
