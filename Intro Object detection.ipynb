{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f8d57e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro: Object Detection with Deep Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03002c4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation\n",
    "***\n",
    "What if we could make computers see?\n",
    "<center><img src=\"img/slides/tu_bs.jpg\"/ style=\"width:1000px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce29a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visual recognition tasks\n",
    "***\n",
    "A camera is not enough!\n",
    "* We want the computer to understand the scene\n",
    "    * Maybe start off by recognizing different objects first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c34fc2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification\n",
    "***\n",
    "Does this image contain a building? What about a plane?\n",
    "<center><img src=\"img/slides/tu_bs.jpg\"/ style=\"width:1000px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc219b8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Detection\n",
    "***\n",
    "Does this image contain a tree? [where?]\n",
    "<center><img src=\"img/slides/tu_bs_tree.png\"/ style=\"width:1000px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f899634e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Detection\n",
    "***\n",
    "Which objects does this image contain?\n",
    "<center><img src=\"img/slides/tu_bs_multi.png\"/ style=\"width:1000px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e26843",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Detection\n",
    "***\n",
    "Accuracate localization (segmentation, pixelwise classification)\n",
    "<center><img src=\"img/slides/tu_bs_seg.png\"/ style=\"width:1000px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37606bb0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Detection\n",
    "***\n",
    "Classes vs instances: Where is *my* bike?\n",
    "<center><img src=\"img/slides/tu_bs_inst.png\"/ style=\"width:1000px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7d2d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visual recognition tasks\n",
    "***\n",
    "There is:\n",
    "* Class and instance detection/localization\n",
    "    * Bounding box, segmentation mask\n",
    "* Object attribute estimation\n",
    "    * How far away, how old, or just who is that\n",
    "* Activity or event recognition\n",
    "    * What is someone doing?\n",
    "* Single image vs video\n",
    "    * All of the above plus a time dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f244e84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's scope\n",
    "***\n",
    "For this lecture and the challenge:\n",
    "* Class detection using bounding box labels\n",
    "    * Single images\n",
    "\n",
    "Next steps:\n",
    "* Single vs multi stage networks\n",
    "    * R-CNN, ...\n",
    "    * YOLO/SSD\n",
    "* Face detection challenge! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583b8c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap: Convolutional neural networks\n",
    "***\n",
    "<center><img src=\"img/slides/cnn.gif\"/ style=\"width:700px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471bda44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap: Convolutional neural networks\n",
    "***\n",
    "<center><img src=\"img/slides/cnn_classifier.jpeg\"/ style=\"width:1000px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de68e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A first approach to localization\n",
    "***\n",
    "<center><img src=\"img/slides/tu_bs.jpg\"/ style=\"width:500px;float:right;\"></center>\n",
    "\n",
    "Let's say we want to find the positions of all bikes in our image:\n",
    "* Train an image classifier for bikes\n",
    "* Apply the classifier to each crop\n",
    "\n",
    "Is that a good solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c399b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* There are *many* crops. We need to check all\n",
    "    * All possible heights and widths...\n",
    "* Computation is not shared between overlapping crops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d9c4b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Better ideas\n",
    "***\n",
    "There are two main streams of object detectors, both refining the sliding window approach:\n",
    "* Two-stage: Classify regions of interest\n",
    "* Single-stage: Predictions on a grid of proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ed85e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Two-stage detection\n",
    "***\n",
    "<center><img src=\"img/slides/region_proposal.jpg\"/ style=\"width:500px;float:right;\"></center>\n",
    "\n",
    "1. Region proposal\n",
    "* Based on the input image, find regions likely to contain objects\n",
    "    * Selective search, region proposal networks\n",
    "    * Usually, few thousand proposals are drawn\n",
    "2. Classification & bounding box regression stage\n",
    "* For each proposed region:\n",
    "    * Predict an object class\n",
    "    * Predict a bounding box, containing the complete object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b95b03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Region-based CNNs (R-CNN)\n",
    "***\n",
    "Proposed by Girshick et al. @CVPR14\n",
    "\n",
    "<center><img src=\"img/slides/rcnn_1.jpg\"/ style=\"width:1000px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9486408f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Region-based CNNs (R-CNN)\n",
    "***\n",
    "Selective search\n",
    "\n",
    "<center><img src=\"img/slides/rcnn_2.jpg\"/ style=\"width:1000px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f607936",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Region-based CNNs (R-CNN)\n",
    "***\n",
    "\n",
    "<center><img src=\"img/slides/rcnn_3.jpg\"/ style=\"width:1000px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09958878",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Region-based CNNs (R-CNN)\n",
    "***\n",
    "Stage two: Feature extraction\n",
    "\n",
    "<center><img src=\"img/slides/rcnn_4.jpg\"/ style=\"width:1000px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b8a726",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Region-based CNNs (R-CNN)\n",
    "***\n",
    "\n",
    "<center><img src=\"img/slides/rcnn_5.jpg\"/ style=\"width:1000px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7941788",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problems with R-CNN\n",
    "***\n",
    "Training is complicated:\n",
    "* Softmax loss for classification\n",
    "* SVM training with hinge loss\n",
    "* Bounding box regression with MSE\n",
    "\n",
    "The method is very slow:\n",
    "* 2000 convnet passes per image!\n",
    "* 47s/image during inference (in 2015)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3bda8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fast R-CNN\n",
    "***\n",
    "Proposed by Girshick @ICCV15\n",
    "\n",
    "<center><img src=\"img/slides/fast_rcnn_1.jpg\"/ style=\"width:1000px;\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155ac3ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fast R-CNN\n",
    "***\n",
    "Region proposal on convnet features!\n",
    "\n",
    "<center><img src=\"img/slides/fast_rcnn_2.jpg\"/ style=\"width:1000px;\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39b8ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fast R-CNN\n",
    "***\n",
    "RoI pooling for fixed size region proposal\n",
    "\n",
    "<center><img src=\"img/slides/fast_rcnn_3.jpg\"/ style=\"width:1000px;\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801da287",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fast R-CNN\n",
    "***\n",
    "<center><img src=\"img/slides/fast_rcnn_4.jpg\"/ style=\"width:1000px;\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6094e3e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fast R-CNN\n",
    "***\n",
    "<center><img src=\"img/slides/fast_rcnn_5.jpg\"/ style=\"width:1000px;\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b07f71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Region-of-interest Pooling\n",
    "***\n",
    "<center><img src=\"img/slides/fast_rcnn_6.jpg\"/ style=\"width:1200px;\"></center>\n",
    "\n",
    "Source: Stanford CS231 lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4cf13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's still wrong?\n",
    "***\n",
    "<img src=\"img/slides/rcnn_meme.jpg\" style=\"width:300px;float:right;\"/>\n",
    "\n",
    "The region proposals still come from an external source:\n",
    "* E.g. selective search, EdgeBoxes, ...\n",
    "\n",
    "\n",
    "Solution: Region proposal networks (RPN)\n",
    "* You guessed it: Faster R-CNN!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86151e71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Faster R-CNN\n",
    "***\n",
    "<center><img src=\"img/slides/faster_rcnn.jpg\"/ style=\"float:right;width:800px;\"></center>\n",
    "\n",
    "Proposed by Ren et al. @NIPS15:\n",
    "* Four losses:\n",
    "    * RPN classification object/no object\n",
    "    * RPN bounding box regression\n",
    "    * Final classification into object classes\n",
    "    * Final box regression\n",
    "\n",
    "Image sources: Stanford CS231 lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6168a33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Two-stage speed\n",
    "***\n",
    "<center><img src=\"img/slides/rcnn_speed.jpg\"/ style=\"width:600px;\"></center>\n",
    "\n",
    "Source: Stanford CS231 lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbaf6c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Single stage detection\n",
    "***\n",
    "Object detection without proposals\n",
    "* Prediction on a *grid*\n",
    "    * Divide image into multiple fixed cells\n",
    "    * Predict for each cell:\n",
    "        * Object class\n",
    "        * Bounding box coordinates for the complete object\n",
    "            * It might span multiple cells!\n",
    "\n",
    "Notable architectures/papers:\n",
    "* You Only Look Once (YOLO), by Redmon et al.\n",
    "* Single Shot Detection (SSD), by Liu et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a929e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grid-based detection\n",
    "***\n",
    "<center><img src=\"img/slides/coverage.png\"/ style=\"width:1000px;\"></center>\n",
    "\n",
    "Source: <a src=\"https://www.jeremyjordan.me/object-detection-one-stage/\">Jeremy Jordan</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50cccba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Network architecture\n",
    "***\n",
    "<center><img src=\"img/slides/anchors.jpg\"/ style=\"float:right;width:600px;\"></center>\n",
    "\n",
    "\n",
    "For each cell in a $H_c\\times W_c$ grid, we need:\n",
    "* The probability that the class $n$ is in that cell\n",
    "    * For $C$ classes: $H_c\\times W_c\\times C$ output map\n",
    "* The bounding box coordinates for an object in a cell\n",
    "    * Simple regression\n",
    "        * Needs similarly shaped BBs\n",
    "    * Anchor box proposals\n",
    "        * Simplifies finding the right shape\n",
    "        \n",
    "Image source: Stanford CS231 lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c68ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Single class detection example\n",
    "***\n",
    "<center><img src=\"img/slides/ssd.png\"/ style=\"width:800px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6304fd3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to train?\n",
    "***\n",
    "The coverage map is just a simple classification task:\n",
    "* Which cell contains an object?\n",
    "\n",
    "Bounding box regression is a little tricky:\n",
    "* Based on the position of the containing cell in the image\n",
    "    * Return bounding box coordinates relative to the cell position\n",
    "* What about labels for empty cells?\n",
    "    * Solution: Train only on cells for which we have objects\n",
    "        * Mask the regression loss!\n",
    "\n",
    "Our challenge will take you through this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a4b602",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to measure\n",
    "***\n",
    "Evaluation is done in steps\n",
    "1. Find all bounding boxes that have a high overlap with a true box\n",
    "    * Overlap measured relatively, also called intersection-over-union (IOU)\n",
    "    * Usually, we consider a box a match for $iou>0.5$\n",
    "2. Each groundtruth bounding box that was covered by a prediction is a *true positive*\n",
    "3. Unmatched groundtruth boxes are then *false negatives*\n",
    "4. All predicted boxes that were not matched to a true box are *false positives*\n",
    "\n",
    "With these metrics we can compute:\n",
    "* The ratio of predictions that were correct, aka *precision*:\n",
    "    * $precision=TP / (TP+FP)$\n",
    "* The ratio of groundtruth boxes, that we found:\n",
    "    * $recall=TP/(TP+FN)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f64a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thanks for the attention & have fun in the challenge!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "rise": {
   "auto_select": "none",
   "enable_chalkboard": true,
   "overlay": "<div class='myheader'><img src='img/ai_camp.png' class='ifis_small'></div><div class='ifis_large'><img src='img/ifis_large.png'></div>",
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
