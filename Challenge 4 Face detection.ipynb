{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701a160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%pip install opencv-python\n",
    "%pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c0940",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Object detection challenge: `WIDER` faces\n",
    "***\n",
    "For this challenge, we ask you to detect faces in the `WIDER` faces dataset\n",
    "* We use a stripped down version of the data to speed up the training process\n",
    "* Your task is to maximize for either:\n",
    "    * `precision`\n",
    "    * `recall`\n",
    "    * `overall`, which really is just precision + recall\n",
    "\n",
    "A fully functioning code example is provided, as always:\n",
    "* End-to-end face detection with a simple `CNN` architecture in `keras`\n",
    "* Feel free to adapt the code to your liking for best results.\n",
    "\n",
    "Post your questions and results in our discord channel!\n",
    "\n",
    "Happy hunting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f614d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Small note:\n",
    "The following cells contain the necessary code for data preprocessing etc. Just run the cells until you reach the \"Welcome back\" slide ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ceb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# image visualization utility\n",
    "def show_image_with_bbs(img, bbs):\n",
    "    '''img and bbs should be numpy arrays\n",
    "        bbs should have format [top, left, bottom, right]\n",
    "        if bbs.dtype == \"float32\":\n",
    "            we interprete bbs as relative bbs\n",
    "        elif bbs.dtype is int32:\n",
    "            we interprete them as absolute bbs\n",
    "    '''\n",
    "    if bbs.dtype == \"float32\":\n",
    "        H, W = img.shape[:2]\n",
    "        bbs[:, [1,3]] *= W\n",
    "        bbs[:, [0,2]] *= H\n",
    "        bbs = bbs.astype(\"int32\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    plt.imshow(img)\n",
    "    for y1,x1,y2,x2 in bbs:\n",
    "        rect = patches.Rectangle((min(x1,x2), min(y2,y1)), \n",
    "                                 abs(x2-x1), abs(y2-y1), \n",
    "                                 linewidth=1, edgecolor='r', \n",
    "                                 facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# image utilities for tensorflow and tf dataset\n",
    "\n",
    "def tf_resize_img_with_bbs(img, bbs, H, W):\n",
    "    '''\n",
    "    Resizes the image and the bboxes with padding.\n",
    "    img and bbs are expected to be Tensors.\n",
    "    '''\n",
    "    # compute the padding that will be added\n",
    "    shape = tf.shape(img)\n",
    "    h = shape[0]\n",
    "    w = shape[1]\n",
    "    scale = tf.cast(tf.minimum(H/h, W/w), \"float32\")\n",
    "    nw = tf.cast(w, \"float32\") * scale\n",
    "    nh = tf.cast(h, \"float32\") * scale\n",
    "\n",
    "    # we divide by 2 to get the padding per side, not in total\n",
    "    pad_x = (tf.cast(W, \"float32\") - nw)/2.0\n",
    "    pad_y = (tf.cast(H, \"float32\") - nh)/2.0\n",
    "\n",
    "    # resize the image\n",
    "    img = tf.image.resize_with_pad(img, H, W)\n",
    "    img = tf.cast(img, \"uint8\")\n",
    "\n",
    "    # transform bboxes\n",
    "    x_coords = (tf.gather(bbs, [1,3], axis=1)*nw + pad_x)/tf.cast(W, \"float32\")\n",
    "    y_coords = (tf.gather(bbs, [0,2], axis=1)*nh + pad_y)/tf.cast(H, \"float32\")\n",
    "    bbs = tf.stack([y_coords[:,0],x_coords[:,0],\n",
    "                    y_coords[:,1],x_coords[:,1],], axis=1)\n",
    "    return img, bbs\n",
    "\n",
    "def tfds_resize_img_with_bbs(H, W):\n",
    "    '''\n",
    "    TF Dataset variant of `tf_resize_img_with_bbs`. Expects \"image\" and a \"bbox\" column\n",
    "    '''\n",
    "    def t(sample):\n",
    "        img, bbs = tf_resize_img_with_bbs(sample[\"image\"], sample[\"bbox\"], H, W)\n",
    "        return {\"image\":img, \"bbox\":bbs}\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf and tfds preprocessing functions for object detection\n",
    "\n",
    "def tf_get_output_maps(bbs, H, W, stride):\n",
    "    th, tw = H//stride, W//stride\n",
    "    num_bbs = tf.shape(bbs)[0]\n",
    "    cx = (bbs[:, 1] + bbs[:, 3])/2.0\n",
    "    cy = (bbs[:, 0] + bbs[:, 2])/2.0\n",
    "    indices = tf.cast(tf.floor(tf.stack([cy*th, cx*tw], axis=1)), \"int32\")\n",
    "    values = tf.ones((num_bbs, 1))\n",
    "    cmap = tf.scatter_nd(indices,values, [th,tw,1])\n",
    "\n",
    "    # bb map\n",
    "    # get absolute bb coords wrt to input dims\n",
    "    abs_bbs = bbs * tf.cast([[H,W,H,W]],\"float32\")\n",
    "    # we want for each cell in bb_map to contain the\n",
    "    # offset of the bb in absolute pixels from the \n",
    "    # top left corner of the cell\n",
    "    coords = tf.cast(tf.tile(indices, [1, 2]), \"float32\")\n",
    "    # multiplying the cell indices by the network's stride\n",
    "    # we get the coords of the top left corner in the input\n",
    "    # image dims\n",
    "    coords = coords * tf.cast([[stride,stride,stride,stride]],\"float32\")\n",
    "    # by subtracting these coords, each bb is now expressed\n",
    "    # relative to the coordinates of the cell it is contained in\n",
    "    # this allows the network to learn position independent\n",
    "    # representations\n",
    "    rel_bbs = abs_bbs - coords\n",
    "    bb_map = tf.scatter_nd(indices,rel_bbs,[th, tw, 4])\n",
    "    return cmap, bb_map\n",
    "\n",
    "def tfds_get_output_maps(H, W, stride):\n",
    "    '''\n",
    "    TF dataset variant of tf_get_output_maps.\n",
    "    This function can be used to map a {.., \"bbox\":..}\n",
    "    dataset to {\"bbox\":.., coverage\":.., \"bbox_map\":..,...,}\n",
    "    '''\n",
    "    def f(sample):\n",
    "        cmap, bbmap = tf_get_output_maps(sample[\"bbox\"], H, W, stride)\n",
    "        sample = sample.copy()\n",
    "        sample.update({\"coverage\": cmap, \"bbox_map\": bbmap})\n",
    "        return sample\n",
    "    return f\n",
    "\n",
    "def tf_bbs_from_output_maps(coverage, bbmap, H, W, stride, threshold = 0.5):\n",
    "    '''\n",
    "    Given coverage[th x tw x 1] and bbmap [th x tw x 4] tensors,\n",
    "    computes the list of bounding boxes with a coverage>=threshold\n",
    "    bbs in bbmap are expected to be relative to their cell\n",
    "    Also supports batched args of shapes (B, th, tw, 1) and (B, th, tw, 4),\n",
    "    respectively.\n",
    "    positions.\n",
    "    Returns:\n",
    "        List of bbs relative to image size, or list thereof (for batched args)\n",
    "        \n",
    "        List of confidence scores of each bb, or list thereof (for batched args)\n",
    "    '''\n",
    "    return_batched = True\n",
    "    if coverage.ndim == 3:\n",
    "        return_batched = False\n",
    "        coverage = tf.expand_dims(coverage, 0)\n",
    "        bbmap = tf.expand_dims(bbmap, 0)\n",
    "\n",
    "    batch_size = tf.shape(bbmap)[0]\n",
    "    bb_list = []\n",
    "    score_list = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # get the absolute offsets for the predicted cells\n",
    "        indices = tf.where(coverage[i,:,:,0]>=threshold)\n",
    "        scores = tf.gather_nd(coverage[i], indices)\n",
    "        score_list.append(scores)\n",
    "\n",
    "        bb_offsets = tf.gather_nd(bbmap[i], indices)\n",
    "        # to reconstruct the bb according to the input image dimensions\n",
    "        # we need to compute which coords in the input image correspond\n",
    "        # to the cell in the output map (aka coverage, bbmap)\n",
    "        # This is done simply by multiplying the indices by the stride\n",
    "        # of the network\n",
    "        indices = tf.cast(tf.tile(indices, [1, 2]),\"float32\")\n",
    "        input_coords = indices * tf.cast([[stride,stride,stride,stride]],\"float32\")\n",
    "        bbs = input_coords + bb_offsets\n",
    "    \n",
    "        bb_list.append(bbs)\n",
    "\n",
    "    return (bb_list,score_list) if return_batched else (bb_list[0], score_list[0])\n",
    "\n",
    "def tfds_bbs_from_output_maps(H, W, stride, threshold=0.5):\n",
    "    '''\n",
    "    TF Dataset equivalent of `tf_bbs_from_output_maps`.\n",
    "    Expects \"coverage\" and \"bbox_map\" columns to exist.\n",
    "    \n",
    "    '''\n",
    "    def t(sample):\n",
    "        bbs, scores = tf_bbs_from_output_maps(sample[\"coverage\"], sample[\"bbox_map\"], H, W, stride, threshold)\n",
    "        sample = sample.copy()\n",
    "        sample.update({\"bbox_pred\": bbs, \"scores\":scores})\n",
    "        return sample\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics for object detection\n",
    "\n",
    "def tf_iou(bbs_true, bbs_pred):\n",
    "    '''\n",
    "    Computes the pair-wise intersection-over-union metric\n",
    "    for the given bounding box tensors.\n",
    "    Both bbs_true and bbs_pred are expected to have shape\n",
    "    [X,4], where X might be different.\n",
    "    \n",
    "    Returns a num_pred_bbs x num_true_bbs\n",
    "    '''\n",
    "    # assume bbs are sorted correctly\n",
    "    n_true = tf.shape(bbs_true)[0]\n",
    "    n_pred = tf.shape(bbs_pred)[0]\n",
    "    # tiled true bbs\n",
    "    bbtt = tf.tile(bbs_true[tf.newaxis],(n_pred,1,1))\n",
    "    # tiled pred bbs\n",
    "    bbpt = tf.tile(bbs_pred[:,tf.newaxis],(1,n_true,1))\n",
    "    # get the highest top, left and the lowest right, bottom\n",
    "    # components for each pair of true,pred bb\n",
    "    # These components are the intersections between true and\n",
    "    # pred pairs.\n",
    "    intersections = tf.stack([\n",
    "        tf.where(bbtt[:,:,0]>bbpt[:,:,0], bbtt[:,:,0], bbpt[:,:,0]),\n",
    "        tf.where(bbtt[:,:,1]>bbpt[:,:,1], bbtt[:,:,1], bbpt[:,:,1]),\n",
    "        tf.where(bbtt[:,:,2]<bbpt[:,:,2], bbtt[:,:,2], bbpt[:,:,2]),\n",
    "        tf.where(bbtt[:,:,3]<bbpt[:,:,3], bbtt[:,:,3], bbpt[:,:,3])\n",
    "    ],axis=-1)\n",
    "\n",
    "    # filter out non-overlapping candidates\n",
    "    index = tf.logical_and(intersections[:,:,0]<intersections[:,:,2],\n",
    "                           intersections[:,:,1]<intersections[:,:,3])\n",
    "    non_empty = intersections[index]\n",
    "    # compute area of the intersections\n",
    "    inter_area = (non_empty[:,2] - non_empty[:,0])*(non_empty[:,3] - non_empty[:,1])\n",
    "\n",
    "    # get only those bbs that have an overlap\n",
    "    bbtto = bbtt[index]\n",
    "    bbpto = bbpt[index]\n",
    "    # next we need the union of those pairs.\n",
    "    # we compute the union as the sum of both areas minus the intersection area\n",
    "    summed_area = (bbtto[:,2]-bbtto[:,0])*(bbtto[:,3]-bbtto[:,1]) + \\\n",
    "                    (bbpto[:,2]-bbpto[:,0])*(bbpto[:,3]-bbpto[:,1])\n",
    "\n",
    "    union_area = summed_area - inter_area\n",
    "    # these are the ious of only the overlapping pairs\n",
    "    # we project them back into the n_pred x n_true matrix\n",
    "    # using the index in the next step. Luckily, non-\n",
    "    # overlapping bbs automatically have an iou of zero.\n",
    "    ious = inter_area / union_area\n",
    "    # project back into 2d matrix where non overlaps are zero.\n",
    "    ious = tf.scatter_nd(tf.where(index), ious,(n_pred,n_true))\n",
    "    return ious\n",
    "\n",
    "def tf_precision_recall(bbs_true, bbs_pred, scores, iou_threshold = 0.5):\n",
    "    '''\n",
    "        Computes precision and recall values for ground truth and predicted\n",
    "        bounding box arrays (N x [t,l,b,r]) and (M x [t,l,b,r]).\n",
    "        `scores` (M x 1) contains the confidence in the predicted bbs \n",
    "        It works as follows:\n",
    "            For each ground truth bounding box, find all \n",
    "            predicted bbs with an iou > iou_threshold.\n",
    "            If there are one or more bbs:\n",
    "                The bb with the highest confidence is considered a true positive\n",
    "                for the current ground truth bb, whereas the other matched bbs are\n",
    "                considered false positives.\n",
    "            else:\n",
    "                the groundtruth bb is considered a false negative\n",
    "            After the above procedure, any remaining predicted bounding boxes \n",
    "            are then automatically false positives\n",
    "    '''\n",
    "    n_true = tf.shape(bbs_true)[0]\n",
    "    n_pred = tf.shape(bbs_pred)[0]\n",
    "\n",
    "    scores = tf.squeeze(scores, axis=-1)\n",
    "    \n",
    "    # a bool map indicating which pred bb has been selected already\n",
    "    available = tf.ones((n_pred,), \"bool\")\n",
    "    fns = 0 # true bbs that werent covered (false negatives)\n",
    "    tps = 0 # true bbs that were covered (true positives)\n",
    "\n",
    "    ious = tf_iou(bbs_true, bbs_pred)\n",
    "\n",
    "    for i in range(n_true):\n",
    "        # find all bbs pred with greater iou than threshold,\n",
    "        # which have not been selected yet\n",
    "        index = tf.logical_and(ious[:,i] >= iou_threshold, available)\n",
    "        if tf.reduce_any(index):\n",
    "            indices = tf.where(index)\n",
    "            # find the bb among the candidates with the highest score\n",
    "            max_conf_index = tf.argmax(scores[index])\n",
    "            bb_index = indices[max_conf_index]\n",
    "            # update the availability index\n",
    "            available = tf.tensor_scatter_nd_update(available,[bb_index],[False])\n",
    "            tps += 1\n",
    "        else:\n",
    "            # the true bb was not covered by any prediction :(\n",
    "            fns += 1\n",
    "\n",
    "    # pred bbs that didnt ever cover a true bb (false positives)\n",
    "    fps = tf.cast(tf.math.count_nonzero(available),\"float32\")\n",
    "\n",
    "    # make sure all the types match for the next calculations...\n",
    "    fns = tf.convert_to_tensor(fns,\"float32\")\n",
    "    tps = tf.convert_to_tensor(tps,\"float32\")\n",
    "\n",
    "    # compute the true positive rate (aka precision)\n",
    "    precision = tf.math.divide_no_nan(tps, tps+fps)\n",
    "    # compute how many true bbs where \"hit\" by a prediction (aka recall)\n",
    "    recall = tf.math.divide_no_nan(tps, tps+fns)\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "def tf_precision_recall_batch(bbs_true, bbs_pred, scores, iou_threshold=0.5):\n",
    "    '''\n",
    "    Batched variant of `tf_precision_recall`.\n",
    "    Expected args:\n",
    "        `bbs_true`: List of [Xs,4] tensors, with varying Xs\n",
    "        `bbs_pred`: List of [Ys,4] tensors, with varying Ys\n",
    "        `scores`: List of [Ys] tensors.\n",
    "        All lists need to have the same length (batch_size).\n",
    "\n",
    "    See `tf_precision_recall` for semantics for the elements of\n",
    "    the lists.\n",
    "    We compute the per sample precision/recall in parallel and\n",
    "    return the mean.\n",
    "    '''\n",
    "    ps,rs = [], []\n",
    "    batch_size = len(bbs_true)\n",
    "    for i in range(batch_size):\n",
    "        p,r = tf_precision_recall(bbs_true[i], bbs_pred[i], scores[i], \n",
    "                                  iou_threshold=iou_threshold)\n",
    "        ps.append(p)\n",
    "        rs.append(r)\n",
    "    precision = tf.reduce_mean(ps)\n",
    "    recall = tf.reduce_mean(rs)\n",
    "    return precision, recall\n",
    "\n",
    "def tf_precision_recall_from_output_maps(bbs_true,\n",
    "                                         coverage_pred, bb_map_pred,\n",
    "                                         H,W, \n",
    "                                         coverage_threshold=0.5,\n",
    "                                         iou_threshold=0.5,):\n",
    "    '''\n",
    "    Computes the precision and recall for a batch of coverage/bb_map\n",
    "    pairs.\n",
    "    The expected args are:\n",
    "        `bbs_true`: List of [X,4] tensors containing the true bbs\n",
    "        `coverage_pred`: Tensor of shape [B, th, tw, 1]\n",
    "        `bb_map_pred`: Tensor of shape [B, th, tw, 4]\n",
    "            Values are expected to be absolute pixel offsets from the \n",
    "            original image H/W.\n",
    "        `H`,`W`: Height and width of input images (int)\n",
    "    Returns the mean precision/recall over the batch \n",
    "    '''\n",
    "    # TODO: write a method that takes single samples of the maps and returns\n",
    "    # the sample precision/recall. This should be vectorizable!\n",
    "    th = tf.shape(coverage_pred)[1]\n",
    "    stride = H//th\n",
    "    bbs_pred, scores = tf_bbs_from_output_maps(coverage_pred, bb_map_pred,H,W,\n",
    "                                      stride,threshold=coverage_threshold)\n",
    "    return tf_precision_recall_batch(bbs_true, bbs_pred, scores, \n",
    "                                    iou_threshold=iou_threshold)\n",
    "\n",
    "def tf_precision_recall_curve(bbs_true, bbs_pred, scores, thresholds=None, iou_threshold=0.5):\n",
    "    '''\n",
    "    Computes the PR curve for the given true and predicted bounding boxes [NO BATCHES].\n",
    "    `thresholds` is a list of specific recall values at which to interpolate the curve.\n",
    "        If None is given, the complete curve will be computed.\n",
    "        Make sure to contain 0 and 1 as values.\n",
    "    `bbs_pred` should contain all bounding boxes with scores within min and max thresholds.\n",
    "    Returns precision, recall and thresholds\n",
    "    '''\n",
    "    scores = tf.squeeze(scores, -1)\n",
    "\n",
    "    if thresholds is None:\n",
    "        t,_ = tf.unique(tf.concat([[0,1],scores], axis=0))\n",
    "        thresholds = tf.sort(t)\n",
    "\n",
    "    ps, rs = [], []\n",
    "    for t in thresholds:\n",
    "        index = scores >= t\n",
    "        p,r = tf_precision_recall(bbs_true, bbs_pred[index], \n",
    "                                  tf.expand_dims(scores[index],-1), \n",
    "                                  iou_threshold)\n",
    "        ps.append(p); rs.append(r)\n",
    "    precision = tf.stack(ps)\n",
    "    recall = tf.stack(rs)\n",
    "    return precision, recall, thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a633561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom losses\n",
    "\n",
    "def masked_loss(loss):\n",
    "    '''\n",
    "    Returns the loss computed ONLY on the TRUE values of\n",
    "    y_true!\n",
    "    This is useful if you only have labels for the true class\n",
    "    but not the false class. Dont expect this to work well\n",
    "    on classification though.\n",
    "    '''\n",
    "    def l(y_true, y_pred):\n",
    "        yp = tf.cast(y_true != 0., \"float32\") * y_pred\n",
    "        return loss(y_true, yp)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7948c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation and validation code\n",
    "\n",
    "\n",
    "def evaluate_model(dataset, model, coverage_threshold=0.5, iou_threshold=0.5):\n",
    "    '''\n",
    "    Runs evaluation on the given dataset and model.\n",
    "    `dataset` must be from the `tfds_wider_test_pipeline`!\n",
    "\n",
    "    '''\n",
    "    H,W = model.input_shape[1:3]\n",
    "    th, tw = model.output_shape[0][1:3]\n",
    "    stride = H//th\n",
    "\n",
    "    model.reset_metrics()\n",
    "    ps, rs = [], []\n",
    "    for x, bbs_true, (cov, bbm) in tqdm(dataset):\n",
    "        # the true bbs are in relative coords, so we convert them\n",
    "        bbs_true = bbs_true * tf.cast([[H,W,H,W]],\"float32\")\n",
    "        # we need to create batches of size 1 for inference...\n",
    "        x = tf.expand_dims(x, 0)\n",
    "        cov = tf.expand_dims(cov,0)\n",
    "        bbm = tf.expand_dims(bbm,0)\n",
    "        y = (cov, bbm)\n",
    "\n",
    "        y_pred = model(x, training=False)\n",
    "        cov_pred, bbm_pred = y_pred\n",
    "\n",
    "        # evaluate losses and metrics\n",
    "        if model.compiled_loss is not None:\n",
    "            model.compiled_loss(y, y_pred, regularization_losses=model.losses)\n",
    "        if model.compiled_metrics is not None:\n",
    "            model.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        # calculate precision and recall\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            bbs_pred, scores = tf_bbs_from_output_maps(cov_pred, bbm_pred, H,W,\n",
    "                                                       stride, coverage_threshold)\n",
    "            p, r = tf_precision_recall(bbs_true, bbs_pred[0], scores[0], iou_threshold)\n",
    "        ps.append(p); rs.append(r)\n",
    "\n",
    "    precision = tf.reduce_mean(ps)\n",
    "    recall = tf.reduce_mean(rs)\n",
    "\n",
    "    # Collect metrics\n",
    "    result_metrics = {\"precision\":precision, \"recall\":recall}\n",
    "    for metric in model.metrics:\n",
    "        result = metric.result()\n",
    "        if isinstance(result, dict):\n",
    "            result_metrics.update(results)\n",
    "        else:\n",
    "            result_metrics[metric.name] = result\n",
    "    return result_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f78441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom callbacks\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PlotCallback(keras.callbacks.Callback):\n",
    "    '''\n",
    "    Callback to plot examples after each epoch\n",
    "    '''\n",
    "    def __init__(self, imgs, threshold=0.5, max_bbs=50):\n",
    "        keras.callbacks.Callback.__init__(self)\n",
    "        self.imgs = imgs\n",
    "        self.threshold = threshold\n",
    "        self.max_bbs = max_bbs\n",
    "\n",
    "    def on_epoch_end(self, *args):\n",
    "        H,W = self.model.input_shape[1:3]\n",
    "        th, tw = self.model.output_shape[0][1:3]\n",
    "        stride = H//th\n",
    "        cov, bbm = self.model.predict(self.imgs)\n",
    "        bb_list, scores = tf_bbs_from_output_maps(cov, bbm, H, W, stride, self.threshold)\n",
    "\n",
    "        for i in range(len(self.imgs)):\n",
    "            fig, (left, right) = plt.subplots(1,2, squeeze=True)\n",
    "            left.imshow(self.imgs[i])\n",
    "            bbs = bb_list[i].numpy()[:self.max_bbs]\n",
    "            bbs = bbs.astype(\"int32\")\n",
    "            for y1,x1,y2,x2 in bbs:\n",
    "                rect = patches.Rectangle((min(x1,x2), min(y2,y1)), \n",
    "                                         abs(x2-x1), abs(y2-y1), \n",
    "                                         linewidth=1, edgecolor='r', \n",
    "                                         facecolor='none')\n",
    "                left.add_patch(rect)\n",
    "            im = right.imshow(cov[i,:,:,0])\n",
    "            fig.colorbar(im, ax=right)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "class MetricCallback(keras.callbacks.Callback):\n",
    "    '''\n",
    "    A special callback for cutomized metric calculation.\n",
    "    It serves two purposes:\n",
    "        1. To compute precision and recall, we need both\n",
    "        coverage and bb_map layer outputs as inputs to a\n",
    "        single metric function. This is not supported by\n",
    "        keras natively. Hence, we do this in this callback.\n",
    "        2. Doing 1. would result in two passes over the\n",
    "        validation data: One for the validation losses etc.\n",
    "        and one for precicsion/recall. As a solution, we\n",
    "        also do the \"standard\" validation that keras would\n",
    "        usually take care of. Specifically, for each metric\n",
    "        of the model, we add a \"val_\" version of it based\n",
    "        on the results of the validation data.\n",
    "    Summing up, we add the following metrics:\n",
    "        val_precision\n",
    "        val_recall\n",
    "        val_loss\n",
    "        Plus any other \"val_\" + X metric of the model.\n",
    "    As a downside, the default keras epoch report won't\n",
    "    show our additional metrics.\n",
    "    However, they do show up in the returned History callback.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dataset, coverage_threshold=0.5, iou_threshold=0.5):\n",
    "        '''\n",
    "        `dataset` must be from the `tfds_wider_test_pipeline`!\n",
    "        '''\n",
    "        self.dataset = dataset\n",
    "        self.coverage_threshold = coverage_threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        metrics = evaluate_model(self.dataset, self.model, \n",
    "                                 self.coverage_threshold,\n",
    "                                 self.iou_threshold)\n",
    "        val_metrics = {\"val_\"+k:v for k,v in metrics.items()}\n",
    "        logs.update(val_metrics)\n",
    "\n",
    "        # create a small report\n",
    "        line = \" - \".join([f\"{k}: {v:.4f}\" for k,v in logs.items()])\n",
    "        print(f\"[VAL {epoch+1}/{self.params['epochs']}]\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ad144c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# WIDER corpus loading & preprocessing\n",
    "\n",
    "def tf_filter_bbs(bbs, min_h=0, max_h=1.0, min_w=0, max_w=1.0):\n",
    "    '''\n",
    "    Given a tensor of relative bbs, return those that lie\n",
    "    within the given constraints.\n",
    "    '''\n",
    "    bbh = bbs[:,2] - bbs[:,0]\n",
    "    bbw = bbs[:,3] - bbs[:,1]\n",
    "    height_res = tf.logical_and(bbh>=min_h, bbh<=max_h)\n",
    "    width_res = tf.logical_and(bbw>=min_w, bbw<=max_w)\n",
    "    restrictions = tf.logical_and(height_res, width_res)\n",
    "    bbs_new = tf.boolean_mask(bbs, restrictions)\n",
    "    return bbs_new\n",
    "\n",
    "def tfds_filter_bbs(min_h=0, max_h=1.0, min_w=0, max_w=1.0):\n",
    "    def t(sample):\n",
    "        bbs = tf_filter_bbs(sample[\"bbox\"], min_h, max_h, min_w, max_w)\n",
    "        sample = sample.copy()\n",
    "        sample.update({\"bbox\": bbs})\n",
    "        return sample\n",
    "    return t    \n",
    "\n",
    "def tfds_wider_train_pipeline(ds, H, W, stride, batch_size):\n",
    "    '''\n",
    "    The pipeline to prepare the wider corpus for training.\n",
    "    Filters out any image with more than 3 faces\n",
    "    Converts the bounding boxes to coverage and bb maps\n",
    "    Resizes the images to uniform height and width\n",
    "    Caches, batches and prefetches the pipeline.\n",
    "    Returns a dataset of tuples (image, (coverage, bb_map))\n",
    "    '''\n",
    "    ds = ds.map(lambda x: {\"image\":x[\"image\"], \"bbox\": x[\"faces\"][\"bbox\"]})\n",
    "    ds = ds.filter(lambda x: len(x[\"bbox\"])>0 and len(x[\"bbox\"])<4)\n",
    "    ds = ds.map(tfds_resize_img_with_bbs(H, W))\n",
    "    ds = ds.map(tfds_get_output_maps(H,W, stride))\n",
    "    \n",
    "    ds = ds.map(lambda x: (x[\"image\"], (x[\"coverage\"],x[\"bbox_map\"])))\n",
    "    ds = ds.cache()\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def tfds_wider_test_pipeline(ds, H, W, stride, batch_size):\n",
    "    '''\n",
    "    The pipeline to prepare the wider corpus for testing.\n",
    "\n",
    "    Filters out any image with more than 3 faces\n",
    "    Converts the bounding boxes to coverage and bb maps\n",
    "    Resizes the images to uniform height and width\n",
    "    Caches and prefetches the pipeline. (no batching!)\n",
    "\n",
    "    Returns tuple of (image, bbs_true, (coverage, bb_map))\n",
    "    '''\n",
    "    ds = ds.map(lambda x: {\"image\":x[\"image\"], \"bbox\": x[\"faces\"][\"bbox\"]})\n",
    "    ds = ds.filter(lambda x: len(x[\"bbox\"])>0 and len(x[\"bbox\"])<4)\n",
    "    ds = ds.map(tfds_resize_img_with_bbs(H, W))\n",
    "    ds = ds.map(tfds_get_output_maps(H,W, stride))\n",
    "    \n",
    "    ds = ds.map(lambda x: (x[\"image\"],x[\"bbox\"], (x[\"coverage\"],x[\"bbox_map\"])))\n",
    "    ds = ds.cache()\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def get_wider_data(H, W, stride, batch_size, val_split=None):\n",
    "    '''\n",
    "    Loads the WIDER corpus as three TF Datasets:\n",
    "    \"train\": The training data after cutting of validation\n",
    "    \"val\": The validation data, sampled from the train samples\n",
    "    \"test\": Completely independent from train and val, only for\n",
    "        testing purposes. (We use WIDER validation split for testing)\n",
    "    Parameters:\n",
    "        `H`,`W`: Height and width for the images\n",
    "        `stride`: Factor by which your network scales down H and W,\n",
    "            usually through Pooling. Must be an integer!\n",
    "        `batch_size`: you probably know this one..\n",
    "        `val_split`: float, proportion of training samples to use\n",
    "            for validation. None means no validation! Should be\n",
    "            0<val_split<1 or None.\n",
    "    Returns:\n",
    "        (train, val, test) data if val_split is not None or\n",
    "        (train, test) else\n",
    "    '''\n",
    "    if val_split is not None:\n",
    "        n = int(val_split*100)\n",
    "        split = [f\"train[{n}%:]\",f\"train[:{n}%]\", \"validation\"]\n",
    "        (ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
    "            'wider_face',\n",
    "            split=split,\n",
    "            shuffle_files=True,\n",
    "            with_info=True,\n",
    "        )\n",
    "        ds_val = tfds_wider_test_pipeline(ds_val, H, W, stride, batch_size)\n",
    "    else:\n",
    "        (ds_train, ds_test), ds_info = tfds.load(\n",
    "            'wider_face',\n",
    "            split=['train', 'validation'],\n",
    "            shuffle_files=True,\n",
    "            with_info=True,\n",
    "        )\n",
    "    # setup data pipelines\n",
    "    ds_train = tfds_wider_train_pipeline(ds_train, H, W, stride, batch_size)\n",
    "    ds_test = tfds_wider_test_pipeline(ds_test, H, W, stride, batch_size)\n",
    "\n",
    "    if val_split is not None:\n",
    "        return ds_train, ds_val, ds_test\n",
    "    else:\n",
    "        return ds_train, ds_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca1aa4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Welcome back\n",
    "***\n",
    "The pre- and post processing code ends here. Let's start with the training!\n",
    "\n",
    "First: Specify the dataset dimensions and load the WIDER corpus\n",
    "\n",
    "Remember: `stride` is the factor by which your network reduces the image dimensions.\\\n",
    "E.g. `stride=8` means that a 128x256 image becomes a 16x32 output grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = 128, 256\n",
    "stride = 8\n",
    "batch_size = 64\n",
    "val_split = 0.05\n",
    "th, tw = H//stride, W//stride\n",
    "ds_train, ds_val, ds_test = get_wider_data(H, W, stride, batch_size, val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da038d3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building the model\n",
    "***\n",
    "We'll build a pretty simple model of three blocks:\n",
    "* 3x `Conv2D` followed by a `MaxPool2D`\n",
    "    This is the 'meat' of the network\n",
    "\n",
    "After that, we form the model output:\n",
    "* The `bbox` layer with 4 output channels, one for each bounding box coordinate\n",
    "* The `coverage` layer, which acts as a classification grid to spot pixels that contain a face\n",
    "\n",
    "Finally, we build the model and print the summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab3754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, GlobalMaxPool2D, SpatialDropout2D, Dense, BatchNormalization,\n",
    "    Input, MaxPool2D, AvgPool2D\n",
    ")\n",
    "\n",
    "input_layer = Input((H,W,3), name=\"image\")\n",
    "l = input_layer\n",
    "for i in range(3):\n",
    "    for i in range(3):\n",
    "        l = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(l)\n",
    "    l = MaxPool2D((2,2))(l)\n",
    "\n",
    "l = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(l)\n",
    "\n",
    "bbox_layer = Conv2D(4, (3,3), activation=\"linear\", padding=\"same\", name=\"bbox\")(l)\n",
    "coverage_layer = Conv2D(1, (3,3), activation=\"sigmoid\", padding=\"same\", name=\"coverage\")(l)\n",
    "\n",
    "model = keras.models.Model(input_layer, [coverage_layer, bbox_layer])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7d620",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preparing the training\n",
    "***\n",
    "To have some more insight into the training process there are two Callbacks to use\n",
    "* `MetricCallback`: It computes precision and recall metrics on our validation data\n",
    "* `PlotCallback`: Draws `n_plot=3` example predictions of the model after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc62c61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take a few validation samples for plotting\n",
    "n_plot = 3\n",
    "ds_plot = ds_val.take(n_plot)\n",
    "imgs = np.stack([x for x,_,_ in ds_plot])\n",
    "\n",
    "# setup callbacks\n",
    "cb = [\n",
    "    MetricCallback(ds_val, coverage_threshold=0.2),\n",
    "    PlotCallback(imgs, threshold=0.1),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b1ea5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Time to train\n",
    "***\n",
    "We need two losses to train our object detector:\n",
    "* `BinaryCrossentropy`: A classification loss for our coverage layer\n",
    "* `MaskedMSE`: A regression loss for bounding box prediction, that only works on the positive class\n",
    "    * The mask is needed since we do not have any bounding boxes for negative classes (what would they even be?)\n",
    "\n",
    "Then, just compile the model with an optimizer of your liking and start the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30468ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = {\n",
    "    \"coverage\": \"binary_crossentropy\",\n",
    "    \"bbox\": masked_loss(keras.losses.MSE),\n",
    "}\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(opt, loss)\n",
    "\n",
    "model.fit(ds_train, epochs=1, verbose=1, callbacks=cb,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b950f35b",
   "metadata": {},
   "source": [
    "# Training visualization\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a73490",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,8))\n",
    "for k, v in model.history.history.items():\n",
    "    plt.plot(v, label=k)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df04b85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model evaluation\n",
    "***\n",
    "We are looking for three best models (at `iou=0.5`):\n",
    "* Max precision\n",
    "* Max recall\n",
    "* Max overall\n",
    "    * Just add recall+precision\n",
    "\n",
    "As always: Post your solutions in the discord!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_model(ds_test, model, coverage_threshold=0.2)\n",
    "print(\"Precision\", metrics[\"precision\"])\n",
    "print(\"Recall\", metrics[\"recall\"])\n",
    "print(\"Overall\", metrics[\"precision\"] + metrics[\"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962fe3a0",
   "metadata": {},
   "source": [
    "# Hints\n",
    "***\n",
    "Here are some areas that will certainly improve your precision + recall:\n",
    "* Model architecture\n",
    "    * The current model is very straight forward. Can you come up with a better one?\n",
    "    * Notable architectures: ResNet, DenseNet, Hour-Glass\n",
    "* Bounding box post processing\n",
    "    * Non maxima suppression: Group together strongly overlapping candidates\n",
    "        * This could be used in the `evaluate_model` function!\n",
    "* Image resolution\n",
    "    * Could input image size and the network stride have positive impact on performance?\n",
    "* `coverage_threshold`:\n",
    "    * Which threshold gives the best overall results?\n",
    "    * Try the evaluation code with different values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427b3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "rise": {
   "auto_select": "none",
   "enable_chalkboard": true,
   "overlay": "<div class='myheader'><img src='img/ai_camp.png' class='ifis_small'></div><div class='ifis_large'><img src='img/ifis_large.png'></div>",
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
