{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "difficult-advantage",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Setup START\n",
    "***\n",
    "Skipped in slideshow mode, run manually before!\n",
    "\n",
    "Ideally, the code would be placed with their corresponding content slides.\\\n",
    "Unfortunately, RISE slideshow mode will not execute SKIP slides and there is no other way to hide them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-pitch",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "# OpenAI Gym related:\n",
    "%pip install cmake\n",
    "%pip install atari_py \n",
    "'''OpenAI devs actually messed up an indentation\n",
    "    in the video recorder class in 0.18.x.\n",
    "    If you want to see videos, wait for a patch or\n",
    "    use 0.17.x.\n",
    "'''\n",
    "%pip install gym[box2d]==0.17.3\n",
    "%pip install pyvirtualdisplay\n",
    "%pip install tqdm\n",
    "\n",
    "# Gridworld related:\n",
    "%pip install pymdptoolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-advantage",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !apt-get install -y xvfb python-opengl > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-negative",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import numpy as np\n",
    "import base64\n",
    "import io\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "if IN_COLAB:\n",
    "    from pyvirtualdisplay import Display\n",
    "    d = Display()\n",
    "    d.start()\n",
    "\n",
    "'''\n",
    "Below is a solution to record and display videos for\n",
    "OpenAI Gym environments.\n",
    "Why the hassle?\n",
    "    It works both in Jupyter Notebook and Google Colab!\n",
    "(The author would like to note how painful it has been\n",
    "developing this)\n",
    "'''    \n",
    "\n",
    "def display_video_from_monitor(monitor):\n",
    "    '''\n",
    "    Converts all videos in a monitor to HTML videos\n",
    "    '''\n",
    "    if len(monitor.videos) == 0:\n",
    "        print(\"No videos to render!\")\n",
    "        return\n",
    "    for f in monitor.videos:\n",
    "        video = io.open(f[0], 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        display.display(display.HTML(data=\"\"\"\n",
    "            <video alt=\"test\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "            </video>\n",
    "            \"\"\".format(encoded.decode('ascii'))))\n",
    "    return\n",
    "\n",
    "def make_video(env, model, max_steps=None):\n",
    "    mon = Monitor(env, \"/data/videos/\", force=True)\n",
    "    state = mon.reset()\n",
    "    i = 0\n",
    "    while True:\n",
    "        action = model(state)\n",
    "        state, r, d, _ = mon.step(action)\n",
    "        if d: break\n",
    "        i+=1\n",
    "        if max_steps is not None and i>=max_steps: break\n",
    "    if mon.stats_recorder.done == False:    \n",
    "        mon.stats_recorder.save_complete()\n",
    "        mon.stats_recorder.done = True\n",
    "    mon.reset()\n",
    "    display_video_from_monitor(mon)\n",
    "    return mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-position",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from mdptoolbox import mdp\n",
    "\n",
    "class Gridworld():\n",
    "    def __init__(self, actions, w=4, h=3, rho=1.0, step_reward=0.0, cell_rewards={},\n",
    "               terminal_states=[], blocked_cells=[]):\n",
    "        self.actions = actions\n",
    "        self.w, self.h = w, h\n",
    "        self.rho = rho\n",
    "        self.step_reward = step_reward\n",
    "        self.cell_rewards = cell_rewards\n",
    "        self.terminal_states = {tuple(x) for x in terminal_states}\n",
    "        self.blocked_cells = {tuple(x) for x in blocked_cells}\n",
    "\n",
    "        self.grid = np.zeros((h,w))\n",
    "        self.states = np.array([[i,j] for i in range(h) for j in range(w)])\n",
    "        self.num_states = w*h\n",
    "        self.num_actions = len(actions)\n",
    "        self.p = np.zeros((self.num_actions, self.num_states, self.num_states), \"float32\")\n",
    "        self.r = np.zeros((self.num_actions, self.num_states, self.num_states), \"float32\")\n",
    "        self._fill_p()\n",
    "        self._fill_r()\n",
    "\n",
    "    def _fill_p(self):\n",
    "        # computing transition matrix:\n",
    "        for a in range(self.num_actions):\n",
    "            for i in range(self.num_states):\n",
    "                # simulate performing action a_ in state i\n",
    "                # action a is performed with prob. rho,\n",
    "                # with prob. 1-rho, any other action is chosen,\n",
    "                # which results in prob. (1-rho)/(num_moves-1) for\n",
    "                # a specific action a_ != a\n",
    "                for a_ in range(self.num_actions):\n",
    "                    rh, rw = self.simulate_move(self.actions[a_], self.states[i])\n",
    "                    j_ = rh*self.w + rw\n",
    "                    prob = self.rho if a == a_ else (1-self.rho)/(self.num_actions-1)\n",
    "                    self.p[a, i, j_] += prob\n",
    "                # due to numerical inaccuracies, we need to ensure that each row\n",
    "                # adds up to exactly 1, not 0.999995, because the solvers\n",
    "                # really dont seem to like that\n",
    "                row_sum = self.p[a,i].sum()\n",
    "                if row_sum != 1:\n",
    "                    if not np.allclose(row_sum,1, rtol=1e-5, atol=1e-5):\n",
    "                        raise ValueError(f\"Probability error at action{a}, state {i}\"\n",
    "                                         f\" with prob. {row_sum}\")\n",
    "                    self.p[a,i] /= row_sum\n",
    "        return\n",
    "        # make sure we can't ever leave terminal states\n",
    "        for (i,j) in self.terminal_states:\n",
    "            # transition probability from state (i,j) to any other state \n",
    "            # must be zero, but one for (i,j)\n",
    "            index = i*self.w+j\n",
    "            self.p[:,index,:] = 0\n",
    "            self.p[:, index, index] = 1\n",
    "\n",
    "    def _fill_r(self):\n",
    "        # transitions always give step_reward\n",
    "        self.r.fill(self.step_reward)\n",
    "        \n",
    "        # ending on a rewarded cell gives the corresponding reward\n",
    "        for (i,j), reward in self.cell_rewards.items():\n",
    "            self.r[:,:, i*self.w+j] += reward\n",
    "\n",
    "        # terminal cells do not give ANY reward after having reached them\n",
    "        for (i,j) in self.terminal_states:\n",
    "            k = i*self.w+j\n",
    "            self.r[:, k, :] = 0\n",
    "\n",
    "        # should staying on cells still give the associated rewards? No!\n",
    "        for i in range(self.num_actions):\n",
    "            np.fill_diagonal(self.r[i],0)\n",
    "\n",
    "    def simulate_move(self, move, state):\n",
    "        if self._is_blocked(state): return state\n",
    "        if self._is_terminal(state): \n",
    "            return state\n",
    "        nm = np.clip(np.array(move)+np.array(state), 0, (self.h-1, self.w-1))\n",
    "        if self._is_blocked(nm):\n",
    "            return state\n",
    "        return nm\n",
    "\n",
    "    def solve(self, discount=0.99, verbose=False):\n",
    "        '''\n",
    "        Solves the underlying MDP using Policy Iteration\n",
    "        Returns an object with the optimal policy. See\n",
    "        `mdp.PolicyIteration` doc for details.\n",
    "        '''\n",
    "        vi = mdp.PolicyIteration(self.p, self.r, discount)\n",
    "        if verbose:\n",
    "            vi.setVerbose()\n",
    "        vi.run()\n",
    "        return vi\n",
    "    \n",
    "    def _is_blocked(self, cy, cx=None):\n",
    "        # cx: position along grid width\n",
    "        if cx is None:\n",
    "            cy, cx = cy\n",
    "        return (cy, cx) in self.blocked_cells\n",
    "\n",
    "    def _is_terminal(self, cy, cx=None):\n",
    "        if cx is None:\n",
    "            cy, cx = cy\n",
    "        return (cy, cx) in self.terminal_states\n",
    "    \n",
    "    def _plot_policy(self, policy):\n",
    "        a_l = 0.6 # arrow length\n",
    "        for (i,j), a in list(zip(self.states, policy)):\n",
    "            if self._is_terminal(i,j) or self._is_blocked(i,j): continue\n",
    "           \n",
    "            dh, dw = self.actions[a]\n",
    "            if dh == -1:\n",
    "                y_off = 1 - (1-a_l)/2.\n",
    "                x_off = 0.5\n",
    "            elif dh == 0:\n",
    "                y_off = 0.5\n",
    "                if dw == -1:\n",
    "                    x_off = 1 - (1-a_l)/2.0\n",
    "                elif dw == 1:\n",
    "                    x_off = (1-a_l)/2.0\n",
    "                else: raise ValueError(\"Invalid action to plot!\")\n",
    "            elif dh == 1:\n",
    "                y_off = (1-a_l)/2.0\n",
    "                x_off = 0.5\n",
    "            else: raise ValueError(\"Invalid action to plot\")\n",
    "            plt.arrow(j+x_off, i+y_off, dw*a_l, dh*a_l, width=0.1, \n",
    "                      head_width=0.4, head_length=0.6*0.35, fc=\"k\", ec=\"k\",\n",
    "                      length_includes_head=True, linewidth=0, overhang=0.3, \n",
    "                      zorder=10)\n",
    "        return\n",
    "\n",
    "    def _plot_terminal_states(self):\n",
    "        for (i,j) in self.terminal_states:\n",
    "            c = plt.Circle((j+0.5,i+0.5), radius=0.3, fill=False, ec=\"black\",\n",
    "                          lw=3, zorder=6)\n",
    "            plt.gca().add_patch(c)\n",
    "\n",
    "    def _plot_blocked_cells(self):\n",
    "        for (i,j) in self.blocked_cells:\n",
    "            rect = patches.Rectangle((j,i),1,1, zorder=1, color=(1,219/255,147/255))\n",
    "            plt.gca().add_patch(rect)        \n",
    "            \n",
    "    def _plot_rewards(self, resize_factor):\n",
    "        if len(self.cell_rewards) == 0: return\n",
    "\n",
    "        neg_cm = plt.get_cmap(\"Reds\")\n",
    "        pos_cm = plt.get_cmap(\"Greens\")\n",
    "        rmax = max(abs(x) for x in self.cell_rewards.values())\n",
    "        rmin = -rmax\n",
    "        for (i,j), r in self.cell_rewards.items():\n",
    "            plt.text(j+0.05,i+0.95, f\"{r:.1f}\", zorder=10, weight=\"bold\",\n",
    "                    size=8*resize_factor)\n",
    "            # to get the right color, we normalize to 0,255\n",
    "            r_norm = 1.0 * (r-rmin)/(rmax-rmin)\n",
    "            cm = pos_cm\n",
    "            if r < 0:\n",
    "                r_norm = 1.0 - r_norm\n",
    "                cm = neg_cm\n",
    "            col = cm(r_norm)\n",
    "            rect = patches.Rectangle((j,i),1,1, zorder=1, color=col)\n",
    "            plt.gca().add_patch(rect)   \n",
    "            \n",
    "    def plot(self, policy=None, height=None, width=None):\n",
    "        if height is None and width is None:\n",
    "            height = self.h\n",
    "            width = self.w\n",
    "        if height is None:\n",
    "            height = self.h*(width/self.w)\n",
    "        if width is None:\n",
    "            width = self.w*(height/self.h)\n",
    "        rf = height/self.h\n",
    "        fig, ax = plt.subplots(figsize=(width, height))\n",
    "        if policy is not None:\n",
    "            self._plot_policy(policy)\n",
    "        self._plot_terminal_states()\n",
    "        self._plot_blocked_cells()\n",
    "        self._plot_rewards(rf)\n",
    "        ax.grid(zorder=10, lw=1)\n",
    "        \n",
    "\n",
    "        plt.xlim(0, self.w)\n",
    "        plt.ylim(0, self.h)\n",
    "        plt.tick_params(axis='both', which='both', bottom=False, top=False, \n",
    "                        labelbottom=False, right=False, left=False, labelleft=False)\n",
    "        plt.xticks(range(self.w))\n",
    "        plt.yticks(range(self.h))\n",
    "        ax.invert_yaxis()\n",
    "        plt.title(\"Grid world\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-pharmacology",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Input, Dropout    \n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def mov_avg(x, l):\n",
    "    return pd.Series(x).rolling(l).mean().iloc[l-1:].values\n",
    "\n",
    "def episode(env, model, max_steps=None):\n",
    "    '''\n",
    "    Runs a single episode of the model in env for at most max_steps\n",
    "    Returns states, rewards, action_probabilities and the\n",
    "    estimated rewards by the critic.\n",
    "    '''\n",
    "    states = []\n",
    "    action_probs = []\n",
    "    est_rewards = []\n",
    "    rewards = []\n",
    "\n",
    "    state = env.reset()\n",
    "    i = 0\n",
    "    max_steps = np.inf if max_steps is None else max_steps\n",
    "    while i < max_steps:\n",
    "        state = tf.convert_to_tensor(state)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        states.append(state)\n",
    "        a_p, e_w = model(state)\n",
    "\n",
    "        action = np.random.choice(env.action_space.n, p=np.squeeze(a_p))\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        action_probs.append(a_p[0, action])\n",
    "        est_rewards.append(e_w[0,0])\n",
    "        rewards.append(reward)\n",
    "\n",
    "        i += 1\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return states, rewards, action_probs, est_rewards\n",
    "\n",
    "def sample_action(model, state):\n",
    "    '''\n",
    "    Given a model and a state, samples an action from model\n",
    "    '''\n",
    "    state = tf.convert_to_tensor(state)\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    a_p, e_w = model(state)\n",
    "    action = np.random.choice(env.action_space.n, p=np.squeeze(a_p))\n",
    "    return action\n",
    "\n",
    "class ActorCriticDiscrete():\n",
    "    '''\n",
    "    Implements the Actor Critic network structure for discrete action spaces\n",
    "    '''\n",
    "    def __init__(self, env, model, critic_loss=keras.losses.Huber(),\n",
    "                opt=\"Adam\", val_env=None):\n",
    "        self.env = env\n",
    "        self.val_env = env if val_env is None else val_env\n",
    "        self.model = model\n",
    "        self.critic_loss = critic_loss\n",
    "        self.opt = opt\n",
    "\n",
    "    def train_episode(self, discount, max_train_steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            states, rewards, action_probs, est_rewards = episode(self.env, self.model, max_train_steps)\n",
    "            reward = sum(rewards)\n",
    "\n",
    "            returns = []\n",
    "            discounted_sum = 0\n",
    "            for r in rewards[::-1]:\n",
    "                discounted_sum = r + discount * discounted_sum\n",
    "                returns.insert(0, discounted_sum)\n",
    "            returns = np.array(returns)\n",
    "            returns = (returns - returns.mean()) / (returns.std()+0.000001)\n",
    "\n",
    "            returns = tf.convert_to_tensor(returns, \"float32\")\n",
    "\n",
    "            critic_loss = self.critic_loss(est_rewards, returns)\n",
    "            al = -tf.math.log(action_probs) * (returns - est_rewards)\n",
    "            actor_loss = tf.reduce_mean(al)\n",
    "            loss = actor_loss + critic_loss\n",
    "\n",
    "            gradients = tape.gradient(loss, self.model.trainable_weights)\n",
    "            self.opt.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "        return reward\n",
    "\n",
    "    def val_episode(self, val_episodes=1, max_val_steps=None):\n",
    "        val_rewards = []\n",
    "        for val_e in range(val_episodes):\n",
    "            s, r, ap, er = episode(self.val_env, self.model, max_val_steps)\n",
    "            val_rewards.append(sum(r))\n",
    "        val_rewards = np.array(val_rewards)\n",
    "        return val_rewards\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        '''\n",
    "        Convenienve wrapper to quickly run a state through model.\n",
    "        Returns an action\n",
    "        '''\n",
    "        return sample_action(self.model, state)\n",
    "\n",
    "    def plot_reward_history(self, history, avg_length=5):\n",
    "        plt.figure(figsize=(14,5))\n",
    "        plt.plot(history, alpha=0.7, label=\"Rewards\")\n",
    "        plt.plot(np.arange(len(history))[avg_length-1:], mov_avg(history, avg_length), \n",
    "                 alpha=0.7, label=\"Smoothed rewards\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(self.env.spec.id)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def train(self, epochs, discount, max_train_steps = None, validate_every=5, \n",
    "              val_episodes=5, max_val_steps = 200, video=True, max_video_steps=500,\n",
    "             avg_reward_goal=None):\n",
    "\n",
    "        reward_history = []\n",
    "        try:\n",
    "            for epoch in tqdm(range(epochs)):\n",
    "\n",
    "                reward = self.train_episode(discount, max_train_steps)\n",
    "                reward_history.append(reward)\n",
    "                if epoch % validate_every == 0:\n",
    "                    if video:\n",
    "                        make_video(self.val_env, self.sample_action, max_video_steps)\n",
    "                    if val_episodes > 0:\n",
    "                        vr = self.val_episode(val_episodes, max_val_steps)\n",
    "                        print(f\"Epoch {epoch}/{epochs}: Validation reward mean/min/max: \"\n",
    "                              f\"{vr.mean():0.3f}, {vr.min():0.3f}, {vr.max():0.3f}\")\n",
    "                        if avg_reward_goal is not None and vr.mean() >= avg_reward_goal:\n",
    "                            print(f\"Reached goal of mean reward {avg_reward_goal}, stopping!\")\n",
    "                            break\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        reward_history = np.array(reward_history)\n",
    "        return reward_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-construction",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Setup END\n",
    "***\n",
    "(Click this cell, and do \"Run all above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-airport",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An introduction to Reinforcement Learning\n",
    "***\n",
    "Based on:\n",
    "* **MIT 6.S091**: Introduction to Deep Reinforcement Learning, by Lex Fridman\n",
    "* **UC Berkeley CS 287**: Advanced Robotics, by Pieter Abbeel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-hacker",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    IN_COLAB\n",
    "    print(\"Setup was run :)\")\n",
    "except:\n",
    "    print(\"Don't forget to run the setup!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-drinking",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "***\n",
    "* Organisational\n",
    "* Short recap of last challenge\n",
    "* Lecture 2: RL & Deep RL\n",
    "* Challenge 2: Landing on the moon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-fitting",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Organisational\n",
    "***\n",
    "The upcoming lectures have been swapped. We will have:\n",
    "* **21.05**: Lecture on NLP and Language Models\n",
    "* **04.06**: Machine Learning in Biomedicine, Prof. Kacprowski guest lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-proposal",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Minimal MNIST Recap\n",
    "***\n",
    "Best solution: 129 parameters @ 90.6% accuracy\n",
    "* Tons of trial and error, grid search for hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-officer",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Reinforcement Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-timer",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Deep Reinforcement Learning?\n",
    "***\n",
    "Last lecture we heard about **Deep Learning**:\n",
    "* Learn from examples, supervised\n",
    "* Find patterns, representations\n",
    "\n",
    "**Reinforcement Learning** is about:\n",
    "* Solving sequential decision making problems\n",
    "    * Comprehend the world and *act* based on that\n",
    "* How? Through trial and error in a world that provides rewards\n",
    "\n",
    "**Deep RL** is RL + Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-production",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Types of learning\n",
    "***\n",
    "* Supervised, Semi-supervised\n",
    "* Unsupervised\n",
    "* Reinforcement learning\n",
    "\n",
    "In each case of machine learning, the loss function **supervises** the learning\n",
    "* Someone has to say what's good and what's bad!\n",
    "\n",
    "It's more about the amount of human effort involved in training.\n",
    "* Supervised learning: Teach by example\n",
    "    * Learning patterns from data\n",
    "* Reinforcement learning: Teach by experience\n",
    "    * Learning from a world by exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-colombia",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement learning in humans\n",
    "***\n",
    "Humans appear to learn from very few examples of trial and error.\n",
    "* How, is still an open question\n",
    "\n",
    "There are several possible answers to why humans might learn so fast\n",
    "* Data & Hardware: 230 million years of trial and error\n",
    "* Imitation learning: Observation of other humans\n",
    "* Algorithms: Better than backpropagation and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-superintendent",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement learning framework\n",
    "***\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Markov_diagram_v2.svg/640px-Markov_diagram_v2.svg.png\" style=\"float: right;\">\n",
    "\n",
    "At each step, the **agent**:\n",
    "* Executes an **action**\n",
    "* Observes a new **state**\n",
    "* Receives a **reward**\n",
    "\n",
    "Different flavors:\n",
    "* Fully Observable (Chess) vs Partially Observable (Poker)\n",
    "* Single Agent (Atari) vs Multi Agent (Deep Traffic)\n",
    "* Deterministic (CartPole) vs Stochastic (Deep Traffic)\n",
    "* Static (Chess) vs Dynamic (Deep Traffic)\n",
    "* Discrete (Chess) vs Continuous (CartPole)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-warning",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The challenge for real world RL applications\n",
    "***\n",
    "The world an agent might learn from is a model designed by us\n",
    "* Agents live in a (partial) simulation of our world\n",
    "\n",
    "To apply RL agents to the real world, they need to *transfer* their experience\n",
    "* Two options:\n",
    "    * Improve the simulations\n",
    "    * Improve algorithms to better generalize from simulation to real world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-sensitivity",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Components of an RL agent\n",
    "***\n",
    "**Policy $\\pi$**, an agent's behavior function:\\\n",
    "$\\pi(s,a)=P\\left(a_t=a\\mid s_t=s\\right)$\n",
    "\n",
    "**Value function $Q$**: How good is each state and/or action\\\n",
    "$\\displaystyle Q^\\pi(s,a)=E\\left[R\\mid s,a,\\pi\\right]$\n",
    "\n",
    "**Reward $R_a(s,s')$**:\\\n",
    "The immediate reward after transition from $s$ to $s'$ with action $a$\n",
    "\n",
    "**Model**: The agent's representation of the environment\n",
    "<br>\n",
    "<br>\n",
    "<center>$s_0,a_0,r_1,s_1,a_1,r_2,\\dots,s_{n-1},a_{n-1},r_n$</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-antique",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Meaning of life: Maximize Reward\n",
    "***\n",
    "The purpose of an RL agent is to maximize the future reward\\\n",
    "$R_t=r_t+r_{t+1}+\\cdots+r_n$\n",
    "\n",
    "Rewards are discounted by factor $\\gamma$:\\\n",
    "$R_t=r_t+\\gamma r_{t+1}+\\gamma^2 r_{t+2}+\\cdots+\\gamma^{n-t}r_n$\n",
    "\n",
    "Why discounted?\n",
    "* Math trick to prove certain convergence properties\n",
    "* Also accounts for increasing uncertainty of far out rewards\n",
    "\n",
    "A good strategy would be to always choose the action that gives the highest future reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-bikini",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grid World or *Robot in a room*\n",
    "***\n",
    "Imagine a robot in a room. It may move and find:\n",
    "* Positive and negative rewards, as shown by value and color\n",
    "* Terminal states, indicated by a circle\n",
    "* Walls/Blocks, appearing yellow(ish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-oriental",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rewards = {(0,3):1,(1,3):-1}\n",
    "terminal_states = rewards.keys()\n",
    "blocks = [(1,1)]\n",
    "grid_world = Gridworld([],w=4,h=3, cell_rewards=rewards, \n",
    "                       terminal_states=terminal_states,\n",
    "                       blocked_cells=blocks)\n",
    "grid_world.plot(width=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-disclaimer",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deterministic movement\n",
    "***\n",
    "We'll allow our robot to move UP, DOWN, LEFT and RIGHT, but each move costs\n",
    "* Denoted by `step_reward=-0.04`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-density",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "actions = np.array([\n",
    "    [-1,0], # UP\n",
    "    [1,0], # DOWN \n",
    "    [0,-1], # Left\n",
    "    [0,1], # Right\n",
    "])\n",
    "\n",
    "rewards = {(0,3):1,\n",
    "           (1,3):-1}\n",
    "terminal_states = list(rewards.keys())\n",
    "blocks = [(1,1)]\n",
    "\n",
    "grid_world = Gridworld(actions,w=4,h=3, step_reward=-0.04,\n",
    "                       cell_rewards=rewards, \n",
    "                       terminal_states=terminal_states,\n",
    "                       blocked_cells=blocks)\n",
    "\n",
    "grid_world.plot(width=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-telephone",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov decision process (MDP)\n",
    "***\n",
    "What would be the optimal move on each cell to achieve the highest possible reward?\n",
    "* The theory behind Markov decision processes allows us to solve this!\n",
    "    * This is where the discount factor $\\gamma$ is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-allocation",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "result = grid_world.solve(discount=0.99)\n",
    "grid_world.plot(policy=result.policy, width=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-duration",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Policy**: Shortest path to $+1$ reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-eclipse",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimal policy for positive `step_reward`\n",
    "***\n",
    "Everything else being equal, how would a positive `step_reward` influence the agent's optimal behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-mattress",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grid_world = Gridworld(actions,w=4,h=3, step_reward=0.04,\n",
    "                       cell_rewards=rewards, \n",
    "                       terminal_states=terminal_states,\n",
    "                       blocked_cells=blocks)\n",
    "result = grid_world.solve(discount=0.99)\n",
    "grid_world.plot(policy=result.policy, width=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-meditation",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Policy**: Never terminate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-greenhouse",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adding uncertainty\n",
    "***\n",
    "So far, each action chosen by the agent resulted in exactly the anticipated outcome\n",
    "* It's a *deterministic* world our robot lives in\n",
    "\n",
    "However, some systems are stochastic by nature, or too complex to model deterministically\n",
    "* How would uncertainty impact the optimal policy in our grid world?\n",
    "* We let $\\rho\\in (0,1]$ denote the probability of executing the intended action $a$\n",
    "    * But with prob. $1-\\rho$ any other action is executed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-omega",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the deterministic result ($\\rho=1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-album",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grid_world = Gridworld(actions,w=4,h=3, step_reward=0.04,\n",
    "                       cell_rewards=rewards, \n",
    "                       terminal_states=terminal_states,\n",
    "                       blocked_cells=blocks)\n",
    "result = grid_world.solve(discount=0.99)\n",
    "grid_world.plot(policy=result.policy, height=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-launch",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What happens for $\\rho=0.7$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-myanmar",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grid_world = Gridworld(actions,w=4,h=3, step_reward=0.04,\n",
    "                       rho=0.7, cell_rewards=rewards, \n",
    "                       terminal_states=terminal_states,\n",
    "                       blocked_cells=blocks)\n",
    "result = grid_world.solve(discount=0.99)\n",
    "grid_world.plot(policy=result.policy, height=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-retreat",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lessons learned from Grid World\n",
    "***\n",
    "1. Environment model has big impact on optimal policy\n",
    "2. Reward structure has big impact on optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-modem",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.HTML('''\n",
    "<iframe width=\"800\" height=\"510\" src=\"https://www.youtube.com/embed/tlOIHko8ySg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-finance",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3 Types of Reinforcement learning\n",
    "***\n",
    "**Model-based**\n",
    "* Learn the model of the world, then plan on it\n",
    "* Update model and re-plan often\n",
    "\n",
    "**Value-based**\n",
    "* Learn the values of states or state-action pairs\n",
    "* Act by choosing best action in state\n",
    "* Exploration is a needed add-on\n",
    "\n",
    "**Policy-based**\n",
    "* Learn the stochastic behavior function $\\pi$\n",
    "* Act by sampling the policy\n",
    "* Exploration comes from the stochastic nature of the policy function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-cornell",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sample efficiency\n",
    "***\n",
    "<center><img src=\"img/slides/rl_sample_efficiency.png\"/></center>\n",
    "\n",
    "**Off-policy** are value-based agents\n",
    "* Constantly update the quality of being in a certain state\n",
    "* Always take the best next state\n",
    "    * Except sometimes, in order to explore other states\n",
    "\n",
    "**On-policy** are policy-based agents\n",
    "\n",
    "**Training RL agents can be a very lengthy process**\n",
    "* Training usually comes with high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-adams",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Actor-Critic\n",
    "***\n",
    "The **Actor** component takes the state of the world and returns a probability distribution over the actions\n",
    "* Ideally, the best actions get high probability\n",
    "* A policy-based idea\n",
    "\n",
    "We want the **Critic** to take a state and return the estimated future rewards\n",
    "* Estimates how good the choices of the actor are\n",
    "* A value-based approach\n",
    "\n",
    "They are trained together to get better at their respective tasks:\n",
    "* Critic gets more accurate value estimations\n",
    "* Actor assigns higher probability to high-valued actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-marriage",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Actor-Critic network structure\n",
    "***\n",
    "<center>\n",
    "<img src=\"https://www.mdpi.com/sensors/sensors-19-01547/article_deploy/html/images/sensors-19-01547-g002.png\" style=\"width:70%;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-authority",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Actor-Critic training outline\n",
    "***\n",
    "1. Run an episode\n",
    "    * Record state, action and reward for each step\n",
    "2. Compute discounted rewards\n",
    "    * Actions towards the end of the episode are usually more important\n",
    "3. Derive labels for the actor\n",
    "    * For each action with high reward, we want the actor to output high probability\n",
    "        * Low rewards should decrease probability of choosing that action\n",
    "4. Critic labels are simply to predict exactly the rewards of the current episode\n",
    "    * Regression task\n",
    "5. Repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-wesley",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variance in training\n",
    "***\n",
    "Most systems with competing network components suffer from high variance (GANs, VAs, RL)\n",
    "<center>\n",
    "<img src=\"img/slides/ll_rewards_single.png\"/>\n",
    "<img src=\"img/slides/ll_rewards_multiple.png\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-louisiana",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Environments: Meet OpenAI Gym\n",
    "***\n",
    "The `gym` package from OpenAI is a repository of multiple RL environments:\n",
    "* Classic control, i.e. balancing a pole on a cart\n",
    "* Robotics: Learning to pick up or stack in a 3D world\n",
    "* MuJoCo: Learning movement in a physics simulator (license required)\n",
    "* Atari: Huge amount of Atari 2600 games, like Breakout, SpaceInvaders or Pong\n",
    "* Tons of third party environments\n",
    "\n",
    "Difficulty ranges from easy to still not solved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-egyptian",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Starting simple: CartPole-v0\n",
    "***\n",
    "Reward: +1 for each step where the pole is upright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-pontiac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "# random action model:\n",
    "model = lambda state: env.action_space.sample()\n",
    "\n",
    "make_video(env, model, max_steps=200)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-sunday",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intermediate: LunarLander-v2\n",
    "***\n",
    "Rewards: Landing, Crashing, Fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-smoke",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "# random action model:\n",
    "model = lambda state: env.action_space.sample()\n",
    "\n",
    "make_video(env, model, max_steps=200)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-scotland",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Impossible: MontezumaRevenge-v0\n",
    "***\n",
    "Please apply to OpenAI, if you solve this:\\\n",
    "(and include me in your paper..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-election",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('MontezumaRevenge-ram-v0')\n",
    "# random action model:\n",
    "model = lambda state: env.action_space.sample()\n",
    "\n",
    "make_video(env, model, max_steps=500)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-frost",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Actor-critic: Code\n",
    "***\n",
    "Training procedure already in class `ActorCriticDiscrete`.\\\n",
    "You just need to provide:\n",
    "* An environment\n",
    "* The actual network structure\n",
    "* Optional: Optimizer, loss function for the critic regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-insurance",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "val_env = env.unwrapped\n",
    "env.seed(1); val_env.seed(1)\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    tf.random.set_seed(1)\n",
    "\n",
    "    input_layer = Input(env.observation_space.shape)\n",
    "    l = Dense(128, \"relu\")(input_layer)\n",
    "    actor = Dense(env.action_space.n, \"softmax\", name=\"actor\")(l)\n",
    "    critic = Dense(1, name=\"critic\")(l)\n",
    "    model = Model(input_layer, [actor, critic])\n",
    "\n",
    "    model.summary(50)\n",
    "    opt = keras.optimizers.Adam(lr=0.02)\n",
    "    critic_loss = keras.losses.Huber()\n",
    "\n",
    "    ac = ActorCriticDiscrete(env, model, critic_loss, opt, \n",
    "                              val_env=val_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-crime",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    tf.random.set_seed(1)\n",
    "    h = ac.train(epochs=2000, discount=0.99, \n",
    "                  max_train_steps=200, validate_every=50, \n",
    "                  val_episodes=10, max_val_steps=200, \n",
    "                  video=True, max_video_steps=500, \n",
    "                  avg_reward_goal=195)\n",
    "ac.plot_reward_history(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-bacon",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shooting for the stars\n",
    "***\n",
    "Can our model learn to land on the moon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-reducing",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "val_env = env.unwrapped\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    tf.random.set_seed(1)\n",
    "\n",
    "    input_layer = Input(env.observation_space.shape)\n",
    "    l = Dense(128, \"relu\")(input_layer)\n",
    "    actor = Dense(env.action_space.n, \"softmax\", name=\"actor\")(l)\n",
    "    critic = Dense(1, name=\"critic\")(l)\n",
    "    model = Model(input_layer, [actor, critic])\n",
    "\n",
    "    model.summary(50)\n",
    "    opt = keras.optimizers.Adam(lr=0.02)\n",
    "    critic_loss = keras.losses.MeanSquaredError()\n",
    "\n",
    "    ll = ActorCriticDiscrete(env, model, critic_loss, opt, \n",
    "                              val_env=val_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-terminology",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    ll_hist = ll.train(epochs=151, discount=0.99, \n",
    "                  max_train_steps=200, validate_every=50, \n",
    "                  val_episodes=5, max_val_steps=200, \n",
    "                  video=True, max_video_steps=500, \n",
    "                  avg_reward_goal=None)\n",
    "    ll.plot_reward_history(ll_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-rescue",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparison\n",
    "***\n",
    "<center><div style=\"display:flex; justify-content: space-evenly;\">\n",
    "    <div style=\"flex-basis:40%\">\n",
    "        <center>Episode 50</center>\n",
    "        <video width=\"95%\" controls src=\"img/slides/ll_ep50.mp4\" type=\"video/mp4\" />\n",
    "    </div>\n",
    "    <div style=\"flex-basis:40%\">\n",
    "        <center>Episode 1500</center>\n",
    "        <video width=\"95%\" controls src=\"img/slides/ll_ep1500.mp4\" type=\"video/mp4\" />\n",
    "    </div>\n",
    "</div>\n",
    "</center>\n",
    "<center><img src=\"img/slides/ll_rewards_2k.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-genealogy",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Possible extensions\n",
    "***\n",
    "* Experience replay\n",
    "    * Add a memory buffer\n",
    "* Decouple training of actor and critic\n",
    "    * Research suggests to train the critic less frequently to improve convergence\n",
    "* Optimize network structure\n",
    "* Play around with\n",
    "    * Number of training episodes (epochs)\n",
    "    * Length of training episodes\n",
    "    * Loss function and hyperparameters\n",
    "        * Learning rate scheduling?\n",
    "\n",
    "### Challenge time!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "rise": {
   "auto_select": "none",
   "enable_chalkboard": true,
   "overlay": "<div class='myheader'><img src='img/ai_camp.png' class='ifis_small'></div><div class='ifis_large'><img src='img/ifis_large.png'></div>",
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
