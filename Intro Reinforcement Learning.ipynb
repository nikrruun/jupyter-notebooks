{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "difficult-advantage",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Setup START\n",
    "***\n",
    "Skipped in slideshow mode, run manually before!\n",
    "\n",
    "Ideally, the code would be placed with their corresponding content slides.\\\n",
    "Unfortunately, RISE slideshow mode will not execute SKIP slides and there is no other way to hide them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-pitch",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "# OpenAI Gym related:\n",
    "%pip install cmake\n",
    "%pip install atari_py \n",
    "'''OpenAI devs actually messed up an indentation\n",
    "    in the video recorder class in 0.18.x.\n",
    "    If you want to see videos, wait for a patch or\n",
    "    use 0.17.x.\n",
    "'''\n",
    "%pip install gym[box2d]==0.17.3\n",
    "%pip install pyvirtualdisplay\n",
    "%pip install tqdm\n",
    "\n",
    "# Gridworld related:\n",
    "%pip install pymdptoolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-advantage",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !apt-get install -y xvfb python-opengl > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-failing",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Setup: OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-negative",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import numpy as np\n",
    "import base64\n",
    "import io\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "if IN_COLAB:\n",
    "    from pyvirtualdisplay import Display\n",
    "    d = Display()\n",
    "    d.start()\n",
    "\n",
    "'''\n",
    "Below is a solution to record and display videos for\n",
    "OpenAI Gym environments.\n",
    "Why the hassle?\n",
    "    It works both in Jupyter Notebook and Google Colab!\n",
    "(The author would like to note how painful it has been\n",
    "developing this)\n",
    "'''    \n",
    "\n",
    "def display_video_from_monitor(monitor):\n",
    "    '''\n",
    "    Converts all videos in a monitor to HTML videos\n",
    "    '''\n",
    "    if len(monitor.videos) == 0:\n",
    "        print(\"No videos to render!\")\n",
    "        return\n",
    "    for f in monitor.videos:\n",
    "        video = io.open(f[0], 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        display.display(display.HTML(data=\"\"\"\n",
    "            <video alt=\"test\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "            </video>\n",
    "            \"\"\".format(encoded.decode('ascii'))))\n",
    "    return\n",
    "\n",
    "def make_video(env, model, max_steps=None):\n",
    "    mon = Monitor(env, \"/data/videos/\", force=True)\n",
    "    state = mon.reset()\n",
    "    i = 0\n",
    "    while True:\n",
    "        action = model(state)\n",
    "        state, r, d, _ = mon.step(action)\n",
    "        if d: break\n",
    "        i+=1\n",
    "        if max_steps is not None and i>=max_steps: break\n",
    "    if mon.stats_recorder.done == False:    \n",
    "        mon.stats_recorder.save_complete()\n",
    "        mon.stats_recorder.done = True\n",
    "    mon.reset()\n",
    "    display_video_from_monitor(mon)\n",
    "    return mon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-trader",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Setup: Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-position",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from mdptoolbox import mdp\n",
    "\n",
    "class Gridworld():\n",
    "    def __init__(self, actions, w=4, h=3, rho=1.0, step_reward=0.0, cell_rewards={},\n",
    "               terminal_states=[], blocked_cells=[]):\n",
    "        self.actions = actions\n",
    "        self.w, self.h = w, h\n",
    "        self.rho = rho\n",
    "        self.step_reward = step_reward\n",
    "        self.cell_rewards = cell_rewards\n",
    "        self.terminal_states = {tuple(x) for x in terminal_states}\n",
    "        self.blocked_cells = {tuple(x) for x in blocked_cells}\n",
    "\n",
    "        self.grid = np.zeros((h,w))\n",
    "        self.states = np.array([[i,j] for i in range(h) for j in range(w)])\n",
    "        self.num_states = w*h\n",
    "        self.num_actions = len(actions)\n",
    "        self.p = np.zeros((self.num_actions, self.num_states, self.num_states), \"float32\")\n",
    "        self.r = np.zeros((self.num_actions, self.num_states, self.num_states), \"float32\")\n",
    "        self._fill_p()\n",
    "        self._fill_r()\n",
    "\n",
    "    def _fill_p(self):\n",
    "        # computing transition matrix:\n",
    "        for a in range(self.num_actions):\n",
    "            for i in range(self.num_states):\n",
    "                # simulate performing action a_ in state i\n",
    "                # action a is performed with prob. rho,\n",
    "                # with prob. 1-rho, any other action is chosen,\n",
    "                # which results in prob. (1-rho)/(num_moves-1) for\n",
    "                # a specific action a_ != a\n",
    "                for a_ in range(self.num_actions):\n",
    "                    rh, rw = self.simulate_move(self.actions[a_], self.states[i])\n",
    "                    j_ = rh*self.w + rw\n",
    "                    prob = self.rho if a == a_ else (1-self.rho)/(self.num_actions-1)\n",
    "                    self.p[a, i, j_] += prob\n",
    "                # due to numerical inaccuracies, we need to ensure that each row\n",
    "                # adds up to exactly 1, not 0.999995, because the solvers\n",
    "                # really dont seem to like that\n",
    "                row_sum = self.p[a,i].sum()\n",
    "                if row_sum != 1:\n",
    "                    if not np.allclose(row_sum,1, rtol=1e-5, atol=1e-5):\n",
    "                        raise ValueError(f\"Probability error at action{a}, state {i}\"\n",
    "                                         f\" with prob. {row_sum}\")\n",
    "                    self.p[a,i] /= row_sum\n",
    "        return\n",
    "        # make sure we can't ever leave terminal states\n",
    "        for (i,j) in self.terminal_states:\n",
    "            # transition probability from state (i,j) to any other state \n",
    "            # must be zero, but one for (i,j)\n",
    "            index = i*self.w+j\n",
    "            self.p[:,index,:] = 0\n",
    "            self.p[:, index, index] = 1\n",
    "\n",
    "    def _fill_r(self):\n",
    "        # transitions always give step_reward\n",
    "        self.r.fill(self.step_reward)\n",
    "        \n",
    "        # ending on a rewarded cell gives the corresponding reward\n",
    "        for (i,j), reward in self.cell_rewards.items():\n",
    "            self.r[:,:, i*self.w+j] += reward\n",
    "\n",
    "        # terminal cells do not give ANY reward after having reached them\n",
    "        for (i,j) in self.terminal_states:\n",
    "            k = i*self.w+j\n",
    "            self.r[:, k, :] = 0\n",
    "\n",
    "        # should staying on cells still give the associated rewards?\n",
    "        for i in range(self.num_actions):\n",
    "            np.fill_diagonal(self.r[i],0)\n",
    "\n",
    "    def simulate_move(self, move, state):\n",
    "        if self._is_blocked(state): return state\n",
    "        if self._is_terminal(state): \n",
    "            return state\n",
    "        nm = np.clip(np.array(move)+np.array(state), 0, (self.h-1, self.w-1))\n",
    "        if self._is_blocked(nm):\n",
    "            return state\n",
    "        return nm\n",
    "\n",
    "    def solve(self, discount=0.99, verbose=False):\n",
    "        '''\n",
    "        Solves the underlying MDP using Policy Iteration\n",
    "        Returns an object with the optimal policy. See\n",
    "        `mdp.PolicyIteration` doc for details.\n",
    "        '''\n",
    "        vi = mdp.PolicyIteration(self.p, self.r, discount)\n",
    "        if verbose:\n",
    "            vi.setVerbose()\n",
    "        vi.run()\n",
    "        return vi\n",
    "    \n",
    "    def _is_blocked(self, cy, cx=None):\n",
    "        # cx: position along grid width\n",
    "        if cx is None:\n",
    "            cy, cx = cy\n",
    "        return (cy, cx) in self.blocked_cells\n",
    "\n",
    "    def _is_terminal(self, cy, cx=None):\n",
    "        if cx is None:\n",
    "            cy, cx = cy\n",
    "        return (cy, cx) in self.terminal_states\n",
    "    \n",
    "    def _plot_policy(self, policy):\n",
    "        a_l = 0.6 # arrow length\n",
    "        for (i,j), a in list(zip(self.states, policy)):\n",
    "            if self._is_terminal(i,j) or self._is_blocked(i,j): continue\n",
    "           \n",
    "            dh, dw = self.actions[a]\n",
    "            if dh == -1:\n",
    "                y_off = 1 - (1-a_l)/2.\n",
    "                x_off = 0.5\n",
    "            elif dh == 0:\n",
    "                y_off = 0.5\n",
    "                if dw == -1:\n",
    "                    x_off = 1 - (1-a_l)/2.0\n",
    "                elif dw == 1:\n",
    "                    x_off = (1-a_l)/2.0\n",
    "                else: raise ValueError(\"Invalid action to plot!\")\n",
    "            elif dh == 1:\n",
    "                y_off = (1-a_l)/2.0\n",
    "                x_off = 0.5\n",
    "            else: raise ValueError(\"Invalid action to plot\")\n",
    "            plt.arrow(j+x_off, i+y_off, dw*a_l, dh*a_l, width=0.1, \n",
    "                      head_width=0.4, head_length=0.6*0.35, fc=\"k\", ec=\"k\",\n",
    "                      length_includes_head=True, linewidth=0, overhang=0.3, \n",
    "                      zorder=10)\n",
    "        return\n",
    "\n",
    "    def _plot_terminal_states(self):\n",
    "        for (i,j) in self.terminal_states:\n",
    "            c = plt.Circle((j+0.5,i+0.5), radius=0.3, fill=False, ec=\"black\",\n",
    "                          lw=3, zorder=6)\n",
    "            plt.gca().add_patch(c)\n",
    "\n",
    "    def _plot_blocked_cells(self):\n",
    "        for (i,j) in self.blocked_cells:\n",
    "            rect = patches.Rectangle((j,i),1,1, zorder=1, color=(1,219/255,147/255))\n",
    "            plt.gca().add_patch(rect)        \n",
    "            \n",
    "    def _plot_rewards(self, resize_factor):\n",
    "        if len(self.cell_rewards) == 0: return\n",
    "\n",
    "        neg_cm = plt.get_cmap(\"Reds\")\n",
    "        pos_cm = plt.get_cmap(\"Greens\")\n",
    "        rmax = max(abs(x) for x in self.cell_rewards.values())\n",
    "        rmin = -rmax\n",
    "        for (i,j), r in self.cell_rewards.items():\n",
    "            plt.text(j+0.05,i+0.95, f\"{r:.1f}\", zorder=10, weight=\"bold\",\n",
    "                    size=8*resize_factor)\n",
    "            # to get the right color, we normalize to 0,255\n",
    "            r_norm = 1.0 * (r-rmin)/(rmax-rmin)\n",
    "            cm = pos_cm\n",
    "            if r < 0:\n",
    "                r_norm = 1.0 - r_norm\n",
    "                cm = neg_cm\n",
    "            col = cm(r_norm)\n",
    "            rect = patches.Rectangle((j,i),1,1, zorder=1, color=col)\n",
    "            plt.gca().add_patch(rect)   \n",
    "            \n",
    "    def plot(self, policy=None, height=None, width=None):\n",
    "        if height is None and width is None:\n",
    "            height = self.h\n",
    "            width = self.w\n",
    "        if height is None:\n",
    "            height = self.h*(width/self.w)\n",
    "        if width is None:\n",
    "            width = self.w*(height/self.h)\n",
    "        rf = height/self.h\n",
    "        fig, ax = plt.subplots(figsize=(width, height))\n",
    "        if policy is not None:\n",
    "            self._plot_policy(policy)\n",
    "        self._plot_terminal_states()\n",
    "        self._plot_blocked_cells()\n",
    "        self._plot_rewards(rf)\n",
    "        ax.grid(zorder=10, lw=1)\n",
    "        \n",
    "\n",
    "        plt.xlim(0, self.w)\n",
    "        plt.ylim(0, self.h)\n",
    "        plt.tick_params(axis='both', which='both', bottom=False, top=False, \n",
    "                        labelbottom=False, right=False, left=False, labelleft=False)\n",
    "        plt.xticks(range(self.w))\n",
    "        plt.yticks(range(self.h))\n",
    "        ax.invert_yaxis()\n",
    "        plt.title(\"Grid world\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-construction",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Setup END\n",
    "***\n",
    "(Click this cell, and do \"Run all above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-airport",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An introduction to Reinforcement Learning\n",
    "***\n",
    "Based on:\n",
    "* **MIT 6.S091**: Introduction to Deep Reinforcement Learning, by Lex Fridman\n",
    "* **UC Berkeley CS 287**: Advanced Robotics, by Pieter Abbeel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-hacker",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    IN_COLAB\n",
    "    print(\"Setup was run :)\")\n",
    "except:\n",
    "    print(\"Don't forget to run the setup!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-drinking",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "***\n",
    "1. What is Deep RL?\n",
    "    * Deep Learning vs Deep Reinforcement Learning\n",
    "2. Types of learning\n",
    "    * Supervised vs unsupervised vs RL\n",
    "    * RL in Humans, why are we so good?\n",
    "3. Reinforcement Learning Framework\n",
    "    * Agent, Action, State, Reward\n",
    "4. Environments & Actions\n",
    "    * Observability, Single VS Multi Agent, Deterministic/Stochastic, Static/Dynamic, Discrete/Continuous\n",
    "5. RL in the real world\n",
    "    * Two open problems:\n",
    "        * Make better simulations\n",
    "        * Make algorithms that generalize better from simulation to real world\n",
    "6. Components of an RL agent\n",
    "    * Policy, the behavior function, probability of taking $a_t$ when in state $s_t$\n",
    "    * Value function, how good is each state/action in terms of expected future reward\n",
    "        * $V_\\pi(s)=E\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_t\\mid s_0 = s\\right]$\n",
    "    * Model, agent's representation of the environment\n",
    "7. Meaning of Life: Maximize Reward\n",
    "    * Future reward, discounted reward\n",
    "        * Sparse reward problem\n",
    "        * Credit assignment problem\n",
    "8. Robot in a room/Gridworld:\n",
    "    * Example to show the impact of reward structure and environment on the optimal policy\n",
    "    * WE HAVE CODE NOW, WOOOOO\n",
    "9. Types of RL (several ways to taxonomize exist!)\n",
    "    * Model-based\n",
    "        * learn a model of the world through interaction, then plan ahead using the model\n",
    "    * Value-based (off policy)\n",
    "        * Learn the values for states or states/actions\n",
    "            * Constantly update how good it is to take action $a$ in state $s$\n",
    "            * Act by choosing the best action in state. Exploration is needed!\n",
    "    * Policy-based\n",
    "        * Learning the right behavior function that maps state to action\n",
    "        * Act by sampling policy, exploration already included\n",
    "10. Taxonomy by OpenAI\n",
    "    * Is this needed?\n",
    "    \n",
    "TODO:\n",
    "DQN oder A2C\n",
    "+ Daran gebundene Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-timer",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Deep Reinforcement Learning?\n",
    "***\n",
    "Last lecture we heard about **Deep Learning**:\n",
    "* Learn from examples, supervised\n",
    "* Find patterns, representations\n",
    "\n",
    "**Reinforcement Learning** is about:\n",
    "* Solving sequential decision making problems\n",
    "    * Comprehend the world and *act* based on that\n",
    "* How? Through trial and error in a world that provides rewards\n",
    "\n",
    "**Deep RL** is RL + Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-production",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Types of learning\n",
    "***\n",
    "* Supervised, Semi-supervised\n",
    "* Unsupervised\n",
    "* Reinforcement learning\n",
    "\n",
    "In each case of machine learning, the loss function **supervises** the learning\n",
    "* Someone has to say what's good and what's bad!\n",
    "\n",
    "It's more about the amount of human effort involved in training.\n",
    "* Supervised learning: Teach by example\n",
    "    * Learning patterns from data\n",
    "* Reinforcement learning: Teach by experience\n",
    "    * Learning from a world by exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-colombia",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement learning in humans\n",
    "***\n",
    "Humans appear to learn from very few examples of trial and error.\n",
    "* How, is still an open question\n",
    "\n",
    "There are several possible answers to why humans might learn so fast\n",
    "* Data & Hardware: 230 million years of trial and error\n",
    "* Imitation learning: Observation of other humans\n",
    "* Algorithms: Better than backpropagation and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-superintendent",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement learning framework\n",
    "***\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Markov_diagram_v2.svg/640px-Markov_diagram_v2.svg.png\" style=\"float: right;\">\n",
    "\n",
    "At each step, the **agent**:\n",
    "* Executes an **action**\n",
    "* Observes a new **state**\n",
    "* Receives a **reward**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-candidate",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Environments and actions\n",
    "***\n",
    "* Fully Observable (Chess) vs Partially Observable (Poker)\n",
    "* Single Agent (Atari) vs Multi Agent (Deep Traffic)\n",
    "* Deterministic (CartPole) vs Stochastic (Deep Traffic)\n",
    "* Static (Chess) vs Dynamic (Deep Traffic)\n",
    "* Discrete (Chess) vs Continuous (CartPole)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-warning",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The challenge for real world RL applications\n",
    "***\n",
    "The world an agent might learn from is a model designed by us\n",
    "* Agents live in a (partial) simulation of our world\n",
    "\n",
    "To apply RL agents to the real world, they need to *transfer* their experience\n",
    "* Two options:\n",
    "    * Improve the simulations\n",
    "    * Improve algorithms to better generalize from simulation to real world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-sensitivity",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Components of an RL agent\n",
    "***\n",
    "**Policy $\\pi$**, an agent's behavior function:\\\n",
    "$\\pi(s,a)=P\\left(a_t=a\\mid s_t=s\\right)$\n",
    "\n",
    "**Value function $Q$**: How good is each state and/or action\\\n",
    "$\\displaystyle Q^\\pi(s,a)=E\\left[R\\mid s,a,\\pi\\right]$\n",
    "\n",
    "**Reward $R_a(s,s')$**:\\\n",
    "The immediate reward after transition from $s$ to $s'$ with action $a$\n",
    "\n",
    "**Model**: The agent's representation of the environment\n",
    "<br>\n",
    "<br>\n",
    "<center>$s_0,a_0,r_1,s_1,a_1,r_2,\\dots,s_{n-1},a_{n-1},r_n$</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-antique",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Meaning of life: Maximize Reward\n",
    "***\n",
    "The purpose of an RL agent is to maximize the future reward\\\n",
    "$R_t=r_t+r_{t+1}+\\cdots+r_n$\n",
    "\n",
    "Rewards are discounted by factor $\\gamma$:\\\n",
    "$R_t=r_t+\\gamma r_{t+1}+\\gamma^2 r_{t+2}+\\cdots+\\gamma^{n-t}r_n$\n",
    "\n",
    "Why discounted?\n",
    "* A math trick to prove certain convergence properties\n",
    "* Also accounts for increasing uncertainty of far out rewards\n",
    "\n",
    "A good strategy would be to always choose the action that gives the highest future reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-bikini",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grid World or *Robot in a room*\n",
    "***\n",
    "Imagine a robot in a room. It may move and find:\n",
    "* Positive and negative rewards, as shown by value and color\n",
    "* Terminal states, indicated by a circle\n",
    "* Walls/Blocks, appearing yellow(ish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-oriental",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rewards = {(0,3):1,(1,3):-1}\n",
    "terminal_states = rewards.keys()\n",
    "blocks = [(1,1)]\n",
    "grid_world = Gridworld([],w=4,h=3, cell_rewards=rewards, \n",
    "                       terminal_states=terminal_states,\n",
    "                       blocked_cells=blocks)\n",
    "grid_world.plot(width=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-disclaimer",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deterministic movement\n",
    "***\n",
    "We'll allow our robot to move UP, DOWN, LEFT and RIGHT, but each move costs\n",
    "* Denoted by `step_reward=-0.04`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-density",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "actions = np.array([\n",
    "    [-1,0], # UP\n",
    "    [1,0], # DOWN \n",
    "    [0,-1], # Left\n",
    "    [0,1], # Right\n",
    "])\n",
    "\n",
    "rewards = {(0,3):1,\n",
    "           (1,3):-1}\n",
    "terminal_states = list(rewards.keys())\n",
    "blocks = [(1,1)]\n",
    "\n",
    "grid_world = Gridworld(actions,w=4,h=3, step_reward=-0.04,\n",
    "                       cell_rewards=rewards, \n",
    "                       terminal_states=terminal_states,\n",
    "                       blocked_cells=blocks)\n",
    "\n",
    "grid_world.plot(width=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-telephone",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov decision process (MDP)\n",
    "***\n",
    "What would be the optimal move on each cell to achieve the highest possible reward?\n",
    "* The theory behind Markov decision processes allows us to solve this!\n",
    "    * This is where the discount factor $\\gamma$ is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-allocation",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "result = grid_world.solve(discount=0.99)\n",
    "grid_world.plot(policy=result.policy, width=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-duration",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Policy**: Shortest path to $+1$ reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-eclipse",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimal policy for positive `step_reward`\n",
    "***\n",
    "Everything else being equal, how would a positive `step_reward` influence the agent's optimal behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-mattress",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grid_world = Gridworld(actions,w=4,h=3, step_reward=0.04,\n",
    "                       cell_rewards=rewards, \n",
    "                       terminal_states=terminal_states,\n",
    "                       blocked_cells=blocks)\n",
    "result = grid_world.solve(discount=0.99)\n",
    "grid_world.plot(policy=result.policy, width=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-meditation",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Policy**: Never terminate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-greenhouse",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adding uncertainty\n",
    "***\n",
    "So far, each action chosen by the agent resulted in exactly the anticipated outcome\n",
    "* It's a *deterministic* world our robot lives in\n",
    "\n",
    "However, some systems are stochastic by nature, or too complex to model deterministically\n",
    "* How would uncertainty impact the optimal policy in our grid world?\n",
    "* We let $\\rho\\in (0,1]$ denote the probability of executing the intended action $a$\n",
    "    * But with prob. $1-\\rho$ any other action is executed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-omega",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the deterministic result ($\\rho=1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-album",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grid_world = Gridworld(actions,w=4,h=3, step_reward=0.04,\n",
    "                       cell_rewards=rewards, \n",
    "                       terminal_states=terminal_states,\n",
    "                       blocked_cells=blocks)\n",
    "result = grid_world.solve(discount=0.99)\n",
    "grid_world.plot(policy=result.policy, height=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-launch",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What happens for $\\rho=0.7$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-myanmar",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grid_world = Gridworld(actions,w=4,h=3, step_reward=0.04,\n",
    "                       rho=0.7, cell_rewards=rewards, \n",
    "                       terminal_states=terminal_states,\n",
    "                       blocked_cells=blocks)\n",
    "result = grid_world.solve(discount=0.99)\n",
    "grid_world.plot(policy=result.policy, height=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-retreat",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lessons learned from Grid World\n",
    "***\n",
    "1. Environment model has big impact on optimal policy\n",
    "2. Reward structure has big impact on optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-modem",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.HTML('''\n",
    "<iframe width=\"800\" height=\"510\" src=\"https://www.youtube.com/embed/tlOIHko8ySg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-portal",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "w=8; h=4; rho=1.0; step_reward=-0.04;\n",
    "\n",
    "actions = np.array([\n",
    "    [-1,0], # UP\n",
    "    #[1,0], # DOWN \n",
    "    [0,-1], # Left\n",
    "    [0,1], # Right\n",
    "])\n",
    "cell_rewards={\n",
    "    (0,3): 1,\n",
    "    (1,3): -100,\n",
    "    #(1,6): 1,\n",
    "    (2,0): -4,\n",
    "}\n",
    "terminal_states=[\n",
    "    [0,3],\n",
    "    [1,3],\n",
    "]\n",
    "blocked=[\n",
    "    [0,5],\n",
    "    [1,5],\n",
    "    [2,5],\n",
    "    [0,6],\n",
    "    [1,6],\n",
    "    [2,6],\n",
    "    [0,7],\n",
    "    [1,7],\n",
    "    [2,7],\n",
    "]\n",
    "\n",
    "gw  = Gridworld(w=w, h=h, rho=rho, step_reward=step_reward,\n",
    "                cell_rewards=cell_rewards, actions=actions,\n",
    "                terminal_states=terminal_states, blocked_cells=blocked)\n",
    "gw.plot(height=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-assumption",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#vi = mdp.ValueIteration(gw.p, gw.r, 0.9, max_iter=1e5)\n",
    "vi = mdp.PolicyIteration(gw.p, gw.r, 0.99, max_iter=50, eval_type=0)\n",
    "vi.setVerbose()\n",
    "vi.run()\n",
    "#print(vi.policy)\n",
    "gw.plot(policy=vi.policy, height=10)\n",
    "#vi.V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-finance",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3 Types of Reinforcement learning\n",
    "***\n",
    "**Model-based**\n",
    "* Learn the model of the world, then plan on it\n",
    "* Update model and re-plan often\n",
    "\n",
    "**Value-based**\n",
    "* Learn the values of states or state-action pairs\n",
    "* Act by choosing best action in state\n",
    "* Exploration is a needed add-on\n",
    "\n",
    "**Policy-based**\n",
    "* Learn the stochastic behavior function $\\pi$\n",
    "* Act by sampling the policy\n",
    "* Exploration comes from the stochastic nature of the policy function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-cornell",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sample efficiency\n",
    "***\n",
    "<center><img src=\"img/slides/rl_sample_efficiency.png\"/></center>\n",
    "\n",
    "**Off-policy** are value-based agents\n",
    "* Constantly update the quality of being in a certain state\n",
    "* Always take the best next state\n",
    "    * Except sometimes, in order to explore other states\n",
    "    \n",
    "**Training RL agents can be a very lengthy process**\n",
    "* Training usually comes with high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-adams",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Actor-Critic\n",
    "***\n",
    "The **Actor** component takes the state of the world and returns a probability distribution over the actions\n",
    "* Ideally, the best actions get high probability\n",
    "* A policy-based idea\n",
    "\n",
    "We want the **Critic** to take a state and return the estimated future rewards\n",
    "* Estimates how good the choices of the actor are\n",
    "* A value-based approach\n",
    "\n",
    "They are trained together to get better at their respective tasks:\n",
    "* Critic gets more accurate value estimations\n",
    "* Actor assigns higher probability to high-valued actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-marriage",
   "metadata": {},
   "source": [
    "# Actor-Critic\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-vision",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Placeholder\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-islam",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Loosely based on:\n",
    "https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic?hl=en\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Input, Dropout    \n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gym\n",
    "\n",
    "\n",
    "def episode(env, model, max_steps=None):\n",
    "    '''\n",
    "    Runs a single episode of the model in env for at most max_steps\n",
    "    Returns states, rewards, action_probabilities and the\n",
    "    estimated rewards by the critic.\n",
    "    '''\n",
    "    states = []\n",
    "    action_probs = []\n",
    "    est_rewards = []\n",
    "    rewards = []\n",
    "\n",
    "    state = env.reset()\n",
    "    i = 0\n",
    "    max_steps = np.inf if max_steps is None else max_steps\n",
    "    while i < max_steps:\n",
    "        state = tf.convert_to_tensor(state)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        states.append(state)\n",
    "        a_p, e_w = model(state)\n",
    "\n",
    "        action = np.random.choice(env.action_space.n, p=np.squeeze(a_p))\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        action_probs.append(a_p[0, action])\n",
    "        est_rewards.append(e_w[0,0])\n",
    "        rewards.append(reward)\n",
    "\n",
    "        i += 1\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return states, rewards, action_probs, est_rewards\n",
    "\n",
    "def sample_action(model, state):\n",
    "    '''\n",
    "    Given a model and a state, samples an action from model\n",
    "    '''\n",
    "    state = tf.convert_to_tensor(state)\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    a_p, e_w = model(state)\n",
    "    action = np.random.choice(env.action_space.n, p=np.squeeze(a_p))\n",
    "    return action\n",
    "\n",
    "class ActorCriticDiscrete():\n",
    "    '''\n",
    "    Implements the Actor Critic network structure for discrete action spaces\n",
    "    '''\n",
    "    def __init__(self, env, model, critic_loss=keras.losses.Huber(),\n",
    "                opt=\"Adam\", val_env=None):\n",
    "        self.env = env\n",
    "        self.val_env = env if val_env is None else val_env\n",
    "        self.model = model\n",
    "        self.critic_loss = critic_loss\n",
    "        self.opt = opt\n",
    "\n",
    "    def train_episode(self, discount, max_train_steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            states, rewards, action_probs, est_rewards = episode(self.env, self.model, max_train_steps)\n",
    "            reward = sum(rewards)\n",
    "\n",
    "            returns = []\n",
    "            discounted_sum = 0\n",
    "            for r in rewards[::-1]:\n",
    "                discounted_sum = r + discount * discounted_sum\n",
    "                returns.insert(0, discounted_sum)\n",
    "            returns = np.array(returns)\n",
    "            returns = (returns - returns.mean()) / (returns.std()+0.000001)\n",
    "\n",
    "            returns = tf.convert_to_tensor(returns, \"float32\")\n",
    "\n",
    "            critic_loss = self.critic_loss(est_rewards, returns)\n",
    "            al = -tf.math.log(action_probs) * (returns - est_rewards)\n",
    "            actor_loss = tf.reduce_mean(al)\n",
    "            loss = actor_loss + critic_loss\n",
    "\n",
    "            gradients = tape.gradient(loss, self.model.trainable_weights)\n",
    "            self.opt.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "        return reward\n",
    "\n",
    "    def val_episode(self, val_episodes=1, max_val_steps=None):\n",
    "        if max_val_steps is None:\n",
    "            max_val_steps = self.val_env._max_episode_steps\n",
    "\n",
    "        self.val_env._max_episode_steps = max_val_steps\n",
    "\n",
    "        val_rewards = []\n",
    "        for val_e in range(val_episodes):\n",
    "            s, r, ap, er = episode(self.val_env, self.model, max_val_steps)\n",
    "            val_rewards.append(sum(r))\n",
    "        val_rewards = np.array(val_rewards)\n",
    "        return val_rewards\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        '''\n",
    "        Convenienve wrapper to quickly run a state through model.\n",
    "        Returns an action\n",
    "        '''\n",
    "        return sample_action(self.model, state)\n",
    "\n",
    "    def train(self, epochs, discount, max_train_steps = 200, validate_every=5, \n",
    "              val_episodes=5, max_val_steps = 200, video=True, max_video_steps=500,\n",
    "             avg_reward_goal=None):\n",
    "        if max_train_steps > self.env._max_episode_steps:\n",
    "            raise ValueError(\"The provided training environment doesn't allow\"\n",
    "                            f\" more than {self.env._max_episode_steps} steps per\"\n",
    "                            \" episode. You might want to change the env settings.\")\n",
    "        reward_history = []\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "\n",
    "            reward = self.train_episode(discount, max_train_steps)\n",
    "            if epoch % validate_every == 0:\n",
    "                if video:\n",
    "                    make_video(self.val_env, self.sample_action, max_video_steps)\n",
    "                if val_episodes > 0:\n",
    "                    vr = self.val_episode(val_episodes, max_val_steps)\n",
    "                    print(f\"Epoch {epoch}/{epochs}: Validation reward mean/min/max: \"\n",
    "                          f\"{vr.mean():0.3f}, {vr.min():0.3f}, {vr.max():0.3f}\")\n",
    "                    if avg_reward_goal is not None and vr.mean() >= avg_reward_goal:\n",
    "                        print(f\"Reached goal of mean reward {avg_reward_goal}, stopping!\")\n",
    "                        break\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-louisiana",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OpenAI Gym\n",
    "***\n",
    "The `gym` package from OpenAI is a repository of multiple RL environments:\n",
    "* Classic control, i.e. balancing a pole on a cart\n",
    "* Robotics: Learning to pick up or stack in a 3D world\n",
    "* MuJoCo: Learning movement in a physics simulator (license required)\n",
    "* Atari: Huge amount of Atari 2600 games, like Breakout, SpaceInvaders or Pong\n",
    "* Tons of third party environments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-insurance",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "val_env = env.unwrapped\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "input_layer = Input(env.observation_space.shape)\n",
    "l = Dense(128, \"relu\")(input_layer)\n",
    "actor = Dense(env.action_space.n, \"softmax\", name=\"actor\")(l)\n",
    "critic = Dense(1, name=\"critic\")(l)\n",
    "model = Model(input_layer, [actor, critic])\n",
    "\n",
    "model.summary(50)\n",
    "opt = keras.optimizers.Adam(lr=0.02)\n",
    "critic_loss = keras.losses.MeanSquaredError()\n",
    "\n",
    "a2c = ActorCriticDiscrete(env, model, critic_loss, opt, \n",
    "                          val_env=val_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-crime",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "a2c.train(epochs=2000, discount=0.99, \n",
    "          max_train_steps=200, validate_every=50, \n",
    "          val_episodes=5, max_val_steps=500, \n",
    "          video=True, max_video_steps=500, \n",
    "          avg_reward_goal=195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "val_env = env.unwrapped\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "input_layer = Input(env.observation_space.shape)\n",
    "l = Dense(128, \"relu\")(input_layer)\n",
    "actor = Dense(env.action_space.n, \"softmax\", name=\"actor\")(l)\n",
    "critic = Dense(1, name=\"critic\")(l)\n",
    "model = Model(input_layer, [actor, critic])\n",
    "\n",
    "model.summary(50)\n",
    "opt = keras.optimizers.Adam(lr=0.02)\n",
    "critic_loss = keras.losses.Huber()\n",
    "\n",
    "a2c = ActorCriticDiscrete(env, model, critic_loss, opt, \n",
    "                          val_env=val_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-consortium",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "a2c.train(epochs=5000, discount=0.99, \n",
    "          max_train_steps=200, validate_every=50, \n",
    "          val_episodes=5, max_val_steps=200, \n",
    "          video=True, max_video_steps=500, \n",
    "          avg_reward_goal=195)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "rise": {
   "auto_select": "none",
   "enable_chalkboard": true,
   "overlay": "<div class='myheader'><img src='img/ai_camp.png' class='ifis_small'></div><div class='ifis_large'><img src='img/ifis_large.png'></div>",
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
